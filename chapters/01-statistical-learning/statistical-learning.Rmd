# (PART) STATISTICAL LEARNING {-}


# What is Statistical Learning?

```{r, include = FALSE}
knitr::opts_chunk$set(fig.align="center")
library(ymlthis)
```

<center>![](img/illos/01-stat.png)</center>

**Statistical learning** refers to a vast set of tools for understanding data. There are 2 types of statistical learning - _**supervised**_ and _**unsupervised**_:

* <mark class="def">Supervised learning</mark> is when you have a label for each data point, which mean it involves building a model that can predict an _output_ based on one or more _inputs_.

* <mark class="def">Unsupervised learning</mark> is when you don't have a label for each data point, where there are _inputs_ but no supervising _output_.

In statistical learning, _**input variables**_ $(X_{n})$ are typically denoted by _features_, _predictors_, _independent variables_ or _variables_ while _**output variable**_ $(Y)$ often called _dependent variable_ or _response_.

To assess the relationship between predictors $X_{1}, X_{2}, ...,X_{p}$, we have the equation as \@ref(eq:rela):

:::formula

\begin{equation}
\Large Y=f(X) + \epsilon
(\#eq:rela) 
\end{equation}
:::

Whereas:

* $f$ is fixed but unknown function of $X_{1},...,X_{p}$ and $\epsilon$ is a random _error term_, which is independent of $X$ and has mean zero.

In essence, statistical learning refers to a set of approaches for estimating $f$.


## Why Estimate f?

There are 2 main reasons: _**prediction**_ and _**inference**_.

### Prediction

Hypothetically, let's say we have the _**error term**_ averages to 0, predicting $Y$ can be assessed using this equation \@ref(eq:errorterm):

:::formula
\begin{equation}
\Large \hat{Y} = \hat{f}(X)
(\#eq:errorterm) \end{equation}
:::

Whereas:

* $\hat{f}$ represents the estimate for $f$

* $\hat{Y}$ represents the resulting prediction for $Y$

The accuracy of $\hat{Y}$ as a prediction for $Y$ depends on 2 quantities: _**reducible error**_ and _**irreducible error**_.

* <mark class="def">**Reducible error**</mark>: Errors arising from the mismatch between $\hat{f}$ and $f$. Can be improved by choosing a better model. Usually caused by <u>**Variance Error/Bias Error**</u>.

* <mark class="def">**Irreducible error**</mark>: Errors which can't be removed no matter what algorithm you apply. These errors are caused by unknown variables that are affecting the independent/output variable but are not one of the dependent/input variable while designing the model.

#### Inference

Necessary questions need to be asked in order to further understand the relationship between predictors $(X_{n})$ and outcome $(Y)$:

* <mark class="quest">Which predictors are associated with response?</mark> Only a small fraction of the available predictors are associated with the response.

* <mark class="quest">What is the relationship between predictors and response?</mark> The relationship between predictors and response is not always linear.

* <mark class="quest">Can the relationship between predictors and response be explained by the linear model or it is more complicated?</mark> The model can explain the relationship between predictors and response if the model is able to predict the response based on the predictors.



## How Do We Estimate f?

In order to estimate $f$, our goal is to apply a statistical learning method to the training data. Broadly speaking, most statistical learning methods for this task can be characterized as either _**parametric**_ or _**non-parametric**_.

### Parametric Methods

<mark class="def">Parametric methods</mark> (model-based approach) are those that are able to estimate the parameters of the model based on the training data. It involves a **two-step model-based approach**:

1. First, we **make an assumption about the functional form**, or shape, of $f$. In other words, we need to choose the model that best fits the data.

2. After the model has been selected, we need a procedure that uses the training data to **_fit_ or _train_ the model**.

Potential disadvantages of parametric methods is that the model will not usually match the true $f$. This can be avoid by choosing a more _flexible_ models that can fit many possible functions for $f$ forms and usually require greater number of parameters.

:::hat
Example fitting models for parametric methods (linear): <mark class="example">Ordinary Least Squares (OLS)</mark>, <mark class="example">Lasso</mark>.
:::

### Non-Parametric Methods

<mark class="def">Non-parametric methods</mark> (model-free approach) seek an estimate of $f$ without make explicit assumptions about the functional form of of $f$. Major disadvantages of this approach is that a very large number of observations is required in order to obtain an accurate estimate for $f$.

:::hat
Example fitting models for non-parametric methods: <mark class="example">smooth thin-plate spline fit</mark> and <mark class="example">rough thin-plate spline fit</mark>.
:::


## The Tradeoff Between Prediction Accuracy and Model Interpretability

```{r tradeoff, echo=FALSE, out.width='85%', fig.cap='Tradeoff Between Prediction Accuracy and Model Interpretability'}
knitr::include_graphics("img/xkcd-interpretability-vs-flexibility.png")
```


<mark class="quest">Why would we ever choose to use a more restrictive method instead of a very flexible approach?</mark>;

If we are mainly interested in the interpretability of the model, we would rather use a more flexible model. This is because the flexibility of the model is usually better than the interpretability of the model.&#x20;

In contrast, if we are interested in the prediction accuracy of the model, we would rather use a more restrictive model. This is because the flexibility of the model is usually better than the prediction accuracy of the model.

The tradeoff between prediction accuracy ad moel interpretability are illustrate in Figure \@ref(fig:tradeoff)

## Supervised Vs. Unsupervised Learning

Most statistical learning problems involve both **supervised** and **unsupervised learning*</mark>**.

<mark class="def">In supervised learning</mark>, we wish to fit a model to the training data and predict the response variable based on the predictors, with the aim of accurately predicting the response variable or better understanding the relationship between predictors and response variable.

:::hat
Some of the statistical approaches that apply the supervised learning method are: <mark class="example">Linear Regression</mark>, <mark class="example">Logistic Regression</mark>, <mark class="example">Boosting & Support Vector Machine</mark>, <mark class="example">Generalized Additive Models (GAMs)</mark>.
:::

In contrast, <mark class="def">unsupervised learning methods</mark> are those that do not require any training data. One statistical learning tool that we may use in this setting is _cluster analysis_ or clustering. The goal of this method is to ascertain, whether observations fall into distinct groups.



## Regression Vs. Classification

Variables can be characterized as either _**quantitative**_ or _**qualitative**_

* <mark class="def">Quantitative variables</mark> are those that can be measured in terms of a number;

* <mark class="def">Qualitative variables</mark> are those that can be measured in terms of a set of categories.

We tend to refer to problems with a quantitative response variable as **regression** problems and problems with a qualitative response variable as **classification** problems. However, an important note is that <u><strong>it does not matter much whether the predictors/variables are quantitative or qualitative</u></strong>.
