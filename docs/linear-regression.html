<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Linear Regression | _main.knit</title>
  <meta name="description" content="Notes and exercise attempts for “An Introduction to Statistical Learning”" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Linear Regression | _main.knit" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://ndleah.github.io/stat-learning/" />
  <meta property="og:image" content="https://ndleah.github.io/stat-learning/images/logo-black.png" />
  <meta property="og:description" content="Notes and exercise attempts for “An Introduction to Statistical Learning”" />
  <meta name="github-repo" content="ndleah/stat_learning" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Linear Regression | _main.knit" />
  
  <meta name="twitter:description" content="Notes and exercise attempts for “An Introduction to Statistical Learning”" />
  <meta name="twitter:image" content="https://ndleah.github.io/stat-learning/images/logo-black.png" />




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="images/favicon.ico" type="image/x-icon" />
<link rel="prev" href="statistical-learning.html"/>
<link rel="next" href="parts.html"/>
<script src="libs/header-attrs-2.9/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  background-color: #f8f8f8; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Stat Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> About</a></li>
<li class="chapter" data-level="2" data-path="statistical-learning.html"><a href="statistical-learning.html"><i class="fa fa-check"></i><b>2</b> Statistical Learning</a>
<ul>
<li class="chapter" data-level="2.1" data-path="statistical-learning.html"><a href="statistical-learning.html#what-is-statistical-learning"><i class="fa fa-check"></i><b>2.1</b> What is Statistical Learning?</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="statistical-learning.html"><a href="statistical-learning.html#why-estimate-f"><i class="fa fa-check"></i><b>2.1.1</b> Why Estimate f?</a></li>
<li class="chapter" data-level="2.1.2" data-path="statistical-learning.html"><a href="statistical-learning.html#how-do-we-estimate-f"><i class="fa fa-check"></i><b>2.1.2</b> How Do We Estimate f?</a></li>
<li class="chapter" data-level="2.1.3" data-path="statistical-learning.html"><a href="statistical-learning.html#the-tradeoff-between-prediction-accuracy-and-model-interpretability"><i class="fa fa-check"></i><b>2.1.3</b> The Tradeoff Between Prediction Accuracy and Model Interpretability</a></li>
<li class="chapter" data-level="2.1.4" data-path="statistical-learning.html"><a href="statistical-learning.html#supervised-vs.-unsupervised-learning"><i class="fa fa-check"></i><b>2.1.4</b> Supervised Vs. Unsupervised Learning</a></li>
<li class="chapter" data-level="2.1.5" data-path="statistical-learning.html"><a href="statistical-learning.html#regression-vs.-classification"><i class="fa fa-check"></i><b>2.1.5</b> Regression Vs. Classification</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="statistical-learning.html"><a href="statistical-learning.html#assessing-model-accuracy"><i class="fa fa-check"></i><b>2.2</b> Assessing Model Accuracy</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="statistical-learning.html"><a href="statistical-learning.html#the-regression-setting"><i class="fa fa-check"></i><b>2.2.1</b> The Regression Setting</a></li>
<li class="chapter" data-level="2.2.2" data-path="statistical-learning.html"><a href="statistical-learning.html#the-bias-variance-trade-off"><i class="fa fa-check"></i><b>2.2.2</b> The Bias-Variance Trade-Off</a></li>
<li class="chapter" data-level="2.2.3" data-path="statistical-learning.html"><a href="statistical-learning.html#the-classification-setting"><i class="fa fa-check"></i><b>2.2.3</b> The Classification Setting</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>3</b> Linear Regression</a>
<ul>
<li class="chapter" data-level="3.1" data-path="linear-regression.html"><a href="linear-regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>3.1</b> Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="linear-regression.html"><a href="linear-regression.html#estimating-the-coefficients"><i class="fa fa-check"></i><b>3.1.1</b> Estimating the Coefficients</a></li>
<li class="chapter" data-level="3.1.2" data-path="linear-regression.html"><a href="linear-regression.html#assessing-the-accuracy-of-the-coefficient-estimates"><i class="fa fa-check"></i><b>3.1.2</b> Assessing the Accuracy of the Coefficient Estimates</a></li>
<li class="chapter" data-level="3.1.3" data-path="linear-regression.html"><a href="linear-regression.html#assessing-the-accuracy-of-the-model"><i class="fa fa-check"></i><b>3.1.3</b> Assessing the Accuracy of the Model</a></li>
<li class="chapter" data-level="3.1.4" data-path="linear-regression.html"><a href="linear-regression.html#consideration"><i class="fa fa-check"></i><b>3.1.4</b> Consideration</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="linear-regression.html"><a href="linear-regression.html#multiple-linear-regression"><i class="fa fa-check"></i><b>3.2</b> Multiple Linear Regression</a></li>
<li class="chapter" data-level="3.3" data-path="linear-regression.html"><a href="linear-regression.html#other-considerations-in-the-regression-model"><i class="fa fa-check"></i><b>3.3</b> Other Considerations in the Regression Model</a></li>
<li class="chapter" data-level="3.4" data-path="linear-regression.html"><a href="linear-regression.html#case-study---marketing-plan"><i class="fa fa-check"></i><b>3.4</b> Case Study - Marketing Plan</a></li>
<li class="chapter" data-level="3.5" data-path="linear-regression.html"><a href="linear-regression.html#chapter-3-lab-linear-regression"><i class="fa fa-check"></i><b>3.5</b> Chapter 3 Lab: Linear Regression</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="linear-regression.html"><a href="linear-regression.html#simple-linear-regression-1"><i class="fa fa-check"></i><b>3.5.1</b> Simple Linear Regression</a></li>
<li class="chapter" data-level="3.5.2" data-path="linear-regression.html"><a href="linear-regression.html#multiple-linear-regression-1"><i class="fa fa-check"></i><b>3.5.2</b> Multiple Linear Regression</a></li>
<li class="chapter" data-level="3.5.3" data-path="linear-regression.html"><a href="linear-regression.html#interaction-terms"><i class="fa fa-check"></i><b>3.5.3</b> Interaction Terms</a></li>
<li class="chapter" data-level="3.5.4" data-path="linear-regression.html"><a href="linear-regression.html#non-linear-transformations-of-the-predictors"><i class="fa fa-check"></i><b>3.5.4</b> Non-linear Transformations of the Predictors</a></li>
<li class="chapter" data-level="3.5.5" data-path="linear-regression.html"><a href="linear-regression.html#qualitative-predictors"><i class="fa fa-check"></i><b>3.5.5</b> Qualitative Predictors</a></li>
<li class="chapter" data-level="3.5.6" data-path="linear-regression.html"><a href="linear-regression.html#writing-functions"><i class="fa fa-check"></i><b>3.5.6</b> Writing Functions</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="linear-regression.html"><a href="linear-regression.html#exercises-applied"><i class="fa fa-check"></i><b>3.6</b> Exercises (Applied)</a>
<ul>
<li class="chapter" data-level="" data-path="linear-regression.html"><a href="linear-regression.html#question-1"><i class="fa fa-check"></i>Question 1</a></li>
<li class="chapter" data-level="" data-path="linear-regression.html"><a href="linear-regression.html#question-2"><i class="fa fa-check"></i>Question 2</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="parts.html"><a href="parts.html"><i class="fa fa-check"></i><b>4</b> Parts</a></li>
<li class="chapter" data-level="5" data-path="footnotes-and-citations.html"><a href="footnotes-and-citations.html"><i class="fa fa-check"></i><b>5</b> Footnotes and citations</a>
<ul>
<li class="chapter" data-level="5.1" data-path="footnotes-and-citations.html"><a href="footnotes-and-citations.html#footnotes"><i class="fa fa-check"></i><b>5.1</b> Footnotes</a></li>
<li class="chapter" data-level="5.2" data-path="footnotes-and-citations.html"><a href="footnotes-and-citations.html#citations"><i class="fa fa-check"></i><b>5.2</b> Citations</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="blocks.html"><a href="blocks.html"><i class="fa fa-check"></i><b>6</b> Blocks</a>
<ul>
<li class="chapter" data-level="6.1" data-path="blocks.html"><a href="blocks.html#equations"><i class="fa fa-check"></i><b>6.1</b> Equations</a></li>
<li class="chapter" data-level="6.2" data-path="blocks.html"><a href="blocks.html#theorems-and-proofs"><i class="fa fa-check"></i><b>6.2</b> Theorems and proofs</a></li>
<li class="chapter" data-level="6.3" data-path="blocks.html"><a href="blocks.html#callout-blocks"><i class="fa fa-check"></i><b>6.3</b> Callout blocks</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="sharing-your-book.html"><a href="sharing-your-book.html"><i class="fa fa-check"></i><b>7</b> Sharing your book</a>
<ul>
<li class="chapter" data-level="7.1" data-path="sharing-your-book.html"><a href="sharing-your-book.html#publishing"><i class="fa fa-check"></i><b>7.1</b> Publishing</a></li>
<li class="chapter" data-level="7.2" data-path="sharing-your-book.html"><a href="sharing-your-book.html#pages"><i class="fa fa-check"></i><b>7.2</b> 404 pages</a></li>
<li class="chapter" data-level="7.3" data-path="sharing-your-book.html"><a href="sharing-your-book.html#metadata-for-sharing"><i class="fa fa-check"></i><b>7.3</b> Metadata for sharing</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/ndleah/stat-learning" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="linear-regression" class="section level1" number="3">
<h1><span class="header-section-number">Chapter 3</span> Linear Regression</h1>
<div id="simple-linear-regression" class="section level2" number="3.1">
<h2><span class="header-section-number">3.1</span> Simple Linear Regression</h2>
<p><strong><em>Simple linear regression</em></strong> is a very straightforward approach for predicting a qualitative response <span class="math inline">\(Y\)</span> on the basis of a single predictor <span class="math inline">\(X\)</span>. Mathematically, we can write this linear relationship as</p>
<p><span class="math display">\[
Y \approx \beta_0 + \beta_1 X
\]</span></p>
<p>Whereas:</p>
<ul>
<li><span class="math inline">\(\beta_0\)</span>: intercept</li>
<li><span class="math inline">\(\beta_1\)</span>: slope</li>
</ul>
<div id="estimating-the-coefficients" class="section level3" number="3.1.1">
<h3><span class="header-section-number">3.1.1</span> Estimating the Coefficients</h3>
<p>In practice, <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are unknown. So before make predictions, we must use data to estimate the coefficients. Let <span class="math inline">\((x_1, Y_1), (x_2, Y_2),...,(x_n, Y_n)\)</span>
represent <span class="math inline">\(n\)</span> observation pairs, each of which consists of a measurement of <span class="math inline">\(X\)</span> and a measurement of <span class="math inline">\(Y\)</span>. Our goal is to obtain coefficient estimates <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> such that the linear model fits the available data well. In other words, we want to find an intercept <span class="math inline">\(\beta_0\)</span> and a slope <span class="math inline">\(\beta_1\)</span> such that the resulting line is as close as possible to the <span class="math inline">\(n\)</span> data points.</p>
<div class="figure">
<img src="img/regression.png" alt="" />
<p class="caption"><strong>Figure 3.1</strong>. Regression Line</p>
</div>
<p>There are numbers of ways of measuring <em>closeness</em>. However, by far the most common approach involves minimizing the <em>least squares</em> criterion, and we take that approach in this chapter.</p>
<div id="the-least-square-approach" class="section level4 unnumbered">
<h4>The Least Square Approach</h4>
<p>Let <span class="math inline">\(Y_i = \beta_0 + \beta_1 X_i\)</span> be the prediction for <span class="math inline">\(Y\)</span> based on the <span class="math inline">\(i\)</span>th value of <span class="math inline">\(X\)</span>. Then <span class="math inline">\(e_i = y_i -\hat{y}+i\)</span> represents the <span class="math inline">\(i\)</span>th <em>residual</em> - this is the difference between the <span class="math inline">\(i\)</span>th value and the <span class="math inline">\(i\)</span>th reponse value that is predicted by our linear model. We define the <strong><em>residual sum of square (RSS)</em></strong> as</p>
<p><span class="math display">\[
RSS = e_1^2 + e_2^2 + ... + e_n^2
\]</span></p>
<p>or equivalent as
<span class="math display">\[
RSS = (y_1 - \hat{\beta_0} - \hat{\beta_1}x_2)^2 + (y_2 - \hat{\beta_0} - \hat{\beta_1}x_2)^2  + ... + (y_n - \hat{\beta_0} - \hat{\beta_1}x_n)^2
\]</span></p>
<p><strong><em>The least square approach</em></strong> chooses <span class="math inline">\(\hat{\beta_0}\)</span> and <span class="math inline">\(\hat{\beta_1}\)</span> to minimize the RSS.</p>
</div>
</div>
<div id="assessing-the-accuracy-of-the-coefficient-estimates" class="section level3" number="3.1.2">
<h3><span class="header-section-number">3.1.2</span> Assessing the Accuracy of the Coefficient Estimates</h3>
<div id="population-regression-line" class="section level4 unnumbered">
<h4><strong>Population Regression Line:</strong></h4>
<p><span class="math display">\[
Y=\beta_0 + \beta_1X + \epsilon
\]</span></p>
<p>where <span class="math inline">\(\epsilon\)</span> mean zero random error term.</p>
</div>
<div id="least-squares-line" class="section level4 unnumbered">
<h4><strong>Least squares line:</strong></h4>
<p><span class="math display">\[
\hat{y}=\hat{\beta_0} + \hat{\beta_1}X 
\]</span></p>
<p>true relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> takes the form <span class="math inline">\(Y=f(X)+\epsilon\)</span>.</p>
<p>Fundamentally, the concept of these two lines (population regression line vs. least squares line) is a natural extension of the standard statistical approach of using information from a sample to estimate characteristics of a large population.</p>
<p>For example, suppose that we are interested in knowing the population mean <span class="math inline">\(\mu\)</span> of some random variable <span class="math inline">\(Y\)</span>. Unfortunately, <span class="math inline">\(\mu\)</span> is unknown, but we do have access to <span class="math inline">\(n\)</span> observations from <span class="math inline">\(Y\)</span>, which we can write as <span class="math inline">\(y_1\)</span>, <span class="math inline">\(y_2\)</span>, <span class="math inline">\(\cdots\)</span>, <span class="math inline">\(y_n\)</span>, and which we can use to estimate <span class="math inline">\(\mu\)</span>. A reasonable estimate is <span class="math inline">\(\hat{\mu}\)</span> = <span class="math inline">\(\hat{y}\)</span>, where <span class="math inline">\(\hat{y} = \dfrac {1}{n}\sum^n_{i=1}{y_i}\)</span> is the sample mean.</p>
<p>The sample mean and the population mean are different, but in general the sample mean will provide a good estimate of the population mean. In the same way, the unknown coefficients <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> in linear regression define the population regression line. We seek to estimate these unknown coefficients using  and  given in the least square line formula above. These coefficient estimates define the least squares line.</p>
<p>The analogy between linear regression and estimation of the mean of a random variable is an apt one based on the concept of bias. If we use the sample mean <span class="math inline">\(\hat{\mu}\)</span> to estimate <span class="math inline">\(\mu\)</span>, this estimate is unbiased, in the sense that on average, we expect <span class="math inline">\(\hat{\mu}\)</span> to equal <span class="math inline">\(\mu\)</span>.</p>
<p>In other words, if we estimate <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> on the basis of a particular data set, then our estimates won’t be exactly equal to <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>. But if we could average the estimates obtained over a huge number of data sets, then the average of these estimates would be spot on!</p>
<p>In fact, we can see from the right-hand panel of Figure 3.3 that the average of many least squares lines, each estimated from a separate data set, is pretty close to the true population regression line.</p>
<div class="figure">
<img src="img/fig3.3.png" alt="" />
<p class="caption"><strong>Figure 3.2.</strong> A simulated data set.</p>
</div>
<ul>
<li><p><strong>Left:</strong> The red line represents the true relationship, <span class="math inline">\(f(X) = 2+3Xf(X)=2+3X\)</span>, which is known as the population regression line. The blue line is the least squares line; it is the least squares estimate for <span class="math inline">\(f(X)\)</span> based on the observed data, shown in black.</p></li>
<li><p><strong>Right:</strong> The population regression line is again shown in red, and the least squares line in dark blue. In light blue, ten least squares lines are shown, each computed on the basis of a separate random set of observations. Each least squares line is different, but on average, the least squares lines are quite close to the population regression line.</p></li>
</ul>
</div>
<div id="how-accurate-is-the-sample-mean-hatmu-as-an-estimate-of-population-mean-mu" class="section level4 unnumbered">
<h4><strong>How Accurate Is The Sample Mean <span class="math inline">\(\hat{\mu}\)</span> As An Estimate Of Population Mean <span class="math inline">\(\mu\)</span></strong></h4>
<p>We answer this question by computing the <strong><em>standard error</em></strong> of<span class="math inline">\(\mu\)</span>, written as SE(<span class="math inline">\(\hat{\mu}\)</span>).</p>
<p>Standard errors can also be used to perform <strong><em>hypothesis tests</em></strong> on the coefficients. The most common hypothesis test involves testing the <strong><em>null hypothesis</em></strong> of</p>
<ul>
<li><span class="math inline">\(H_0\)</span>: There is no relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span></li>
<li><span class="math inline">\(H_a\)</span>: There is some relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span></li>
</ul>
<p>Mathematically, this corresponds to testing</p>
<p><span class="math display">\[
H_0: \beta_1 = 0
\]</span></p>
<p>versus
<span class="math display">\[
H_1: \beta_1 \neq 0
\]</span></p>
<p>If <span class="math inline">\(\beta_1 = 0\)</span> then the model reduces to <span class="math inline">\(Y=\beta_0+\epsilon\)</span>, and <span class="math inline">\(X\)</span> is not associated with <span class="math inline">\(Y\)</span>.</p>
<p>To test the null hypothesis, we need to determine whether <span class="math inline">\(\hat{\beta_1}\)</span>, our estimate for <span class="math inline">\(\beta_1\)</span>, is sufficiently far from zero that we can be confident that <span class="math inline">\(\beta_1\)</span> is non-zero.</p>
</div>
<div id="how-far-is-far-enough" class="section level4 unnumbered">
<h4><strong>How far is far enough?</strong></h4>
<p>This is course depends on the accuracy of <span class="math inline">\(\hat{\beta_1}\)</span> -that is, it depends on <span class="math inline">\(SE(\hat{\beta_1})\)</span>. If SE(<span class="math inline">\(\hat{\beta_1}\)</span>) is small, then even relatively small values of <span class="math inline">\(\hat{\beta_1}\)</span> may provide strong evidence that <span class="math inline">\(\beta_1 \neq 0\)</span>​, and hence that there is a relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. In contrast, if SE(<span class="math inline">\(\hat{\beta_1}\)</span>) is large, then <span class="math inline">\(\hat{\beta_1}\)</span> must be large in absolute value in order for us to reject the null hypothesis.</p>
</div>
<div id="t-distribution" class="section level4 unnumbered">
<h4><strong>T-distribution</strong></h4>
<p><strong>The t-distribution</strong> describes the standardized distances of sample means to the population mean when the population standard deviation is not known, and the observations come from a normally distributed population.</p>
<p>In practice, we compute a <strong><em>t-statistic</em></strong>, given by
<span class="math display">\[
t=\dfrac{\hat{\beta_1}-0}{SE(\hat{\beta_1})}
\]</span></p>
<p>For example, we have a t-distribution with <span class="math inline">\(n-2\)</span> degrees of freedom as in Figure 3.3.</p>
<div class="figure">
<img src="img/tstat.png" alt="" />
<p class="caption"><strong>Figure 3.3.</strong> Sampling Distribution of Test Statistic</p>
</div>
<p>The t-distribution has a bell shape and for values of n greater than approximately 30 it is quite similar to the normal distribution. Consequently, it is a simple matter to compute probability of observing any value equal to <span class="math inline">\(|t|\)</span> or larger, assuming <span class="math inline">\(\beta_1 =0\)</span>. We call this probability the <strong><em>p-value</em></strong>.</p>
<p><span class="math display">\[
P(|T| &gt; t) = p
\]</span></p>
<p>Roughly speaking, we interpret the <strong><em>p-value</em></strong> as follows:</p>
<p>If we see a small p-value, then we can infer that there is an association between the predictor and the response. We reject the null hypothesis-that is, we declare a relationship to exist between X and Y- if the p-value is small enough.</p>
<p>Typical p-value cutoffs for rejecting the null hypothesis are 5 or 1 %. When n = 30, these corresponds to t-statistics of around 2 and 2.75, respectively.</p>
</div>
</div>
<div id="assessing-the-accuracy-of-the-model" class="section level3" number="3.1.3">
<h3><span class="header-section-number">3.1.3</span> Assessing the Accuracy of the Model</h3>
<p>After we have rejected the null hypothesis, t is natural to want to quantify the <em>extend of the model fits the data</em>.</p>
<p>The quality of a linear regression fit is typically assessed using 2 related quantities: the <em>residual standard error</em> and the <span class="math inline">\(R^2\)</span> statistic.</p>
<div id="residual-standard-error" class="section level4" number="3.1.3.1">
<h4><span class="header-section-number">3.1.3.1</span> Residual Standard Error</h4>
<p>The RSE is an estimation of the standard deviation of <span class="math inline">\(\epsilon\)</span>. Roughly speaking, it is the average amount that the response will deviate from the true regression line. It is computed using the formula:</p>
<p><span class="math display">\[
RSE=\sqrt{\frac{1}{n-2}} RSS= \sqrt{\frac{1}{n-2} {\sum_{i=1}^n (y_{i} - \hat{y_1})^{2}}}
\]</span></p>
<p>The RSE is considered a measure of the <em>lack of fit</em> of the model to the data:</p>
<ul>
<li><p>If RSE is small, we can conclude that the model fits the data very well.</p></li>
<li><p>In contrast, if RSE is large, it indicates that the model doesn’t fit the data well.</p></li>
</ul>
<div class="alert alert-danger hints-alert">
<div class="hints-icon">
<i class="fa fa-info"></i>
</div>
<div class="hints-container">
One consideration of RSE is it is the measure in the units of <span class="math inline">\(Y\)</span> and thus, the method is not always clear what constitute a good RSE.
</div>
</div>
<div class="puzzle">
<p>hello halle</p>
</div>
</div>
<div id="r2-standard-error" class="section level4" number="3.1.3.2">
<h4><span class="header-section-number">3.1.3.2</span> <strong><span class="math inline">\(R^2\)</span> Standard Error</strong></h4>
<p>The <span class="math inline">\(R^2\)</span> statistic takes the form of a <em>proportion</em>-the proportion of variance explained-and so it always takes on a value between 0 and 1, and is independent of the scale of <span class="math inline">\(Y\)</span>. To calculate <span class="math inline">\(R^2\)</span>, we use the formula:</p>
<p><span class="math display">\[
R^2 = \frac{TSS}{TSS−RSS}=1−\frac{TSS}{RSS}
\]</span></p>
<p>Whereas:</p>
<ul>
<li><strong>TSS</strong>: total sum of square.</li>
<li><strong>RSS</strong>: measures the amount of variability that is left unexplained after performing the regression.</li>
</ul>
<p>In terms of evaluating the <span class="math inline">\(R^2\)</span> statistic result:</p>
<ul>
<li><span class="math inline">\(R^2\)</span> statistic close to 1 - the model fits well.</li>
<li><span class="math inline">\(R^2\)</span> statistic close to 0 - the model does not fit well.</li>
</ul>
</div>
</div>
<div id="consideration" class="section level3" number="3.1.4">
<h3><span class="header-section-number">3.1.4</span> Consideration</h3>
<p><strong>Simple linear regression</strong> is a useful approach for predicting a response on the basis of a single predictor variable. However, in practice we often have more than one predictor.</p>
<p>The solution for this is to use another linear regression named as <strong>Multiple Linear Regression</strong>, which will be discussed in the next section.</p>
</div>
</div>
<div id="multiple-linear-regression" class="section level2" number="3.2">
<h2><span class="header-section-number">3.2</span> Multiple Linear Regression</h2>
<p>In general, the multiple linear regression model takes the form</p>
<p><span class="math display">\[
Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_nX_n + \epsilon
\]</span></p>
<div id="some-important-questions" class="section level4 unnumbered">
<h4>Some Important Questions:</h4>
<p>When we perform multiple linear regression, we usually are interested in answering a few important questions.</p>
<ol style="list-style-type: decimal">
<li><em>Is at least one of the predictor <span class="math inline">\(X_1, X_2,...,X_p\)</span> useful in predicting the response?</em></li>
<li><em>Do all the predictors help to explain <span class="math inline">\(Y\)</span>, or is only a subset of the predictors useful?</em></li>
<li><em>How well does th model fit the data?</em></li>
<li><em>Given a set of predictor values, what response value should we predict, and how accurate is our prediction?</em></li>
</ol>
</div>
<div id="response-and-predictors-relationships" class="section level4" number="3.2.0.1">
<h4><span class="header-section-number">3.2.0.1</span> Response and Predictors Relationships</h4>
<p>In the multiple regression setting with <span class="math inline">\(p\)</span> predictors, we can use a hypothesis test to discover the relationship between predictors and response:</p>
<p><span class="math display">\[
H_0 = \beta_1 = \beta_2 = ... = \beta_p = 0
\]</span></p>
<p>versus the alternative
<span class="math display">\[
H_a = \beta_j \neq 0 
\]</span></p>
<p>This hypothesis test is performed by computing the <em>F-statistic</em></p>
<p><span class="math display">\[
F = \frac{(TSS-RSS)/p}{RSS/(n-p-1)}
\]</span></p>
<p>If F-statistic:</p>
<ul>
<li><p><strong>Near 1</strong> - no relationship between response and predictors.</p></li>
<li><p><strong>Greater than 1</strong> - there is some relationship between response and predictors.</p></li>
</ul>
<div class="alert alert-danger hints-alert">
<div class="hints-icon">
<i class="fa fa-info"></i>
</div>
<div class="hints-container">
<p>F-statistic only apply when the number of <span class="math inline">\(p\)</span> predictors is small.</p>
<p>When <em>p</em> is large, some of the approaches discussed in the next section, such as <em>forward selection</em>, can be used.</p>
</div>
</div>
</div>
<div id="dealing-with-large-number-of-variables" class="section level4" number="3.2.0.2">
<h4><span class="header-section-number">3.2.0.2</span> Dealing With Large Number Of Variables</h4>
<p>The task of determining which predictors are associated with the response is referred to as <em>variable selection</em>.</p>
<p>There are 3 classical approaches:</p>
<ul>
<li><strong><em>Forward selection</em></strong>: starts with no selected variables. During subsequent steps, it evaluates if each candidate variable improves some chosen statistical criterion given previously selected variables, and adds the variable that improves the criterion most. It repeats these steps until none of the remaining variables improves the criterion.</li>
<li><strong><em>Backward selection</em></strong>:starts with the full model, that is, all the variables. At each step, it removes a variable that is least important and does not meet the criterion.</li>
<li><strong><em>Mixed (Stepwise) selection</em></strong>: is the combination of forward selection and backward elimination. Start with no variables in the model, then add variables that is the best fit. However, if at any point the p-value for one of the variables in the model series rises above a certain threshold, then we remove that variable out of the model.</li>
</ul>
</div>
<div id="model-fit" class="section level4" number="3.2.0.3">
<h4><span class="header-section-number">3.2.0.3</span> Model Fit</h4>
<p>Using the same methods: RSE and <span class="math inline">\(R^2\)</span> statistic as simple linear regression.</p>
</div>
</div>
<div id="other-considerations-in-the-regression-model" class="section level2" number="3.3">
<h2><span class="header-section-number">3.3</span> Other Considerations in the Regression Model</h2>
<div id="potential-problems" class="section level4 unnumbered">
<h4>Potential Problems</h4>
<p>When we fit a linear regression model, many problems can occur, to name a few:</p>
<ul>
<li><em>Non-linearity of the response-predictor relationships.</em></li>
<li><em>Correlation of error terms.</em></li>
<li><em>Non-constant variance of error terms.</em></li>
<li><em>Outliers.</em></li>
<li><em>High-leverage points.</em></li>
<li><em>Collinearity.</em></li>
</ul>
</div>
<div id="non-linearity-of-the-data" class="section level4" number="3.3.0.1">
<h4><span class="header-section-number">3.3.0.1</span> Non-linearity of the Data</h4>
<p>The first assumption of Linear Regression is that relations between the independent and dependent variables must be linear.</p>
<p>Although this assumption is not always cited in the literature, it is logical and important to check for it. After all, if your relationships are not linear, you should not use a linear model, but rather a non-linear model of which plenty exist.</p>
<p>We can check for linear relationships easily by making a scatter plot for each independent variable with the dependent variable as in Figure 3.9.</p>
<div class="figure">
<img src="img/107-Figure3.9-1.png" alt="" />
<p class="caption">Figure 3.9. Plots of residuals versus predicted (or fitted) values for the Auto data set</p>
</div>
<p>In each plot, the red line is a smooth fit to the residuals, intended to make it easier to identify a trend.</p>
<ul>
<li><strong>Left:</strong> A linear regression of <code>mpg</code> on horsepower. A strong pattern in the residuals indicates non-linearity in the data.</li>
<li><strong>Right:</strong> A linear regression of <code>mpg</code> on <code>horsepower</code> and <code>horsepower2</code>. There is little pattern in the residuals.</li>
</ul>
</div>
<div id="correlation-of-error-terms" class="section level4" number="3.3.0.2">
<h4><span class="header-section-number">3.3.0.2</span> Correlation of Error Terms</h4>
<p>If there is correlation among the error terms <span class="math inline">\(\epsilon_1, \epsilon_2,...,\epsilon_n\)</span>, then the estimated standard errors (SE) will tend to underestimate the true SE. As the result, p-value associated with the model will be lower than they should be, which could cause us to erroneously conclude that a parameter is statistically significant.</p>
<div class="alert alert-danger hints-alert">
<div class="hints-icon">
<i class="fa fa-info"></i>
</div>
<div class="hints-container">
<p>Such correlations frequently occur in the context of <em>time series</em> data, which consist of observations for which measurements are obtained at adjacent time points will have positively correlated errors.</p>
</div>
</div>
<div class="alert alert-success hints-alert">
<div class="hints-icon">
<i class="fa fa-info"></i>
</div>
<div class="hints-container">
In order to determine if this is the case for a given data set, we can plot the residuals from our model as a function of time.
</div>
</div>
<p>Figure 3.10 provides an illustration. <strong>In the top panel</strong>, we see the residuals from a linear regression fit to data generated with uncorrelated errors. There is no evidence of a time-related trend in the residuals.</p>
<p>In contrast, <strong>the residuals in the bottom panel</strong> are from a data set in which adjacent errors had a correlation of 0.9. Now there is a clear pattern in the residuals—adjacent residuals tend to take on similar values.</p>
<p>Finally, <strong>the center panel</strong> illustrates a more moderate case in which the residuals had a correlation of 0.5. There is still evidence of tracking, but the pattern is less clear.</p>
<div class="figure">
<img src="img/109-Figure3.10-1.png" alt="" />
<p class="caption">Figure 3.10. Plots of residuals from simulated time series data sets generated with differing levels of correlation ρ between error terms for adjacent time points</p>
</div>
</div>
<div id="non-constant-variance-of-error-terms-heteroscedasticity" class="section level4" number="3.3.0.3">
<h4><span class="header-section-number">3.3.0.3</span> Non-constant Variance of Error Terms (Heteroscedasticity)</h4>
<p>Heteroscedasticity in a model means that the error is constant along the values of the dependent variable.</p>
<div class="alert alert-success hints-alert">
<div class="hints-icon">
<i class="fa fa-info"></i>
</div>
<div class="hints-container">
<p>One can identify non-constant variances in the errors, or heteroscedasticity, from the presence of a funnel shape in heteroscedathe residual plot.</p>
</div>
</div>
<p>An example is shown in the left-hand panel of Figure 3.11, sticity in which the magnitude of the residuals tends to increase with the fitted values. In each plot, the red line is a smooth fit to the residuals, intended to make it easier to identify a trend. The blue lines track the outer quantiles of the residuals, and emphasize patterns:</p>
<ul>
<li><p><strong>Left:</strong> The <em>funnel shape</em> indicates heteroscedasticity.</p></li>
<li><p><strong>Right:</strong> The response has been log transformed, and there is now no evidence of heteroscedasticity.</p></li>
</ul>
<div class="figure">
<img src="img/110-Figure3.11-1.png" alt="" />
<p class="caption">Figure 3.11. Residual plots</p>
</div>
<p>Some of the suggested solutions are:</p>
<ol style="list-style-type: decimal">
<li><p><strong><em>Do some work on your input data</em></strong> like having some variables to add or remove.</p></li>
<li><p><strong><em>Do transformations</em></strong>, like applying concave function such as <em>logistics</em> (<span class="math inline">\(logY\)</span>) or <em>square root</em> <span class="math inline">\(\sqrt{Y}\)</span>.</p></li>
<li><p>If this doesn’t change anything, you can also switch to the <strong><em>weighted least squares model</em></strong>. <em>Weighted least squares</em> is a model that can deal with unconstant variances and heteroscedasticity is therefore not a problem.</p></li>
</ol>
</div>
<div id="outlier" class="section level4" number="3.3.0.4">
<h4><span class="header-section-number">3.3.0.4</span> Outlier</h4>
<p>An <strong>outlier</strong> is a point for which yi is far from the value predicted by model. Outliers can arise for a variety of reasons, such as incorrect recording of an observation during data collection.</p>
<p>As illustrated in Figure 3.12:</p>
<ul>
<li><p><strong>Left:</strong> The least squares regression line is shown in red, and the regression line after removing the outlier is shown in blue.</p></li>
<li><p><strong>Center:</strong> The residual plot clearly identifies the outlier.</p></li>
<li><p><strong>Right:</strong> The outlier has a <strong><em>studentized residual</em></strong> of 6; typically we expect values between −3 and 3.</p></li>
</ul>
<div class="figure">
<img src="img/111-Figure3.12-1.png" alt="" />
<p class="caption">Figure 3.12. Outlier plots</p>
</div>
<p>If we believe that an outlier has occurred due to an error in data collection or recording, then one solution is to simply remove the observation.</p>
</div>
<div id="high-leverage-points" class="section level4" number="3.3.0.5">
<h4><span class="header-section-number">3.3.0.5</span> High Leverage Points</h4>
<p>In contrast to outlier with unusual for response value <span class="math inline">\(y\)</span>, observations with high leverage high leverage have an unusual value for <span class="math inline">\(x_i\)</span>.</p>
<p>As illustrated in Figure 3.12:</p>
<div class="figure">
<img src="img/112-Figure3.13-1.png" alt="" />
<p class="caption">Figure 3.13. Leveraging Observations Plots</p>
</div>
<ul>
<li><p><strong>Left:</strong> Observation 41 is a high leverage point, while 20 is not. The red line is the fit to all the data, and the blue line is the fit with observation 41 removed.</p></li>
<li><p><strong>Center:</strong> The red observation is not unusual in terms of its X1 value or its X2 value, but still falls outside the bulk of the data, and hence has high leverage.</p></li>
<li><p><strong>Right:</strong> Observation 41 has a high leverage and a high residual.</p></li>
</ul>
<p>In order to quantify an observation’s leverage, we compute the <strong><em>leverage
statistic</em></strong>.</p>
<p><span class="math display">\[
h_i = \frac{1}{n} + \frac{(x_i - \overline{x})^2}{\sum_{i&#39;=1}^{n}(x_i&#39; - \overline x)^2}
\]</span></p>
<p>A large value of this statistic indicates an observation with high leverage.</p>
</div>
<div id="collinearity" class="section level4" number="3.3.0.6">
<h4><span class="header-section-number">3.3.0.6</span> Collinearity</h4>
<p><strong><em>Collinearity</em></strong> refers to the situation in which two or more predictor variables are closely related to one another.</p>
<p>In order to check for collinearity, we can either use <strong><em>Correlation Matrix</em></strong> or <strong><em>Variance Inflation Factor (VIF)</em></strong>.</p>
<div id="correlation-matrix" class="section level5 unnumbered">
<h5>Correlation Matrix</h5>
<p>A simple way to detect collinearity is to look at the correlation matrix
of the predictors. An element of this matrix that is large in absolute value indicates a pair of highly correlated variables, and therefore a collinearity problem in the data.</p>
<div class="figure">
<img src="img/unnamed.png" alt="" />
<p class="caption">Figure 3.14. Sample Correlation Matrix using R</p>
</div>
<p>Unfortunately, not all collinearity problems can be detected by inspection of the correlation matrix: it is possible for collinearity to exist between three or more variables even if no pair of variables has a particularly high correlation. We call this situation <strong><em>multicollinearity</em></strong>.</p>
<p>Multicollinearity causes problems in using regression models to draw conclusions about the relationships between predictors and outcome. An individual predictor’s P value may test non-significant even though it is important. Confidence intervals for regression coefficients in a multicollinear model may be so high that tiny changes in individual observations have a large effect on the coefficients, sometimes reversing their signs.</p>
</div>
<div id="variance-inflation-factor-vif" class="section level5 unnumbered">
<h5>Variance Inflation Factor (VIF)</h5>
<p>Instead of inspecting the correlation matrix, a better way to assess collinearity is to compute the <strong>variance inflation factor (VIF)</strong>. This can easily be calculated in <code>R</code> using software packages.</p>
<p>When faced with the problem of collinearity, there are two simple solutions:</p>
<ol style="list-style-type: decimal">
<li><p>The first is to drop one of the problematic variables from the regression.</p></li>
<li><p>The second solution is to combine the collinear variables together into a single predictor.</p></li>
</ol>
</div>
</div>
</div>
<div id="case-study---marketing-plan" class="section level2" number="3.4">
<h2><span class="header-section-number">3.4</span> Case Study - Marketing Plan</h2>
<div id="data-overview" class="section level4" number="3.4.0.1">
<h4><span class="header-section-number">3.4.0.1</span> Data Overview</h4>
<p>The <code>Advertising</code> data set consists of the sales of that product in 200 different markets, along with advertising budgets for the product in each of those markets for three different media: <code>TV</code>, <code>radio</code>, and <code>newspaper</code>. The data are displayed in Figure 2.1.</p>
<p>The plot displays sales, in thousands of units, as a function of <code>TV</code>, <code>radio</code>, and <code>newspaper</code> budgets, in thousands of dollars, for 200 different markets. In each plot we show the simple least squares fit of sales to that variable, as described in Chapter 3. In other words, each blue line represents a simple model that can be used to predict sales using <code>TV</code>, <code>radio</code>, and newspaper`, respectively.</p>
<div class="figure">
<img src="img/30-Figure2.1-1.png" alt="" />
<p class="caption">Figure 2.1. The Advertising data set</p>
</div>
</div>
<div id="important-questions" class="section level4" number="3.4.0.2">
<h4><span class="header-section-number">3.4.0.2</span> Important Questions</h4>
<p>Suppose that in our role as statistical consultants we are asked to suggest, on the basis of this data, a marketing plan for next year that will result in high product sales. Here are a few important questions that we might seek to address:</p>
<ol style="list-style-type: decimal">
<li><strong><em>Is there a relationship between advertising sales and budget?</em></strong></li>
</ol>
<p>This question can be answered by fitting a multiple regression model
of sales onto <code>TV</code>, <code>radio</code>, and <code>newspaper</code>, as:</p>
<p><span class="math display">\[
sales = \beta_0 + \beta_1 × TV + \beta_2 × radio + \beta_3 × newspaper + \epsilon.
\]</span></p>
<p>and testing the hypothesis
<span class="math display">\[
\Large H_0: \beta_{TV} = \beta_{radio} = \beta_{newspaper} = 0
\]</span></p>
<p>F-statistic can be used to determine whether or not we should reject this null hypothesis. In this case, the p-value corresponding to the F-statistic in Table 3.6 is very low, indicating clear evidence of a relationship between advertising and sales.</p>
<div class="figure">
<img src="img/90-Table3.6-1.png" alt="" />
<p class="caption">Table 3.6. More information about the least squares model for the regression of number of units sold on TV, newspaper, and radio advertising budgets in the Advertising data</p>
</div>
<ol start="2" style="list-style-type: decimal">
<li><strong><em>How strong is the relationship?</em></strong></li>
</ol>
<p>First, the RSE estimates the standard deviation of the response from the
population regression line. For the <code>Advertising</code> data, the RSE is 1,681 units while the mean value for the response is 14,022, indicating a percentage error of roughly 12%. Second, the <span class="math inline">\(R^2\)</span> statistic records the percentage of variability in the response that is explained by the predictors. The predictors explain almost 90% of the variance in sales. The RSE and <span class="math inline">\(R^2\)</span> statistics are displayed in Table 3.6.</p>
<ol start="3" style="list-style-type: decimal">
<li><strong><em>Which media contribute to sales?</em></strong></li>
</ol>
<p>To answer this question, we can examine the p-values associated with each predictor’s t-statistic. In the multiple linear regression displayed in Table 3.4, the p-values for <code>TV</code> and <code>radio</code> are low, but the p-value for <code>newspaper</code> is not. This suggests that only <code>TV</code> and <code>radio</code> are related to sales.</p>
<div class="figure">
<img src="img/88-Table3.4-1.png" alt="" />
<p class="caption">Table 3.4. For the Advertising data, least squares coefficient estimates of the multiple linear regression of number of units sold on radio, TV, and newspaper advertising budgets.</p>
</div>
<ol start="4" style="list-style-type: decimal">
<li><strong><em>How large is the effect of each medium on sales?</em></strong></li>
</ol>
<p>The standard error of <span class="math inline">\(\hat{\beta_j}\)</span> can be used to construct confidence intervals for <span class="math inline">\(\beta_j\)</span>. For the <code>Advertising</code> data, the 95% confidence intervals are as follows: (0.043, 0.049) for <code>TV</code>, (0.172, 0.206) for <code>radio</code>, and (−0.013, 0.011) for <code>newspaper</code>.</p>
<p>The confidence intervals for <code>TV</code> and <code>radio</code> are narrow and far from zero, providing evidence that these media are related to sales. But the interval for <code>newspaper</code> includes zero, indicating that the variable is not statistically significant given the values of <code>TV</code> and <code>radio</code>.</p>
<p>We saw in previous section that collinearity can result in very wide standard errors.</p>
<div class="alert alert-info hints-alert">
<div class="hints-icon">
<i class="fa fa-info"></i>
</div>
<div class="hints-container">
<p><strong><em>Could collinearity be the reason that the confidence interval associated with newspaper is so wide?</em></strong></p>
The VIF scores are 1.005, 1.145, and 1.145 for <code>TV</code>,<code>radio</code>, and <code>newspaper</code>, suggesting no evidence of collinearity.
</div>
</div>
<p>In order to assess the association of each medium individually on sales, we can perform three separate simple linear regressions. Results are shown in Tables 3.1 and 3.3.</p>
<div class="figure">
<img src="img/82-Table3.1-1.png" alt="" />
<p class="caption">Table 3.1. More simple linear regression models for the Advertising data</p>
</div>
<p>Coefficients of the least squares model for the regression of number of units sold on <code>TV</code> advertising budget. An increase of $1,000 in the <code>TV</code> advertising budget is associated with an increase in sales by around 50 units.</p>
<div class="figure">
<img src="img/86-Table3.3-1.png" alt="" />
<p class="caption">Table 3.3. More simple linear regression models for the Advertising data</p>
</div>
<p>Coefficients of the simple linear regression model for number of units sold on:</p>
<ul>
<li><p><strong>Top:</strong> <code>radio</code> advertising budget.</p></li>
<li><p><strong>Bottom:</strong> <code>newspaper</code> advertising budget.</p></li>
</ul>
<p>A $1,000 increase in spending on <code>radio</code> advertising is associated with an average increase in sales by around 203 units, while the same increase in spending on <code>newspaper</code> advertising is associated with an average increase in sales by around 55 units.</p>
<p>There is evidence of an extremely strong association between <code>TV</code> and <code>sales</code> and between <code>radio</code> and <code>sales</code>. There is evidence of a mild association between <code>newspaper</code> and <code>sales</code>, when the values of <code>TV</code> and <code>radio</code> are ignored.</p>
<ol start="5" style="list-style-type: decimal">
<li><strong><em>How accurately can we predict future sales?</em></strong></li>
</ol>
<p>The response can be predicted using:</p>
<p><span class="math display">\[
y = \hat{\beta_0} + \hat{\beta_1}x_1 + \hat{\beta_2}x_2 + · · · + \hat{\beta_0}x_p.
\]</span></p>
<p>The accuracy associated with this estimate depends on whether we wish to predict an individual response, <span class="math inline">\(Y = f(X) + \epsilon\)</span>, or the average response, <span class="math inline">\(f(X)\)</span>. If the former, we use a prediction interval, and if the latter, we use a confidence interval. Prediction intervals will always be wider than confidence intervals because they account for the uncertainty associated with <span class="math inline">\(\epsilon\)</span>, the irreducible error.</p>
<ol start="6" style="list-style-type: decimal">
<li><strong><em>Is the relationship linear?</em></strong></li>
</ol>
<p>In previous section, we saw that residual plots can be used in order to
identify non-linearity. If the relationships are linear, then the residual plots should display no pattern. In the case of the <code>Advertising</code> data, we observe a non-linear effect in Figure 3.5, though this effect could also be observed in a residual plot. We also discussed the inclusion of transformations of the predictors in the linear regression model in order to accommodate non-linear relationships.</p>
<div class="figure">
<img src="img/95-Figure3.5-1.png" alt="" />
<p class="caption">Figure 3.5. A linear regression fit to sales using TV and radio as predictors</p>
</div>
<p>From the pattern of the residuals, we can see that there is a pronounced non-linear relationship in the data. The positive residuals (those visible above the surface), tend to lie along the 45-degree line, where <code>TV</code> and <code>radio</code> budgets are split evenly. The negative residuals (most not visible), tend to lie away from this line, where budgets are more lopsided.</p>
<ol start="7" style="list-style-type: decimal">
<li><strong><em>Is there synergy among the advertising media?</em></strong></li>
</ol>
<p>The standard linear regression model assumes an additive relationship
between the predictors and the response. An additive model is easy to interpret because the effect of each predictor on the response is unrelated to the values of the other predictors. However, the additive assumption may be unrealistic for certain data sets. A small p-value associated with the interaction term indicates the presence of such relationships.</p>
<p>Figure 3.5 suggested that the <code>Advertising</code> data may not be additive. Including an interaction term in the model results in a substantial increase in R2, from around 90% to almost 97%.</p>
</div>
</div>
<div id="chapter-3-lab-linear-regression" class="section level2" number="3.5">
<h2><span class="header-section-number">3.5</span> Chapter 3 Lab: Linear Regression</h2>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="linear-regression.html#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MASS)</span>
<span id="cb1-2"><a href="linear-regression.html#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb1-3"><a href="linear-regression.html#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Attaching package: &#39;MASS&#39;</span></span>
<span id="cb1-4"><a href="linear-regression.html#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; The following object is masked _by_ &#39;.GlobalEnv&#39;:</span></span>
<span id="cb1-5"><a href="linear-regression.html#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb1-6"><a href="linear-regression.html#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;     Boston</span></span>
<span id="cb1-7"><a href="linear-regression.html#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ISLR)</span></code></pre></div>
<div id="simple-linear-regression-1" class="section level3" number="3.5.1">
<h3><span class="header-section-number">3.5.1</span> Simple Linear Regression</h3>
<p>The Boston data set records <code>medv</code> (median house value) for 506 neighborhoods around Boston. We will seek to predict <code>medv</code> using 13 predictors such as <code>rm</code> (average number of rooms per house), <code>age</code> (average age of houses), and <code>lstat</code> (percent of households with low socioeconomic status).</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="linear-regression.html#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">fix</span>(Boston)</span>
<span id="cb2-2"><a href="linear-regression.html#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(Boston)</span>
<span id="cb2-3"><a href="linear-regression.html#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  [1] &quot;crim&quot;    &quot;zn&quot;      &quot;indus&quot;   &quot;chas&quot;    &quot;nox&quot;     &quot;rm&quot;      &quot;age&quot;    </span></span>
<span id="cb2-4"><a href="linear-regression.html#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  [8] &quot;dis&quot;     &quot;rad&quot;     &quot;tax&quot;     &quot;ptratio&quot; &quot;black&quot;   &quot;lstat&quot;   &quot;medv&quot;</span></span></code></pre></div>
<p>We will start by using the <code>lm()</code> function to fit a simple linear regression <code>lm()</code> model, with <code>medv</code> as the response and <code>lstat</code> as the predictor.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="linear-regression.html#cb3-1" aria-hidden="true" tabindex="-1"></a>lm.fit<span class="ot">=</span><span class="fu">lm</span>(medv<span class="sc">~</span>lstat,Boston)</span>
<span id="cb3-2"><a href="linear-regression.html#cb3-2" aria-hidden="true" tabindex="-1"></a>lm.fit</span>
<span id="cb3-3"><a href="linear-regression.html#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb3-4"><a href="linear-regression.html#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb3-5"><a href="linear-regression.html#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; lm(formula = medv ~ lstat, data = Boston)</span></span>
<span id="cb3-6"><a href="linear-regression.html#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb3-7"><a href="linear-regression.html#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb3-8"><a href="linear-regression.html#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; (Intercept)        lstat  </span></span>
<span id="cb3-9"><a href="linear-regression.html#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;       34.55        -0.95</span></span></code></pre></div>
<p>For more detailed information, we use <code>summary(lm.fit)</code>. This gives us p-values and standard errors for the coefficients, as well as the <span class="math inline">\(R^2\)</span> statistic and F-statistic for the model.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="linear-regression.html#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lm.fit)</span>
<span id="cb4-2"><a href="linear-regression.html#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb4-3"><a href="linear-regression.html#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb4-4"><a href="linear-regression.html#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; lm(formula = medv ~ lstat, data = Boston)</span></span>
<span id="cb4-5"><a href="linear-regression.html#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb4-6"><a href="linear-regression.html#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb4-7"><a href="linear-regression.html#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span id="cb4-8"><a href="linear-regression.html#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; -15.168  -3.990  -1.318   2.034  24.500 </span></span>
<span id="cb4-9"><a href="linear-regression.html#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb4-10"><a href="linear-regression.html#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb4-11"><a href="linear-regression.html#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb4-12"><a href="linear-regression.html#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; (Intercept) 34.55384    0.56263   61.41   &lt;2e-16 ***</span></span>
<span id="cb4-13"><a href="linear-regression.html#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; lstat       -0.95005    0.03873  -24.53   &lt;2e-16 ***</span></span>
<span id="cb4-14"><a href="linear-regression.html#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb4-15"><a href="linear-regression.html#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb4-16"><a href="linear-regression.html#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb4-17"><a href="linear-regression.html#cb4-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residual standard error: 6.216 on 504 degrees of freedom</span></span>
<span id="cb4-18"><a href="linear-regression.html#cb4-18" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.5441, Adjusted R-squared:  0.5432 </span></span>
<span id="cb4-19"><a href="linear-regression.html#cb4-19" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; F-statistic: 601.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16</span></span></code></pre></div>
<p>We can use <code>names()</code> function in order to find out what other pieces of information are store in <code>lm.fit</code>.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="linear-regression.html#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(lm.fit)</span>
<span id="cb5-2"><a href="linear-regression.html#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  [1] &quot;coefficients&quot;  &quot;residuals&quot;     &quot;effects&quot;       &quot;rank&quot;         </span></span>
<span id="cb5-3"><a href="linear-regression.html#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  [5] &quot;fitted.values&quot; &quot;assign&quot;        &quot;qr&quot;            &quot;df.residual&quot;  </span></span>
<span id="cb5-4"><a href="linear-regression.html#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  [9] &quot;xlevels&quot;       &quot;call&quot;          &quot;terms&quot;         &quot;model&quot;</span></span></code></pre></div>
<p>Assessing the coefficient of the model:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="linear-regression.html#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(lm.fit)</span>
<span id="cb6-2"><a href="linear-regression.html#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; (Intercept)       lstat </span></span>
<span id="cb6-3"><a href="linear-regression.html#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  34.5538409  -0.9500494</span></span></code></pre></div>
<p>Assessing the confidence interval for the coefficient estimates:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="linear-regression.html#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(lm.fit)</span>
<span id="cb7-2"><a href="linear-regression.html#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;                 2.5 %     97.5 %</span></span>
<span id="cb7-3"><a href="linear-regression.html#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; (Intercept) 33.448457 35.6592247</span></span>
<span id="cb7-4"><a href="linear-regression.html#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; lstat       -1.026148 -0.8739505</span></span></code></pre></div>
<p>To produce confidence interval an prediction intervals for the prediction of <code>medv</code> for a given value of <code>lstat</code>.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="linear-regression.html#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(lm.fit,<span class="fu">data.frame</span>(<span class="at">lstat=</span>(<span class="fu">c</span>(<span class="dv">5</span>,<span class="dv">10</span>,<span class="dv">15</span>))), <span class="at">interval=</span><span class="st">&quot;confidence&quot;</span>)</span>
<span id="cb8-2"><a href="linear-regression.html#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;        fit      lwr      upr</span></span>
<span id="cb8-3"><a href="linear-regression.html#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 1 29.80359 29.00741 30.59978</span></span>
<span id="cb8-4"><a href="linear-regression.html#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 2 25.05335 24.47413 25.63256</span></span>
<span id="cb8-5"><a href="linear-regression.html#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 3 20.30310 19.73159 20.87461</span></span></code></pre></div>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="linear-regression.html#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(lm.fit,<span class="fu">data.frame</span>(<span class="at">lstat=</span>(<span class="fu">c</span>(<span class="dv">5</span>,<span class="dv">10</span>,<span class="dv">15</span>))), <span class="at">interval=</span><span class="st">&quot;prediction&quot;</span>)</span>
<span id="cb9-2"><a href="linear-regression.html#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;        fit       lwr      upr</span></span>
<span id="cb9-3"><a href="linear-regression.html#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 1 29.80359 17.565675 42.04151</span></span>
<span id="cb9-4"><a href="linear-regression.html#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 2 25.05335 12.827626 37.27907</span></span>
<span id="cb9-5"><a href="linear-regression.html#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 3 20.30310  8.077742 32.52846</span></span></code></pre></div>
<p>We will now plot <code>medv</code> and <code>lstat</code> along with the least squares regression line using the <code>plot()</code> and <code>abline()</code> functions.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="linear-regression.html#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(Boston)</span>
<span id="cb10-2"><a href="linear-regression.html#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lstat,medv)</span>
<span id="cb10-3"><a href="linear-regression.html#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(lm.fit)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-10-1.png" width="672" />
Next we examine some diagnostic plots, several of which were discussed. Four diagnostic plots are automatically produced by applying the <code>plot()</code> function directly to the output from <code>lm()</code>.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="linear-regression.html#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb11-2"><a href="linear-regression.html#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lm.fit)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="linear-regression.html#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">predict</span>(lm.fit), <span class="fu">residuals</span>(lm.fit))</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-12-1.png" width="672" />
Alternatively, we can compute the residuals from a linear regression fit using the <code>residuals()</code> function. The function <code>rstudent()</code> will return the studentized residuals, and we can use this function to plot the residuals against the fitted values.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="linear-regression.html#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">predict</span>(lm.fit), <span class="fu">rstudent</span>(lm.fit))</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>On the basis of the residual plots, there is some evidence of non-linearity. Leverage statistics can be computed for any number of predictors using the <code>hatvalues()</code> function.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="linear-regression.html#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">hatvalues</span>(lm.fit))</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p>The <code>which.max()</code> function identifies the index of the largest element of a <code>which.max()</code> vector. In this case, it tells us which observation has the largest leverage statistic.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="linear-regression.html#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="fu">which.max</span>(<span class="fu">hatvalues</span>(lm.fit))</span>
<span id="cb15-2"><a href="linear-regression.html#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 375 </span></span>
<span id="cb15-3"><a href="linear-regression.html#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 375</span></span></code></pre></div>
</div>
<div id="multiple-linear-regression-1" class="section level3" number="3.5.2">
<h3><span class="header-section-number">3.5.2</span> Multiple Linear Regression</h3>
<p>In order to fit a multiple linear regression model using least squares, we again use the <code>lm()</code> function. The syntax <code>lm(y ∼ x1 + x2 + x3)</code> is used to fit a model with three predictors, <code>x1</code>, <code>x2</code>, and <code>x3.</code> The <code>summary()</code> function now outputs the regression coefficients for all the predictors.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="linear-regression.html#cb16-1" aria-hidden="true" tabindex="-1"></a>lm.fit<span class="ot">=</span><span class="fu">lm</span>(medv<span class="sc">~</span>lstat<span class="sc">+</span>age,<span class="at">data=</span>Boston)</span>
<span id="cb16-2"><a href="linear-regression.html#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lm.fit)</span>
<span id="cb16-3"><a href="linear-regression.html#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb16-4"><a href="linear-regression.html#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb16-5"><a href="linear-regression.html#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; lm(formula = medv ~ lstat + age, data = Boston)</span></span>
<span id="cb16-6"><a href="linear-regression.html#cb16-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb16-7"><a href="linear-regression.html#cb16-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb16-8"><a href="linear-regression.html#cb16-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span id="cb16-9"><a href="linear-regression.html#cb16-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; -15.981  -3.978  -1.283   1.968  23.158 </span></span>
<span id="cb16-10"><a href="linear-regression.html#cb16-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb16-11"><a href="linear-regression.html#cb16-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb16-12"><a href="linear-regression.html#cb16-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb16-13"><a href="linear-regression.html#cb16-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; (Intercept) 33.22276    0.73085  45.458  &lt; 2e-16 ***</span></span>
<span id="cb16-14"><a href="linear-regression.html#cb16-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; lstat       -1.03207    0.04819 -21.416  &lt; 2e-16 ***</span></span>
<span id="cb16-15"><a href="linear-regression.html#cb16-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; age          0.03454    0.01223   2.826  0.00491 ** </span></span>
<span id="cb16-16"><a href="linear-regression.html#cb16-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb16-17"><a href="linear-regression.html#cb16-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb16-18"><a href="linear-regression.html#cb16-18" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb16-19"><a href="linear-regression.html#cb16-19" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residual standard error: 6.173 on 503 degrees of freedom</span></span>
<span id="cb16-20"><a href="linear-regression.html#cb16-20" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.5513, Adjusted R-squared:  0.5495 </span></span>
<span id="cb16-21"><a href="linear-regression.html#cb16-21" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; F-statistic:   309 on 2 and 503 DF,  p-value: &lt; 2.2e-16</span></span></code></pre></div>
<p>To perform a regression using all of the predictors, we can use:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="linear-regression.html#cb17-1" aria-hidden="true" tabindex="-1"></a>lm.fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(medv <span class="sc">~</span>., <span class="at">data =</span> Boston)</span>
<span id="cb17-2"><a href="linear-regression.html#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lm.fit)</span>
<span id="cb17-3"><a href="linear-regression.html#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb17-4"><a href="linear-regression.html#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb17-5"><a href="linear-regression.html#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; lm(formula = medv ~ ., data = Boston)</span></span>
<span id="cb17-6"><a href="linear-regression.html#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb17-7"><a href="linear-regression.html#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb17-8"><a href="linear-regression.html#cb17-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span id="cb17-9"><a href="linear-regression.html#cb17-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; -15.595  -2.730  -0.518   1.777  26.199 </span></span>
<span id="cb17-10"><a href="linear-regression.html#cb17-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb17-11"><a href="linear-regression.html#cb17-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb17-12"><a href="linear-regression.html#cb17-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;               Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb17-13"><a href="linear-regression.html#cb17-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; (Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***</span></span>
<span id="cb17-14"><a href="linear-regression.html#cb17-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; crim        -1.080e-01  3.286e-02  -3.287 0.001087 ** </span></span>
<span id="cb17-15"><a href="linear-regression.html#cb17-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; zn           4.642e-02  1.373e-02   3.382 0.000778 ***</span></span>
<span id="cb17-16"><a href="linear-regression.html#cb17-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; indus        2.056e-02  6.150e-02   0.334 0.738288    </span></span>
<span id="cb17-17"><a href="linear-regression.html#cb17-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; chas         2.687e+00  8.616e-01   3.118 0.001925 ** </span></span>
<span id="cb17-18"><a href="linear-regression.html#cb17-18" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; nox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***</span></span>
<span id="cb17-19"><a href="linear-regression.html#cb17-19" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; rm           3.810e+00  4.179e-01   9.116  &lt; 2e-16 ***</span></span>
<span id="cb17-20"><a href="linear-regression.html#cb17-20" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; age          6.922e-04  1.321e-02   0.052 0.958229    </span></span>
<span id="cb17-21"><a href="linear-regression.html#cb17-21" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; dis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***</span></span>
<span id="cb17-22"><a href="linear-regression.html#cb17-22" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; rad          3.060e-01  6.635e-02   4.613 5.07e-06 ***</span></span>
<span id="cb17-23"><a href="linear-regression.html#cb17-23" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; tax         -1.233e-02  3.760e-03  -3.280 0.001112 ** </span></span>
<span id="cb17-24"><a href="linear-regression.html#cb17-24" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; ptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***</span></span>
<span id="cb17-25"><a href="linear-regression.html#cb17-25" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; black        9.312e-03  2.686e-03   3.467 0.000573 ***</span></span>
<span id="cb17-26"><a href="linear-regression.html#cb17-26" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; lstat       -5.248e-01  5.072e-02 -10.347  &lt; 2e-16 ***</span></span>
<span id="cb17-27"><a href="linear-regression.html#cb17-27" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb17-28"><a href="linear-regression.html#cb17-28" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb17-29"><a href="linear-regression.html#cb17-29" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb17-30"><a href="linear-regression.html#cb17-30" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residual standard error: 4.745 on 492 degrees of freedom</span></span>
<span id="cb17-31"><a href="linear-regression.html#cb17-31" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.7406, Adjusted R-squared:  0.7338 </span></span>
<span id="cb17-32"><a href="linear-regression.html#cb17-32" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; F-statistic: 108.1 on 13 and 492 DF,  p-value: &lt; 2.2e-16</span></span></code></pre></div>
<p>To perform a regression using all of the variables but one, we can use:</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="linear-regression.html#cb18-1" aria-hidden="true" tabindex="-1"></a>lm.fit1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(medv <span class="sc">~</span>. <span class="sc">-</span>age, <span class="at">data =</span> Boston)</span>
<span id="cb18-2"><a href="linear-regression.html#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lm.fit1)</span>
<span id="cb18-3"><a href="linear-regression.html#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb18-4"><a href="linear-regression.html#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb18-5"><a href="linear-regression.html#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; lm(formula = medv ~ . - age, data = Boston)</span></span>
<span id="cb18-6"><a href="linear-regression.html#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb18-7"><a href="linear-regression.html#cb18-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb18-8"><a href="linear-regression.html#cb18-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;      Min       1Q   Median       3Q      Max </span></span>
<span id="cb18-9"><a href="linear-regression.html#cb18-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; -15.6054  -2.7313  -0.5188   1.7601  26.2243 </span></span>
<span id="cb18-10"><a href="linear-regression.html#cb18-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb18-11"><a href="linear-regression.html#cb18-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb18-12"><a href="linear-regression.html#cb18-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;               Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb18-13"><a href="linear-regression.html#cb18-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; (Intercept)  36.436927   5.080119   7.172 2.72e-12 ***</span></span>
<span id="cb18-14"><a href="linear-regression.html#cb18-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; crim         -0.108006   0.032832  -3.290 0.001075 ** </span></span>
<span id="cb18-15"><a href="linear-regression.html#cb18-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; zn            0.046334   0.013613   3.404 0.000719 ***</span></span>
<span id="cb18-16"><a href="linear-regression.html#cb18-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; indus         0.020562   0.061433   0.335 0.737989    </span></span>
<span id="cb18-17"><a href="linear-regression.html#cb18-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; chas          2.689026   0.859598   3.128 0.001863 ** </span></span>
<span id="cb18-18"><a href="linear-regression.html#cb18-18" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; nox         -17.713540   3.679308  -4.814 1.97e-06 ***</span></span>
<span id="cb18-19"><a href="linear-regression.html#cb18-19" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; rm            3.814394   0.408480   9.338  &lt; 2e-16 ***</span></span>
<span id="cb18-20"><a href="linear-regression.html#cb18-20" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; dis          -1.478612   0.190611  -7.757 5.03e-14 ***</span></span>
<span id="cb18-21"><a href="linear-regression.html#cb18-21" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; rad           0.305786   0.066089   4.627 4.75e-06 ***</span></span>
<span id="cb18-22"><a href="linear-regression.html#cb18-22" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; tax          -0.012329   0.003755  -3.283 0.001099 ** </span></span>
<span id="cb18-23"><a href="linear-regression.html#cb18-23" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; ptratio      -0.952211   0.130294  -7.308 1.10e-12 ***</span></span>
<span id="cb18-24"><a href="linear-regression.html#cb18-24" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; black         0.009321   0.002678   3.481 0.000544 ***</span></span>
<span id="cb18-25"><a href="linear-regression.html#cb18-25" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; lstat        -0.523852   0.047625 -10.999  &lt; 2e-16 ***</span></span>
<span id="cb18-26"><a href="linear-regression.html#cb18-26" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb18-27"><a href="linear-regression.html#cb18-27" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb18-28"><a href="linear-regression.html#cb18-28" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb18-29"><a href="linear-regression.html#cb18-29" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residual standard error: 4.74 on 493 degrees of freedom</span></span>
<span id="cb18-30"><a href="linear-regression.html#cb18-30" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.7406, Adjusted R-squared:  0.7343 </span></span>
<span id="cb18-31"><a href="linear-regression.html#cb18-31" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; F-statistic: 117.3 on 12 and 493 DF,  p-value: &lt; 2.2e-16</span></span></code></pre></div>
<p>or Alternatively, the <code>update()</code> function can be used:</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="linear-regression.html#cb19-1" aria-hidden="true" tabindex="-1"></a>lm.fit1 <span class="ot">&lt;-</span> <span class="fu">update</span>(lm.fit,<span class="sc">~</span>. <span class="sc">-</span>age)</span>
<span id="cb19-2"><a href="linear-regression.html#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lm.fit1)</span>
<span id="cb19-3"><a href="linear-regression.html#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb19-4"><a href="linear-regression.html#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb19-5"><a href="linear-regression.html#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; lm(formula = medv ~ crim + zn + indus + chas + nox + rm + dis + </span></span>
<span id="cb19-6"><a href="linear-regression.html#cb19-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;     rad + tax + ptratio + black + lstat, data = Boston)</span></span>
<span id="cb19-7"><a href="linear-regression.html#cb19-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb19-8"><a href="linear-regression.html#cb19-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb19-9"><a href="linear-regression.html#cb19-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;      Min       1Q   Median       3Q      Max </span></span>
<span id="cb19-10"><a href="linear-regression.html#cb19-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; -15.6054  -2.7313  -0.5188   1.7601  26.2243 </span></span>
<span id="cb19-11"><a href="linear-regression.html#cb19-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb19-12"><a href="linear-regression.html#cb19-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb19-13"><a href="linear-regression.html#cb19-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;               Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb19-14"><a href="linear-regression.html#cb19-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; (Intercept)  36.436927   5.080119   7.172 2.72e-12 ***</span></span>
<span id="cb19-15"><a href="linear-regression.html#cb19-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; crim         -0.108006   0.032832  -3.290 0.001075 ** </span></span>
<span id="cb19-16"><a href="linear-regression.html#cb19-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; zn            0.046334   0.013613   3.404 0.000719 ***</span></span>
<span id="cb19-17"><a href="linear-regression.html#cb19-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; indus         0.020562   0.061433   0.335 0.737989    </span></span>
<span id="cb19-18"><a href="linear-regression.html#cb19-18" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; chas          2.689026   0.859598   3.128 0.001863 ** </span></span>
<span id="cb19-19"><a href="linear-regression.html#cb19-19" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; nox         -17.713540   3.679308  -4.814 1.97e-06 ***</span></span>
<span id="cb19-20"><a href="linear-regression.html#cb19-20" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; rm            3.814394   0.408480   9.338  &lt; 2e-16 ***</span></span>
<span id="cb19-21"><a href="linear-regression.html#cb19-21" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; dis          -1.478612   0.190611  -7.757 5.03e-14 ***</span></span>
<span id="cb19-22"><a href="linear-regression.html#cb19-22" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; rad           0.305786   0.066089   4.627 4.75e-06 ***</span></span>
<span id="cb19-23"><a href="linear-regression.html#cb19-23" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; tax          -0.012329   0.003755  -3.283 0.001099 ** </span></span>
<span id="cb19-24"><a href="linear-regression.html#cb19-24" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; ptratio      -0.952211   0.130294  -7.308 1.10e-12 ***</span></span>
<span id="cb19-25"><a href="linear-regression.html#cb19-25" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; black         0.009321   0.002678   3.481 0.000544 ***</span></span>
<span id="cb19-26"><a href="linear-regression.html#cb19-26" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; lstat        -0.523852   0.047625 -10.999  &lt; 2e-16 ***</span></span>
<span id="cb19-27"><a href="linear-regression.html#cb19-27" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb19-28"><a href="linear-regression.html#cb19-28" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb19-29"><a href="linear-regression.html#cb19-29" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb19-30"><a href="linear-regression.html#cb19-30" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residual standard error: 4.74 on 493 degrees of freedom</span></span>
<span id="cb19-31"><a href="linear-regression.html#cb19-31" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.7406, Adjusted R-squared:  0.7343 </span></span>
<span id="cb19-32"><a href="linear-regression.html#cb19-32" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; F-statistic: 117.3 on 12 and 493 DF,  p-value: &lt; 2.2e-16</span></span></code></pre></div>
</div>
<div id="interaction-terms" class="section level3" number="3.5.3">
<h3><span class="header-section-number">3.5.3</span> Interaction Terms</h3>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="linear-regression.html#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(medv<span class="sc">~</span>lstat<span class="sc">*</span>age,<span class="at">data=</span>Boston))</span>
<span id="cb20-2"><a href="linear-regression.html#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb20-3"><a href="linear-regression.html#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb20-4"><a href="linear-regression.html#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; lm(formula = medv ~ lstat * age, data = Boston)</span></span>
<span id="cb20-5"><a href="linear-regression.html#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb20-6"><a href="linear-regression.html#cb20-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb20-7"><a href="linear-regression.html#cb20-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span id="cb20-8"><a href="linear-regression.html#cb20-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; -15.806  -4.045  -1.333   2.085  27.552 </span></span>
<span id="cb20-9"><a href="linear-regression.html#cb20-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb20-10"><a href="linear-regression.html#cb20-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb20-11"><a href="linear-regression.html#cb20-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;               Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb20-12"><a href="linear-regression.html#cb20-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; (Intercept) 36.0885359  1.4698355  24.553  &lt; 2e-16 ***</span></span>
<span id="cb20-13"><a href="linear-regression.html#cb20-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; lstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***</span></span>
<span id="cb20-14"><a href="linear-regression.html#cb20-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; age         -0.0007209  0.0198792  -0.036   0.9711    </span></span>
<span id="cb20-15"><a href="linear-regression.html#cb20-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; lstat:age    0.0041560  0.0018518   2.244   0.0252 *  </span></span>
<span id="cb20-16"><a href="linear-regression.html#cb20-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb20-17"><a href="linear-regression.html#cb20-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb20-18"><a href="linear-regression.html#cb20-18" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb20-19"><a href="linear-regression.html#cb20-19" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residual standard error: 6.149 on 502 degrees of freedom</span></span>
<span id="cb20-20"><a href="linear-regression.html#cb20-20" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.5557, Adjusted R-squared:  0.5531 </span></span>
<span id="cb20-21"><a href="linear-regression.html#cb20-21" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; F-statistic: 209.3 on 3 and 502 DF,  p-value: &lt; 2.2e-16</span></span></code></pre></div>
</div>
<div id="non-linear-transformations-of-the-predictors" class="section level3" number="3.5.4">
<h3><span class="header-section-number">3.5.4</span> Non-linear Transformations of the Predictors</h3>
<p>The <code>lm()</code> function can also accommodate non-linear transformations of the predictors. For instance, given a predictor <span class="math inline">\(X\)</span>, we can create a predictor <span class="math inline">\(X^2\)</span> using <code>I(X^2)</code>. We now perform a regression of <code>medv</code> onto <code>lstat</code> and <code>lstat2</code>.</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="linear-regression.html#cb21-1" aria-hidden="true" tabindex="-1"></a>lm.fit2<span class="ot">=</span><span class="fu">lm</span>(medv<span class="sc">~</span>lstat<span class="sc">+</span><span class="fu">I</span>(lstat<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb21-2"><a href="linear-regression.html#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lm.fit2)</span>
<span id="cb21-3"><a href="linear-regression.html#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb21-4"><a href="linear-regression.html#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb21-5"><a href="linear-regression.html#cb21-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; lm(formula = medv ~ lstat + I(lstat^2))</span></span>
<span id="cb21-6"><a href="linear-regression.html#cb21-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb21-7"><a href="linear-regression.html#cb21-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb21-8"><a href="linear-regression.html#cb21-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;      Min       1Q   Median       3Q      Max </span></span>
<span id="cb21-9"><a href="linear-regression.html#cb21-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; -15.2834  -3.8313  -0.5295   2.3095  25.4148 </span></span>
<span id="cb21-10"><a href="linear-regression.html#cb21-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb21-11"><a href="linear-regression.html#cb21-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb21-12"><a href="linear-regression.html#cb21-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;              Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb21-13"><a href="linear-regression.html#cb21-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; (Intercept) 42.862007   0.872084   49.15   &lt;2e-16 ***</span></span>
<span id="cb21-14"><a href="linear-regression.html#cb21-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; lstat       -2.332821   0.123803  -18.84   &lt;2e-16 ***</span></span>
<span id="cb21-15"><a href="linear-regression.html#cb21-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; I(lstat^2)   0.043547   0.003745   11.63   &lt;2e-16 ***</span></span>
<span id="cb21-16"><a href="linear-regression.html#cb21-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb21-17"><a href="linear-regression.html#cb21-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb21-18"><a href="linear-regression.html#cb21-18" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb21-19"><a href="linear-regression.html#cb21-19" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residual standard error: 5.524 on 503 degrees of freedom</span></span>
<span id="cb21-20"><a href="linear-regression.html#cb21-20" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.6407, Adjusted R-squared:  0.6393 </span></span>
<span id="cb21-21"><a href="linear-regression.html#cb21-21" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; F-statistic: 448.5 on 2 and 503 DF,  p-value: &lt; 2.2e-16</span></span></code></pre></div>
<p>We use the <code>anova()</code> function to further quantify the extent to which the quadratic fit is superior to the linear fit.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="linear-regression.html#cb22-1" aria-hidden="true" tabindex="-1"></a>lm.fit<span class="ot">=</span><span class="fu">lm</span>(medv<span class="sc">~</span>lstat)</span>
<span id="cb22-2"><a href="linear-regression.html#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(lm.fit,lm.fit2)</span>
<span id="cb22-3"><a href="linear-regression.html#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Analysis of Variance Table</span></span>
<span id="cb22-4"><a href="linear-regression.html#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb22-5"><a href="linear-regression.html#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Model 1: medv ~ lstat</span></span>
<span id="cb22-6"><a href="linear-regression.html#cb22-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Model 2: medv ~ lstat + I(lstat^2)</span></span>
<span id="cb22-7"><a href="linear-regression.html#cb22-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   Res.Df   RSS Df Sum of Sq     F    Pr(&gt;F)    </span></span>
<span id="cb22-8"><a href="linear-regression.html#cb22-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 1    504 19472                                 </span></span>
<span id="cb22-9"><a href="linear-regression.html#cb22-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 2    503 15347  1    4125.1 135.2 &lt; 2.2e-16 ***</span></span>
<span id="cb22-10"><a href="linear-regression.html#cb22-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb22-11"><a href="linear-regression.html#cb22-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span></code></pre></div>
<p>Here <strong>Model 1</strong> represents the linear submodel containing only one predictor, <code>lstat</code>, while <strong>Model 2</strong> corresponds to the larger quadratic model that has two predictors, <code>lstat</code> and <code>lstat2</code>.</p>
<p>The <code>anova()</code> function performs a hypothesis test comparing the two models. The null hypothesis is that the two models fit the data equally well, and the alternative hypothesis is that the full model is superior. Here the F-statistic is 135 and the associated p-value is virtually zero. This provides very clear evidence that the model containing the predictors <code>lstat</code> and <code>lstat2</code> is far superior to the model that only contains the predictor <code>lstat</code>. This is not surprising, since earlier we saw evidence for non-linearity in the relationship between <code>medv</code> and <code>lstat</code>. If we type</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="linear-regression.html#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb23-2"><a href="linear-regression.html#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lm.fit2)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<p>then we see that when the <code>lstat2</code> term is included in the model, there is little discernible pattern in the residuals.</p>
<p>A better approach involves using the <code>poly()</code> function to create the polynomial within <code>lm()</code>. For example, the following command produces a fifth-order polynomial fit:</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="linear-regression.html#cb24-1" aria-hidden="true" tabindex="-1"></a>lm.fit5<span class="ot">=</span><span class="fu">lm</span>(medv<span class="sc">~</span><span class="fu">poly</span>(lstat,<span class="dv">5</span>))</span>
<span id="cb24-2"><a href="linear-regression.html#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lm.fit5)</span>
<span id="cb24-3"><a href="linear-regression.html#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb24-4"><a href="linear-regression.html#cb24-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb24-5"><a href="linear-regression.html#cb24-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; lm(formula = medv ~ poly(lstat, 5))</span></span>
<span id="cb24-6"><a href="linear-regression.html#cb24-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb24-7"><a href="linear-regression.html#cb24-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb24-8"><a href="linear-regression.html#cb24-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;      Min       1Q   Median       3Q      Max </span></span>
<span id="cb24-9"><a href="linear-regression.html#cb24-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; -13.5433  -3.1039  -0.7052   2.0844  27.1153 </span></span>
<span id="cb24-10"><a href="linear-regression.html#cb24-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb24-11"><a href="linear-regression.html#cb24-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb24-12"><a href="linear-regression.html#cb24-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;                  Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb24-13"><a href="linear-regression.html#cb24-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; (Intercept)       22.5328     0.2318  97.197  &lt; 2e-16 ***</span></span>
<span id="cb24-14"><a href="linear-regression.html#cb24-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; poly(lstat, 5)1 -152.4595     5.2148 -29.236  &lt; 2e-16 ***</span></span>
<span id="cb24-15"><a href="linear-regression.html#cb24-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; poly(lstat, 5)2   64.2272     5.2148  12.316  &lt; 2e-16 ***</span></span>
<span id="cb24-16"><a href="linear-regression.html#cb24-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; poly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***</span></span>
<span id="cb24-17"><a href="linear-regression.html#cb24-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; poly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***</span></span>
<span id="cb24-18"><a href="linear-regression.html#cb24-18" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; poly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***</span></span>
<span id="cb24-19"><a href="linear-regression.html#cb24-19" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb24-20"><a href="linear-regression.html#cb24-20" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb24-21"><a href="linear-regression.html#cb24-21" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb24-22"><a href="linear-regression.html#cb24-22" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residual standard error: 5.215 on 500 degrees of freedom</span></span>
<span id="cb24-23"><a href="linear-regression.html#cb24-23" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.6817, Adjusted R-squared:  0.6785 </span></span>
<span id="cb24-24"><a href="linear-regression.html#cb24-24" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; F-statistic: 214.2 on 5 and 500 DF,  p-value: &lt; 2.2e-16</span></span></code></pre></div>
<p>This suggests that including additional polynomial terms, up to fifth order, leads to an improvement in the model fit! However, further investigation of the data reveals that no polynomial terms beyond fifth order have significant p-values in a regression fit.</p>
<p>Of course, we are in no way restricted to using polynomial transformations of the predictors. Here we try a log transformation.</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="linear-regression.html#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(medv<span class="sc">~</span><span class="fu">log</span>(rm),<span class="at">data=</span>Boston))</span>
<span id="cb25-2"><a href="linear-regression.html#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb25-3"><a href="linear-regression.html#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb25-4"><a href="linear-regression.html#cb25-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; lm(formula = medv ~ log(rm), data = Boston)</span></span>
<span id="cb25-5"><a href="linear-regression.html#cb25-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb25-6"><a href="linear-regression.html#cb25-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb25-7"><a href="linear-regression.html#cb25-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span id="cb25-8"><a href="linear-regression.html#cb25-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; -19.487  -2.875  -0.104   2.837  39.816 </span></span>
<span id="cb25-9"><a href="linear-regression.html#cb25-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb25-10"><a href="linear-regression.html#cb25-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb25-11"><a href="linear-regression.html#cb25-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb25-12"><a href="linear-regression.html#cb25-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; (Intercept)  -76.488      5.028  -15.21   &lt;2e-16 ***</span></span>
<span id="cb25-13"><a href="linear-regression.html#cb25-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; log(rm)       54.055      2.739   19.73   &lt;2e-16 ***</span></span>
<span id="cb25-14"><a href="linear-regression.html#cb25-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb25-15"><a href="linear-regression.html#cb25-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb25-16"><a href="linear-regression.html#cb25-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb25-17"><a href="linear-regression.html#cb25-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residual standard error: 6.915 on 504 degrees of freedom</span></span>
<span id="cb25-18"><a href="linear-regression.html#cb25-18" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.4358, Adjusted R-squared:  0.4347 </span></span>
<span id="cb25-19"><a href="linear-regression.html#cb25-19" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; F-statistic: 389.3 on 1 and 504 DF,  p-value: &lt; 2.2e-16</span></span></code></pre></div>
</div>
<div id="qualitative-predictors" class="section level3" number="3.5.5">
<h3><span class="header-section-number">3.5.5</span> Qualitative Predictors</h3>
<p>We will now examine the <code>Carseats</code> data, which is part of the <code>ISLR2</code> library. We will attempt to predict Sales (child car seat sales) in 400 locations based on a number of predictors.</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="linear-regression.html#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(Carseats)</span>
<span id="cb26-2"><a href="linear-regression.html#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   Sales CompPrice Income Advertising Population Price ShelveLoc Age Education</span></span>
<span id="cb26-3"><a href="linear-regression.html#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 1  9.50       138     73          11        276   120       Bad  42        17</span></span>
<span id="cb26-4"><a href="linear-regression.html#cb26-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 2 11.22       111     48          16        260    83      Good  65        10</span></span>
<span id="cb26-5"><a href="linear-regression.html#cb26-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 3 10.06       113     35          10        269    80    Medium  59        12</span></span>
<span id="cb26-6"><a href="linear-regression.html#cb26-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 4  7.40       117    100           4        466    97    Medium  55        14</span></span>
<span id="cb26-7"><a href="linear-regression.html#cb26-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 5  4.15       141     64           3        340   128       Bad  38        13</span></span>
<span id="cb26-8"><a href="linear-regression.html#cb26-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 6 10.81       124    113          13        501    72       Bad  78        16</span></span>
<span id="cb26-9"><a href="linear-regression.html#cb26-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   Urban  US</span></span>
<span id="cb26-10"><a href="linear-regression.html#cb26-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 1   Yes Yes</span></span>
<span id="cb26-11"><a href="linear-regression.html#cb26-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 2   Yes Yes</span></span>
<span id="cb26-12"><a href="linear-regression.html#cb26-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 3   Yes Yes</span></span>
<span id="cb26-13"><a href="linear-regression.html#cb26-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 4   Yes Yes</span></span>
<span id="cb26-14"><a href="linear-regression.html#cb26-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 5   Yes  No</span></span>
<span id="cb26-15"><a href="linear-regression.html#cb26-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 6    No Yes</span></span></code></pre></div>
<p>The <code>Carseats</code> data includes qualitative predictors such as <code>Shelveloc</code>, an indicator
of the quality of the shelving location—that is, the space within a store in which the car seat is displayed—at each location.</p>
<p>The predictor Shelveloc takes on three possible values: <em>Bad</em>, <em>Medium</em>, and <em>Good</em>. Given a qualitative variable such as <code>Shelveloc</code>, R generates dummy variables automatically. Below we fit a multiple regression model that includes some interaction terms.</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="linear-regression.html#cb27-1" aria-hidden="true" tabindex="-1"></a>lm.fit<span class="ot">=</span><span class="fu">lm</span>(Sales<span class="sc">~</span>.<span class="sc">+</span>Income<span class="sc">:</span>Advertising<span class="sc">+</span>Price<span class="sc">:</span>Age,<span class="at">data=</span>Carseats)</span>
<span id="cb27-2"><a href="linear-regression.html#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lm.fit)</span>
<span id="cb27-3"><a href="linear-regression.html#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb27-4"><a href="linear-regression.html#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb27-5"><a href="linear-regression.html#cb27-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; lm(formula = Sales ~ . + Income:Advertising + Price:Age, data = Carseats)</span></span>
<span id="cb27-6"><a href="linear-regression.html#cb27-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb27-7"><a href="linear-regression.html#cb27-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb27-8"><a href="linear-regression.html#cb27-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span id="cb27-9"><a href="linear-regression.html#cb27-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; -2.9208 -0.7503  0.0177  0.6754  3.3413 </span></span>
<span id="cb27-10"><a href="linear-regression.html#cb27-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb27-11"><a href="linear-regression.html#cb27-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb27-12"><a href="linear-regression.html#cb27-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;                      Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb27-13"><a href="linear-regression.html#cb27-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; (Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***</span></span>
<span id="cb27-14"><a href="linear-regression.html#cb27-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; CompPrice           0.0929371  0.0041183  22.567  &lt; 2e-16 ***</span></span>
<span id="cb27-15"><a href="linear-regression.html#cb27-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Income              0.0108940  0.0026044   4.183 3.57e-05 ***</span></span>
<span id="cb27-16"><a href="linear-regression.html#cb27-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Advertising         0.0702462  0.0226091   3.107 0.002030 ** </span></span>
<span id="cb27-17"><a href="linear-regression.html#cb27-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Population          0.0001592  0.0003679   0.433 0.665330    </span></span>
<span id="cb27-18"><a href="linear-regression.html#cb27-18" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Price              -0.1008064  0.0074399 -13.549  &lt; 2e-16 ***</span></span>
<span id="cb27-19"><a href="linear-regression.html#cb27-19" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; ShelveLocGood       4.8486762  0.1528378  31.724  &lt; 2e-16 ***</span></span>
<span id="cb27-20"><a href="linear-regression.html#cb27-20" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; ShelveLocMedium     1.9532620  0.1257682  15.531  &lt; 2e-16 ***</span></span>
<span id="cb27-21"><a href="linear-regression.html#cb27-21" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Age                -0.0579466  0.0159506  -3.633 0.000318 ***</span></span>
<span id="cb27-22"><a href="linear-regression.html#cb27-22" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Education          -0.0208525  0.0196131  -1.063 0.288361    </span></span>
<span id="cb27-23"><a href="linear-regression.html#cb27-23" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; UrbanYes            0.1401597  0.1124019   1.247 0.213171    </span></span>
<span id="cb27-24"><a href="linear-regression.html#cb27-24" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; USYes              -0.1575571  0.1489234  -1.058 0.290729    </span></span>
<span id="cb27-25"><a href="linear-regression.html#cb27-25" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Income:Advertising  0.0007510  0.0002784   2.698 0.007290 ** </span></span>
<span id="cb27-26"><a href="linear-regression.html#cb27-26" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Price:Age           0.0001068  0.0001333   0.801 0.423812    </span></span>
<span id="cb27-27"><a href="linear-regression.html#cb27-27" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb27-28"><a href="linear-regression.html#cb27-28" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb27-29"><a href="linear-regression.html#cb27-29" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb27-30"><a href="linear-regression.html#cb27-30" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residual standard error: 1.011 on 386 degrees of freedom</span></span>
<span id="cb27-31"><a href="linear-regression.html#cb27-31" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.8761, Adjusted R-squared:  0.8719 </span></span>
<span id="cb27-32"><a href="linear-regression.html#cb27-32" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; F-statistic:   210 on 13 and 386 DF,  p-value: &lt; 2.2e-16</span></span></code></pre></div>
<p>The contrasts() function returns the coding that R uses for the dummy <code>contrasts()</code> variables.</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="linear-regression.html#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(Carseats)</span>
<span id="cb28-2"><a href="linear-regression.html#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="fu">contrasts</span>(ShelveLoc)</span>
<span id="cb28-3"><a href="linear-regression.html#cb28-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;        Good Medium</span></span>
<span id="cb28-4"><a href="linear-regression.html#cb28-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Bad       0      0</span></span>
<span id="cb28-5"><a href="linear-regression.html#cb28-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Good      1      0</span></span>
<span id="cb28-6"><a href="linear-regression.html#cb28-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Medium    0      1</span></span></code></pre></div>
<p>R has created a <code>ShelveLocGood</code> dummy variable that takes on a value of 1 if the shelving location is good, and 0 otherwise. It has also created a <code>ShelveLocMedium</code> dummy variable that equals 1 if the shelving location is medium, and 0 otherwise. A bad shelving location corresponds to a zero for each of the two dummy variables.</p>
<p>The fact that the coefficient for <code>ShelveLocGood</code> in the regression output is positive indicates that a good shelving location is associated with high sales (relative to a bad location). And <code>ShelveLocMedium</code> has a smaller positive coefficient, indicating that a
medium shelving location is associated with higher sales than a bad shelving
location but lower sales than a good shelving location.</p>
</div>
<div id="writing-functions" class="section level3" number="3.5.6">
<h3><span class="header-section-number">3.5.6</span> Writing Functions</h3>
<p>In order to write our own function, for instance, below we provide a simple function that reads in the <code>ISLR2</code> and <code>MASS</code> libraries, called
<code>LoadLibraries()</code>.</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="linear-regression.html#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="co">#create the function</span></span>
<span id="cb29-2"><a href="linear-regression.html#cb29-2" aria-hidden="true" tabindex="-1"></a>LoadLibraries<span class="ot">=</span><span class="cf">function</span>(){</span>
<span id="cb29-3"><a href="linear-regression.html#cb29-3" aria-hidden="true" tabindex="-1"></a> <span class="fu">library</span>(ISLR)</span>
<span id="cb29-4"><a href="linear-regression.html#cb29-4" aria-hidden="true" tabindex="-1"></a> <span class="fu">library</span>(MASS)</span>
<span id="cb29-5"><a href="linear-regression.html#cb29-5" aria-hidden="true" tabindex="-1"></a> <span class="fu">print</span>(<span class="st">&quot;The libraries have been loaded.&quot;</span>)</span>
<span id="cb29-6"><a href="linear-regression.html#cb29-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb29-7"><a href="linear-regression.html#cb29-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-8"><a href="linear-regression.html#cb29-8" aria-hidden="true" tabindex="-1"></a><span class="co"># execute the function</span></span>
<span id="cb29-9"><a href="linear-regression.html#cb29-9" aria-hidden="true" tabindex="-1"></a><span class="fu">LoadLibraries</span>()</span>
<span id="cb29-10"><a href="linear-regression.html#cb29-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] &quot;The libraries have been loaded.&quot;</span></span></code></pre></div>
</div>
</div>
<div id="exercises-applied" class="section level2" number="3.6">
<h2><span class="header-section-number">3.6</span> Exercises (Applied)</h2>
<div id="question-1" class="section level3 unnumbered">
<h3>Question 1</h3>
<p>This question involves the use of simple linear regression on the <code>Auto</code> data set.</p>
<div id="a-use-the-lm-function-to-perform-a-simple-linear-regression-with-mpg-as-the-response-and-horsepower-as-the-predictor.-use-the-summary-function-to-print-the-results.-comment-on-the-output." class="section level4 unnumbered">
<h4>(a) Use the <code>lm()</code> function to perform a simple linear regression with <code>mpg</code> as the response and <code>horsepower</code> as the predictor. Use the <code>summary()</code> function to print the results. Comment on the output.</h4>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="linear-regression.html#cb30-1" aria-hidden="true" tabindex="-1"></a>mpg.fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> horsepower, <span class="at">data =</span> Auto)</span>
<span id="cb30-2"><a href="linear-regression.html#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mpg.fit)</span>
<span id="cb30-3"><a href="linear-regression.html#cb30-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb30-4"><a href="linear-regression.html#cb30-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb30-5"><a href="linear-regression.html#cb30-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; lm(formula = mpg ~ horsepower, data = Auto)</span></span>
<span id="cb30-6"><a href="linear-regression.html#cb30-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb30-7"><a href="linear-regression.html#cb30-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb30-8"><a href="linear-regression.html#cb30-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;      Min       1Q   Median       3Q      Max </span></span>
<span id="cb30-9"><a href="linear-regression.html#cb30-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; -13.5710  -3.2592  -0.3435   2.7630  16.9240 </span></span>
<span id="cb30-10"><a href="linear-regression.html#cb30-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb30-11"><a href="linear-regression.html#cb30-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb30-12"><a href="linear-regression.html#cb30-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;              Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb30-13"><a href="linear-regression.html#cb30-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; (Intercept) 39.935861   0.717499   55.66   &lt;2e-16 ***</span></span>
<span id="cb30-14"><a href="linear-regression.html#cb30-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; horsepower  -0.157845   0.006446  -24.49   &lt;2e-16 ***</span></span>
<span id="cb30-15"><a href="linear-regression.html#cb30-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb30-16"><a href="linear-regression.html#cb30-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb30-17"><a href="linear-regression.html#cb30-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb30-18"><a href="linear-regression.html#cb30-18" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residual standard error: 4.906 on 390 degrees of freedom</span></span>
<span id="cb30-19"><a href="linear-regression.html#cb30-19" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.6059, Adjusted R-squared:  0.6049 </span></span>
<span id="cb30-20"><a href="linear-regression.html#cb30-20" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; F-statistic: 599.7 on 1 and 390 DF,  p-value: &lt; 2.2e-16</span></span></code></pre></div>
<p>In order to interpret the result, a set of following questions are set up to better investigate the model:</p>
<div id="i.-is-there-a-relationship-between-the-predictor-and-the-response" class="section level5 unnumbered">
<h5>i. <em>Is there a relationship between the predictor and the response?</em></h5>
<p>This question can be answered by fitting the simple linear regression model of <code>mpg</code> on <code>horsepower</code> and testing the null hypothesis:</p>
<p><span class="math display">\[
H_0: \beta_j = 0
\]</span></p>
<p>The p-value for the <code>horsepower</code> variable is very small (&lt;&lt;0.05), so there is strong evidence to believe that <code>horsepower</code> is associated with <code>mpg</code>. Therefore, there is a relationship between the predictor and response.</p>
</div>
<div id="ii.-how-strong-is-the-relationship-between-the-predictor-and-the-response" class="section level5 unnumbered">
<h5>ii. <em>How strong is the relationship between the predictor and the response?</em></h5>
<p>There are 2 measures of model accuracy to evaluate the degree of relationship between the predictor and the response, which are: <strong>RSE</strong> and <span class="math inline">\(R^2\)</span> <strong>statistic</strong>.</p>
<p>For the <code>Auto</code> data, the RSE is:</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="linear-regression.html#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mpg.fit)<span class="sc">$</span>sigma</span>
<span id="cb31-2"><a href="linear-regression.html#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 4.905757</span></span></code></pre></div>
<p>The RSE is different (good or bad) in the sense that it takes on the units of y, but we can divide this by mean y to get the <strong>percentage error</strong>:</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="linear-regression.html#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mpg.fit)<span class="sc">$</span>sigma<span class="sc">/</span><span class="fu">mean</span>(Auto<span class="sc">$</span>mpg)</span>
<span id="cb32-2"><a href="linear-regression.html#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.2092371</span></span></code></pre></div>
<p>So the percentage error = 20.92%.</p>
<p>Second, the <span class="math inline">\(R^2\)</span> of the linear model, which can be thought of as “the percentage of variability in the response that is explained by the predictor,” is given by:</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="linear-regression.html#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mpg.fit)<span class="sc">$</span>r.squared</span>
<span id="cb33-2"><a href="linear-regression.html#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.6059483</span></span></code></pre></div>
<p>So the <code>horsepower</code> explains 60.59% of the variance in <code>mpg</code>.</p>
</div>
<div id="iii.-is-the-relationship-between-the-predictor-and-the-response-positive-or-negative" class="section level5 unnumbered">
<h5>iii. <em>Is the relationship between the predictor and the response positive or negative?</em></h5>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="linear-regression.html#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coefficients</span>(mpg.fit)</span>
<span id="cb34-2"><a href="linear-regression.html#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; (Intercept)  horsepower </span></span>
<span id="cb34-3"><a href="linear-regression.html#cb34-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  39.9358610  -0.1578447</span></span></code></pre></div>
<p>The relationship is negative between <code>mpg</code> and <code>horsepower</code> as the coefficient estimate is -0.1578447.</p>
</div>
<div id="iv.-what-is-the-predicted-mpg-associated-with-a-horsepower-of-98-what-are-the-associated-95-confidence-and-prediction-intervals" class="section level5 unnumbered">
<h5>iv. <em>What is the predicted <code>mpg</code> associated with a <code>horsepower</code> of 98? What are the associated 95% confidence and prediction intervals?</em></h5>
<p>The confidence interval:</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="linear-regression.html#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(mpg.fit, <span class="fu">data.frame</span>(<span class="at">horsepower =</span> <span class="dv">98</span>), <span class="at">interval =</span> <span class="st">&quot;confidence&quot;</span>, <span class="at">level =</span> <span class="fl">0.95</span>)</span>
<span id="cb35-2"><a href="linear-regression.html#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;        fit      lwr      upr</span></span>
<span id="cb35-3"><a href="linear-regression.html#cb35-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 1 24.46708 23.97308 24.96108</span></span></code></pre></div>
<p>The prediction interval:</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="linear-regression.html#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(mpg.fit, <span class="fu">data.frame</span>(<span class="at">horsepower =</span> <span class="dv">98</span>), <span class="at">interval =</span> <span class="st">&quot;prediction&quot;</span>, <span class="at">level =</span> <span class="fl">0.95</span>)</span>
<span id="cb36-2"><a href="linear-regression.html#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;        fit     lwr      upr</span></span>
<span id="cb36-3"><a href="linear-regression.html#cb36-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 1 24.46708 14.8094 34.12476</span></span></code></pre></div>
<p>The prediction interval is wider than the confidence interval as we would expect.</p>
</div>
</div>
<div id="b-plot-the-response-and-the-predictor.-use-the-abline-function-to-display-the-least-squares-regression-line." class="section level4 unnumbered">
<h4>(b) Plot the response and the predictor. Use the <code>abline()</code> function to display the least squares regression line.</h4>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="linear-regression.html#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(Auto<span class="sc">$</span>horsepower,Auto<span class="sc">$</span>mpg,<span class="at">xlab =</span> <span class="st">&quot;horsepower&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;mpg&quot;</span>,<span class="at">main =</span> <span class="st">&quot;Scatterplot of mpg vs. horsepower&quot;</span>)</span>
<span id="cb37-2"><a href="linear-regression.html#cb37-2" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(mpg.fit,<span class="at">col=</span><span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-37-1.png" width="672" /></p>
</div>
<div id="c-use-the-plot-function-to-produce-diagnostic-plots-of-the-least-squares-regression-fit.-comment-on-any-problems-you-see-with-the-fit." class="section level4 unnumbered">
<h4>(c) Use the <code>plot()</code> function to produce diagnostic plots of the least squares regression fit. Comment on any problems you see with the fit.</h4>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb38-1"><a href="linear-regression.html#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb38-2"><a href="linear-regression.html#cb38-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(mpg.fit)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-38-1.png" width="672" /></p>
<p>The diagnostic plots show residuals in four different ways. Let’s take a look at the first type of plot:</p>
<div id="i-residuals-vs-fitted" class="section level5 unnumbered">
<h5><strong>(i) Residuals vs Fitted</strong></h5>
<p>This plot shows if residuals have non-linear patterns.</p>
<p>In this case, The plot of residuals versus fitted values indicates the presence of non linearity in the data.</p>
</div>
<div id="ii-normal-q-q" class="section level5 unnumbered">
<h5><strong>(ii) Normal Q-Q</strong></h5>
<p>This plot shows if residuals are normally distributed. Do residuals follow a straight line well or do they deviate severely? It’s good to see that residuals in this model are lined well on the straight dashed line.</p>
</div>
<div id="iii-scale-location" class="section level5 unnumbered">
<h5><strong>(iii) Scale-Location</strong></h5>
<p>This plot show how you can check the assumption of equal variance (homoscedasticity).</p>
<p>In this case, there also appears to be non-constant variance in the error terms (heteroscedasticity), but this could be corrected to an extent when trying a quadratic fit. If not, transformations such as <span class="math inline">\(log(y)\)</span> or <span class="math inline">\(\sqrt{y}\)</span> can shrink larger responses by a greater amount and reduce this issue.</p>
</div>
<div id="iv-residuals-vs-leverage" class="section level5 unnumbered">
<h5><strong>(iv) Residuals vs Leverage</strong></h5>
<p>This plot helps us to find influential cases (i.e., subjects) if any.</p>
<p>The plot of standardized residuals versus leverage indicates the presence of a few outliers and a few high leverage points.</p>
</div>
</div>
</div>
<div id="question-2" class="section level3 unnumbered">
<h3>Question 2</h3>
<p>This question involves the use of multiple linear regression on the <code>Auto</code> data set.</p>
<div id="a-produce-a-scatterplot-matrix-which-includes-all-of-the-variables-in-the-data-set." class="section level4 unnumbered">
<h4>(a) Produce a scatterplot matrix which includes all of the variables in the data set.</h4>
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb39-1"><a href="linear-regression.html#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="fu">pairs</span>(Auto)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-39-1.png" width="672" /></p>
</div>
<div id="b-compute-the-matrix-of-correlations-between-the-variables-using-the-function-cor.-you-will-need-to-exclude-the-name-variable-which-is-qualitative." class="section level4 unnumbered">
<h4>(b) Compute the matrix of correlations between the variables using the function <code>cor()</code>. You will need to exclude the “name” variable, which is qualitative.</h4>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="linear-regression.html#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(Auto)</span>
<span id="cb40-2"><a href="linear-regression.html#cb40-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] &quot;mpg&quot;          &quot;cylinders&quot;    &quot;displacement&quot; &quot;horsepower&quot;   &quot;weight&quot;      </span></span>
<span id="cb40-3"><a href="linear-regression.html#cb40-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [6] &quot;acceleration&quot; &quot;year&quot;         &quot;origin&quot;       &quot;name&quot;</span></span></code></pre></div>
<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb41-1"><a href="linear-regression.html#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(Auto[<span class="sc">-</span><span class="dv">9</span>])</span>
<span id="cb41-2"><a href="linear-regression.html#cb41-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;                     mpg  cylinders displacement horsepower     weight</span></span>
<span id="cb41-3"><a href="linear-regression.html#cb41-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; mpg           1.0000000 -0.7776175   -0.8051269 -0.7784268 -0.8322442</span></span>
<span id="cb41-4"><a href="linear-regression.html#cb41-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; cylinders    -0.7776175  1.0000000    0.9508233  0.8429834  0.8975273</span></span>
<span id="cb41-5"><a href="linear-regression.html#cb41-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; displacement -0.8051269  0.9508233    1.0000000  0.8972570  0.9329944</span></span>
<span id="cb41-6"><a href="linear-regression.html#cb41-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; horsepower   -0.7784268  0.8429834    0.8972570  1.0000000  0.8645377</span></span>
<span id="cb41-7"><a href="linear-regression.html#cb41-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; weight       -0.8322442  0.8975273    0.9329944  0.8645377  1.0000000</span></span>
<span id="cb41-8"><a href="linear-regression.html#cb41-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; acceleration  0.4233285 -0.5046834   -0.5438005 -0.6891955 -0.4168392</span></span>
<span id="cb41-9"><a href="linear-regression.html#cb41-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; year          0.5805410 -0.3456474   -0.3698552 -0.4163615 -0.3091199</span></span>
<span id="cb41-10"><a href="linear-regression.html#cb41-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; origin        0.5652088 -0.5689316   -0.6145351 -0.4551715 -0.5850054</span></span>
<span id="cb41-11"><a href="linear-regression.html#cb41-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;              acceleration       year     origin</span></span>
<span id="cb41-12"><a href="linear-regression.html#cb41-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; mpg             0.4233285  0.5805410  0.5652088</span></span>
<span id="cb41-13"><a href="linear-regression.html#cb41-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; cylinders      -0.5046834 -0.3456474 -0.5689316</span></span>
<span id="cb41-14"><a href="linear-regression.html#cb41-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; displacement   -0.5438005 -0.3698552 -0.6145351</span></span>
<span id="cb41-15"><a href="linear-regression.html#cb41-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; horsepower     -0.6891955 -0.4163615 -0.4551715</span></span>
<span id="cb41-16"><a href="linear-regression.html#cb41-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; weight         -0.4168392 -0.3091199 -0.5850054</span></span>
<span id="cb41-17"><a href="linear-regression.html#cb41-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; acceleration    1.0000000  0.2903161  0.2127458</span></span>
<span id="cb41-18"><a href="linear-regression.html#cb41-18" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; year            0.2903161  1.0000000  0.1815277</span></span>
<span id="cb41-19"><a href="linear-regression.html#cb41-19" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; origin          0.2127458  0.1815277  1.0000000</span></span></code></pre></div>
</div>
<div id="c-use-the-lm-function-to-perform-a-multiple-linear-regression-with-mpg-as-the-response-and-all-other-variables-except-name-as-the-predictors.-use-the-summary-function-to-print-the-results.-comment-on-the-output." class="section level4 unnumbered">
<h4>(c) Use the <code>lm()</code> function to perform a multiple linear regression with <code>mpg</code> as the response and all other variables except “name” as the predictors. Use the <code>summary()</code> function to print the results. Comment on the output.</h4>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="linear-regression.html#cb42-1" aria-hidden="true" tabindex="-1"></a>mpg.fit2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span>. <span class="sc">-</span>name, <span class="at">data=</span>Auto)</span>
<span id="cb42-2"><a href="linear-regression.html#cb42-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mpg.fit2)</span>
<span id="cb42-3"><a href="linear-regression.html#cb42-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb42-4"><a href="linear-regression.html#cb42-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb42-5"><a href="linear-regression.html#cb42-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; lm(formula = mpg ~ . - name, data = Auto)</span></span>
<span id="cb42-6"><a href="linear-regression.html#cb42-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb42-7"><a href="linear-regression.html#cb42-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb42-8"><a href="linear-regression.html#cb42-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span id="cb42-9"><a href="linear-regression.html#cb42-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; -9.5903 -2.1565 -0.1169  1.8690 13.0604 </span></span>
<span id="cb42-10"><a href="linear-regression.html#cb42-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb42-11"><a href="linear-regression.html#cb42-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb42-12"><a href="linear-regression.html#cb42-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;                Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb42-13"><a href="linear-regression.html#cb42-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; (Intercept)  -17.218435   4.644294  -3.707  0.00024 ***</span></span>
<span id="cb42-14"><a href="linear-regression.html#cb42-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; cylinders     -0.493376   0.323282  -1.526  0.12780    </span></span>
<span id="cb42-15"><a href="linear-regression.html#cb42-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; displacement   0.019896   0.007515   2.647  0.00844 ** </span></span>
<span id="cb42-16"><a href="linear-regression.html#cb42-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; horsepower    -0.016951   0.013787  -1.230  0.21963    </span></span>
<span id="cb42-17"><a href="linear-regression.html#cb42-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; weight        -0.006474   0.000652  -9.929  &lt; 2e-16 ***</span></span>
<span id="cb42-18"><a href="linear-regression.html#cb42-18" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; acceleration   0.080576   0.098845   0.815  0.41548    </span></span>
<span id="cb42-19"><a href="linear-regression.html#cb42-19" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; year           0.750773   0.050973  14.729  &lt; 2e-16 ***</span></span>
<span id="cb42-20"><a href="linear-regression.html#cb42-20" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; origin         1.426141   0.278136   5.127 4.67e-07 ***</span></span>
<span id="cb42-21"><a href="linear-regression.html#cb42-21" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb42-22"><a href="linear-regression.html#cb42-22" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb42-23"><a href="linear-regression.html#cb42-23" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb42-24"><a href="linear-regression.html#cb42-24" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residual standard error: 3.328 on 384 degrees of freedom</span></span>
<span id="cb42-25"><a href="linear-regression.html#cb42-25" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.8215, Adjusted R-squared:  0.8182 </span></span>
<span id="cb42-26"><a href="linear-regression.html#cb42-26" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; F-statistic: 252.4 on 7 and 384 DF,  p-value: &lt; 2.2e-16</span></span></code></pre></div>
<div id="i.-is-there-a-relationship-between-the-predictors-and-the-response" class="section level5 unnumbered">
<h5>i. Is there a relationship between the predictors and the response?</h5>
<p>We answer this question by performing an F-test, where we test the null hypothesis that all of the regression coefficients are zero:</p>
<p>$$
H_0:_1=_2=…=_p=0\</p>
<p>H_a: at ;least ;one ;_j
$$</p>
<p>The p-value is given at the bottom of the model summary (<code>p-value: &lt; 2.2e-16</code>), so it’s clear that the probability of the null hypothesis being true (given our data) is practically zero.</p>
<p>We reject the null hypothesis (and hence conclude that there is a relationship between the predictors and <code>mpg</code>).</p>
</div>
<div id="ii.-which-predictors-appear-to-have-a-statistically-significant-relationship-to-the-response" class="section level5 unnumbered">
<h5>ii. Which predictors appear to have a statistically significant relationship to the response?</h5>
<p>We can answer this question by checking the p-values associated with each predictor’s t-statistic. We may conclude that all predictors are statistically significant except <code>cylinders</code>, <code>horsepower</code> and <code>acceleration</code>.</p>
</div>
<div id="iii.-what-does-the-coefficient-for-the-year-variable-suggest" class="section level5 unnumbered">
<h5>iii. What does the coefficient for the <code>year</code> variable suggest?</h5>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb43-1"><a href="linear-regression.html#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(mpg.fit2)[<span class="dv">7</span>]</span>
<span id="cb43-2"><a href="linear-regression.html#cb43-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;      year </span></span>
<span id="cb43-3"><a href="linear-regression.html#cb43-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 0.7507727</span></span></code></pre></div>
<p>The coefficient ot the <code>year</code> variable suggests that the average effect of an increase of 1 year is an increase of 0.7507727 in <code>mpg</code> (all other predictors remaining constant). In other words, cars become more fuel efficient every year by almost 1 mpg / year.</p>
</div>
</div>
<div id="d-use-the-plot-function-to-produce-diagnostic-plots-of-the-linear-regression-fit.-comment-on-any-problems-you-see-with-the-fit." class="section level4 unnumbered">
<h4>(d) Use the <code>plot()</code> function to produce diagnostic plots of the linear regression fit. Comment on any problems you see with the fit.</h4>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="linear-regression.html#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb44-2"><a href="linear-regression.html#cb44-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(mpg.fit2)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-44-1.png" width="672" /></p>
<div id="i-residuals-vs-fitted-1" class="section level6 unnumbered">
<h6><strong>(i) Residuals vs Fitted</strong></h6>
<p>This plot shows if residuals have non-linear patterns.</p>
<p>In this case, The plot of residuals versus fitted values indicates the presence of mild non linearity in the data.</p>
</div>
<div id="ii-normal-q-q-1" class="section level6 unnumbered">
<h6><strong>(ii) Normal Q-Q</strong></h6>
<p>This plot shows if residuals are normally distributed. Do residuals follow a straight line well or do they deviate severely? It’s good to see that residuals in this model are lined well on the straight dashed line.</p>
</div>
<div id="iii-scale-location-1" class="section level6 unnumbered">
<h6><strong>(iii) Scale-Location</strong></h6>
<p>This plot show how you can check the assumption of equal variance (homoscedasticity).</p>
<p>In this case, there also appears to be non-constant variance in the error terms (heteroscedasticity), but this could be corrected to an extent when trying a quadratic fit. If not, transformations such as <span class="math inline">\(log(y)\)</span> or <span class="math inline">\(\sqrt{y}\)</span> can shrink larger responses by a greater amount and reduce this issue.</p>
</div>
<div id="iv-residuals-vs-leverage-1" class="section level6 unnumbered">
<h6><strong>(iv) Residuals vs Leverage</strong></h6>
<p>This plot helps us to find influential cases (i.e., subjects) if any.</p>
<p>We can see some evidence of observations (e.g. 14) with both high leverage and high residual statistics, that may be disproportionately influencing the regression predictions.</p>
</div>
</div>
<div id="e-use-the-and-symbols-to-fit-linear-regression-models-with-interaction-effects.-do-any-interactions-appear-to-be-statistically-significant" class="section level4" number="3.6.0.1">
<h4><span class="header-section-number">3.6.0.1</span> (e) Use the <code>*</code> and : symbols to fit linear regression models with interaction effects. Do any interactions appear to be statistically significant?</h4>
<p>Since we have relatively few predictors, we can test all interactions with <code>mpg ~ . * .</code> in the call to <code>lm()</code>:</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb45-1"><a href="linear-regression.html#cb45-1" aria-hidden="true" tabindex="-1"></a>mpg.fit3 <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span>.<span class="sc">*</span> ., <span class="at">data=</span>Auto[,<span class="sc">-</span><span class="dv">9</span>]) </span>
<span id="cb45-2"><a href="linear-regression.html#cb45-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mpg.fit3)</span>
<span id="cb45-3"><a href="linear-regression.html#cb45-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb45-4"><a href="linear-regression.html#cb45-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb45-5"><a href="linear-regression.html#cb45-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; lm(formula = mpg ~ . * ., data = Auto[, -9])</span></span>
<span id="cb45-6"><a href="linear-regression.html#cb45-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb45-7"><a href="linear-regression.html#cb45-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb45-8"><a href="linear-regression.html#cb45-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span id="cb45-9"><a href="linear-regression.html#cb45-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; -7.6303 -1.4481  0.0596  1.2739 11.1386 </span></span>
<span id="cb45-10"><a href="linear-regression.html#cb45-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb45-11"><a href="linear-regression.html#cb45-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb45-12"><a href="linear-regression.html#cb45-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;                             Estimate Std. Error t value Pr(&gt;|t|)   </span></span>
<span id="cb45-13"><a href="linear-regression.html#cb45-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; (Intercept)                3.548e+01  5.314e+01   0.668  0.50475   </span></span>
<span id="cb45-14"><a href="linear-regression.html#cb45-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; cylinders                  6.989e+00  8.248e+00   0.847  0.39738   </span></span>
<span id="cb45-15"><a href="linear-regression.html#cb45-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; displacement              -4.785e-01  1.894e-01  -2.527  0.01192 * </span></span>
<span id="cb45-16"><a href="linear-regression.html#cb45-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; horsepower                 5.034e-01  3.470e-01   1.451  0.14769   </span></span>
<span id="cb45-17"><a href="linear-regression.html#cb45-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; weight                     4.133e-03  1.759e-02   0.235  0.81442   </span></span>
<span id="cb45-18"><a href="linear-regression.html#cb45-18" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; acceleration              -5.859e+00  2.174e+00  -2.696  0.00735 **</span></span>
<span id="cb45-19"><a href="linear-regression.html#cb45-19" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; year                       6.974e-01  6.097e-01   1.144  0.25340   </span></span>
<span id="cb45-20"><a href="linear-regression.html#cb45-20" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; origin                    -2.090e+01  7.097e+00  -2.944  0.00345 **</span></span>
<span id="cb45-21"><a href="linear-regression.html#cb45-21" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; cylinders:displacement    -3.383e-03  6.455e-03  -0.524  0.60051   </span></span>
<span id="cb45-22"><a href="linear-regression.html#cb45-22" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; cylinders:horsepower       1.161e-02  2.420e-02   0.480  0.63157   </span></span>
<span id="cb45-23"><a href="linear-regression.html#cb45-23" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; cylinders:weight           3.575e-04  8.955e-04   0.399  0.69000   </span></span>
<span id="cb45-24"><a href="linear-regression.html#cb45-24" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; cylinders:acceleration     2.779e-01  1.664e-01   1.670  0.09584 . </span></span>
<span id="cb45-25"><a href="linear-regression.html#cb45-25" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; cylinders:year            -1.741e-01  9.714e-02  -1.793  0.07389 . </span></span>
<span id="cb45-26"><a href="linear-regression.html#cb45-26" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; cylinders:origin           4.022e-01  4.926e-01   0.816  0.41482   </span></span>
<span id="cb45-27"><a href="linear-regression.html#cb45-27" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; displacement:horsepower   -8.491e-05  2.885e-04  -0.294  0.76867   </span></span>
<span id="cb45-28"><a href="linear-regression.html#cb45-28" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; displacement:weight        2.472e-05  1.470e-05   1.682  0.09342 . </span></span>
<span id="cb45-29"><a href="linear-regression.html#cb45-29" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; displacement:acceleration -3.479e-03  3.342e-03  -1.041  0.29853   </span></span>
<span id="cb45-30"><a href="linear-regression.html#cb45-30" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; displacement:year          5.934e-03  2.391e-03   2.482  0.01352 * </span></span>
<span id="cb45-31"><a href="linear-regression.html#cb45-31" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; displacement:origin        2.398e-02  1.947e-02   1.232  0.21875   </span></span>
<span id="cb45-32"><a href="linear-regression.html#cb45-32" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; horsepower:weight         -1.968e-05  2.924e-05  -0.673  0.50124   </span></span>
<span id="cb45-33"><a href="linear-regression.html#cb45-33" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; horsepower:acceleration   -7.213e-03  3.719e-03  -1.939  0.05325 . </span></span>
<span id="cb45-34"><a href="linear-regression.html#cb45-34" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; horsepower:year           -5.838e-03  3.938e-03  -1.482  0.13916   </span></span>
<span id="cb45-35"><a href="linear-regression.html#cb45-35" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; horsepower:origin          2.233e-03  2.930e-02   0.076  0.93931   </span></span>
<span id="cb45-36"><a href="linear-regression.html#cb45-36" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; weight:acceleration        2.346e-04  2.289e-04   1.025  0.30596   </span></span>
<span id="cb45-37"><a href="linear-regression.html#cb45-37" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; weight:year               -2.245e-04  2.127e-04  -1.056  0.29182   </span></span>
<span id="cb45-38"><a href="linear-regression.html#cb45-38" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; weight:origin             -5.789e-04  1.591e-03  -0.364  0.71623   </span></span>
<span id="cb45-39"><a href="linear-regression.html#cb45-39" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; acceleration:year          5.562e-02  2.558e-02   2.174  0.03033 * </span></span>
<span id="cb45-40"><a href="linear-regression.html#cb45-40" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; acceleration:origin        4.583e-01  1.567e-01   2.926  0.00365 **</span></span>
<span id="cb45-41"><a href="linear-regression.html#cb45-41" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; year:origin                1.393e-01  7.399e-02   1.882  0.06062 . </span></span>
<span id="cb45-42"><a href="linear-regression.html#cb45-42" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb45-43"><a href="linear-regression.html#cb45-43" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb45-44"><a href="linear-regression.html#cb45-44" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb45-45"><a href="linear-regression.html#cb45-45" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residual standard error: 2.695 on 363 degrees of freedom</span></span>
<span id="cb45-46"><a href="linear-regression.html#cb45-46" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.8893, Adjusted R-squared:  0.8808 </span></span>
<span id="cb45-47"><a href="linear-regression.html#cb45-47" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; F-statistic: 104.2 on 28 and 363 DF,  p-value: &lt; 2.2e-16</span></span></code></pre></div>
<p>We can see the significant terms (at the 0.05 level) are those with at least one asterisk (*). It is probably unreasonable to use a significance level of 0.05 here, as we are testing such a large number of hypothesis, perhaps a lower threshold for significance (or a p-value correction using the p.adjust() function) would be appropriate.</p>
<p>Using the standard threshold of 0.05, the significant interaction terms are given by:
* <code>displacement:year</code>
* <code>acceleration:year</code>
* <code>acceleration:origin</code></p>
</div>
<div id="try-a-few-different-transformations-of-the-variables-such-as-logx-sqrtx-x2.-comment-on-your-findings." class="section level4 unnumbered">
<h4>Try a few different transformations of the variables, such as <span class="math inline">\(log(X)\)</span>, <span class="math inline">\(\sqrt{X}\)</span>, <span class="math inline">\(X^2\)</span>. Comment on your findings.</h4>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="linear-regression.html#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb46-2"><a href="linear-regression.html#cb46-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">log</span>(Auto<span class="sc">$</span>horsepower), Auto<span class="sc">$</span>mpg, <span class="at">xlab=</span> <span class="st">&quot;log(horsepower)&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;mpg&quot;</span>, <span class="at">main=</span><span class="st">&quot;Log Tranformation&quot;</span>)</span>
<span id="cb46-3"><a href="linear-regression.html#cb46-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">sqrt</span>(Auto<span class="sc">$</span>horsepower), Auto<span class="sc">$</span>mpg, <span class="at">xlab=</span> <span class="st">&quot;sqrt(horsepower)&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;mpg&quot;</span>, <span class="at">main=</span><span class="st">&quot;Square Root Transformation&quot;</span>)</span>
<span id="cb46-4"><a href="linear-regression.html#cb46-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>((Auto<span class="sc">$</span>horsepower)<span class="sc">^</span><span class="dv">2</span>, Auto<span class="sc">$</span>mpg, <span class="at">xlab=</span> <span class="st">&quot;horsepower&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;mpg&quot;</span>, <span class="at">main=</span><span class="st">&quot;X^2 Transformation&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-46-1.png" width="672" /></p>
<p>We limit ourselves to examining <code>horsepower</code> as sole predictor. It seems that the log transformation gives the most linear looking plot.</p>

</div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="statistical-learning.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="parts.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/ndleah/stat-learning/edit/main/02-linear-regression.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
