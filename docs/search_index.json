[["what-is-statistical-learning.html", "Statistics Learning Section 1 What is Statistical Learning? 1.1 Why Estimate f? 1.2 How Do We Estimate f? 1.3 The Tradeoff Between Prediction Accuracy and Model Interpretability 1.4 Supervised Vs. Unsupervised Learning 1.5 Regression Vs. Classification", " Statistics Learning Section 1 What is Statistical Learning? Statistical learning refers to a vast set of tools for understanding data. There are 2 types of statistical learning - supervised and unsupervised: Supervised learning is when you have a label for each data point, which mean it involves building a model that can predict an output based on one or more inputs. Unsupervised learning is when you dont have a label for each data point, where there are inputs but no supervising output. In statistical learning, input variables \\((X_{n})\\) are typically denoted by features, predictors, independent variables or variables while output variable \\((Y)\\) often called dependent variable or response. To assess the relationship between predictors \\(X_{1}, X_{2}, ...,X_{p}\\), we have the equation as (1.1): \\[\\begin{equation} \\Large Y=f(X) + \\epsilon \\tag{1.1} \\end{equation}\\] Whereas: \\(f\\) is fixed but unknown function of \\(X_{1},...,X_{p}\\) and \\(\\epsilon\\) is a random error term, which is independent of \\(X\\) and has mean zero. In essence, statistical learning refers to a set of approaches for estimating \\(f\\). 1.1 Why Estimate f? There are 2 main reasons: prediction and inference. 1.1.1 Prediction Hypothetically, lets say we have the error term averages to 0, predicting \\(Y\\) can be assessed using this equation (1.2): \\[\\begin{equation} \\Large \\hat{Y} = \\hat{f}(X) \\tag{1.2} \\end{equation}\\] Whereas: \\(\\hat{f}\\) represents the estimate for \\(f\\) \\(\\hat{Y}\\) represents the resulting prediction for \\(Y\\) The accuracy of \\(\\hat{Y}\\) as a prediction for \\(Y\\) depends on 2 quantities: reducible error and irreducible error. Reducible error Errors arising from the mismatch between \\(\\hat{f}\\) and \\(f\\). Can be improved by choosing a better model. Usually caused by Variance Error/Bias Error. Irreducible error Errors which cant be removed no matter what algorithm you apply. These errors are caused by unknown variables that are affecting the output variable but are not one of the input variable while designing the model. 1.1.1.1 Inference Necessary questions need to be asked in order to further understand the relationship between predictors \\((X_{n})\\) and outcome \\((Y)\\): Which predictors are associated with response? Only a small fraction of the available predictors are associated with the response. What is the relationship between predictors and response? The relationship between predictors and response is not always linear. Can the relationship between predictors and response be explained by the linear model or it is more complicated? The model can explain the relationship between predictors and response if the model is able to predict the response based on the predictors. 1.2 How Do We Estimate f? In order to estimate \\(f\\), our goal is to apply a statistical learning method to the training data. Broadly speaking, most statistical learning methods for this task can be characterized as either parametric or non-parametric. 1.2.1 Parametric Methods Parametric methods (model-based approach) are those that are able to estimate the parameters of the model based on the training data. It involves a two-step model-based approach: First, we make an assumption about the functional form, or shape, of \\(f\\). In other words, we need to choose the model that best fits the data. After the model has been selected, we need a procedure that uses the training data to fit or train the model. Potential disadvantages of parametric methods is that the model will not usually match the true \\(f\\). This can be avoid by choosing a more flexible models that can fit many possible functions for \\(f\\) forms and usually require greater number of parameters. Example fitting models for parametric methods (linear): Ordinary Least Squares (OLS), Lasso. 1.2.2 Non-Parametric Methods Non-parametric methods (model-free approach) seek an estimate of \\(f\\) without make explicit assumptions about the functional form of of \\(f\\). Major disadvantages of this approach is that a very large number of observations is required in order to obtain an accurate estimate for \\(f\\). Example fitting models for non-parametric methods: smooth thin-plate spline fit and rough thin-plate spline fit. 1.3 The Tradeoff Between Prediction Accuracy and Model Interpretability Figure 1.1: Tradeoff Between Prediction Accuracy and Model Interpretability Why would we ever choose to use a more restrictive method instead of a very flexible approach? If we are mainly interested in the interpretability of the model, we would rather use a more flexible model. This is because the flexibility of the model is usually better than the interpretability of the model. In contrast, if we are interested in the prediction accuracy of the model, we would rather use a more restrictive model. This is because the flexibility of the model is usually better than the prediction accuracy of the model. The tradeoff between prediction accuracy ad moel interpretability are illustrate in Figure 1.1 1.4 Supervised Vs. Unsupervised Learning Most statistical learning problems involve both supervised and unsupervised learning. In supervised learning, we wish to fit a model to the training data and predict the response variable based on the predictors, with the aim of accurately predicting the response variable or better understanding the relationship between predictors and response variable. Some of the statistical approaches that apply the supervised learning method are: Linear Regression, Logistic Regression, Boosting &amp; Support Vector Machine, Generalized Additive Models (GAMs). In contrast, unsupervised learning methods are those that do not require any training data. One statistical learning tool that we may use in this setting is cluster analysis or clustering. The goal of this method is to ascertain, whether observations fall into distinct groups. 1.5 Regression Vs. Classification Variables can be characterized as either quantitative or qualitative Quantitative variables are those that can be measured in terms of a number Qualitative variables are those that can be measured in terms of a set of categories. We tend to refer to problems with a quantitative response variable as regression problems and problems with a qualitative response variable as classification problems. However, an important note is that it does not matter much whether the predictors/variables are quantitative or qualitative. "],["assessing-model-accuracy.html", "Section 2 Assessing Model Accuracy 2.1 The Regression Setting 2.2 The Bias-Variance Trade-Off 2.3 The Classification Setting", " Section 2 Assessing Model Accuracy \"There is no free lunch in statistics\" No one method dominates all others over all possible data sets. This section introduce some common ways to assess the accuracy of a model to select a statistical learning procedure for a specific data set. 2.1 The Regression Setting 2.1.1 Measuring the Quality of fit In order to evaluate the performance of a model, we need to measure how well its predictions actually match the observed data. In the regression setting, the most commonly used measure is the mean squared error (MSE). MSE is the average squared difference between the estimated values and the actual value. The MSE formula illustrated in (2.1): \\[\\begin{equation} \\Large MSE = \\frac{1}{n}\\sum_{i = 1}^{n} (y_{n} - \\hat{f}(x_{i})^2 \\tag{2.1} \\end{equation}\\] given by where \\(\\hat{f}(x_{i}\\) is the prediction that \\(\\hat{f}\\) gives for the \\(i\\)th observations. The MSE will be small if the predicted responses are very close to the true response, and will be large if for some observations, the predicted and true responses differ substantially. Training MSE computed using the training data that was used to fit the model. Test MSE computed using the previously unseen test observation not used to train the statistical learning method. When a given method yields a small training MSE but a large test MSE, we are said to be overfitting the data. When we overfit the training data, the test MSE will be very large because the supposed patterns that the method found in the training data simply dont exist in the test data. 2.2 The Bias-Variance Trade-Off 2.2.1 Variance Error Variance is the amount that the estimate of \\(\\hat{f}\\) will change if different training data was used. Ideally, it should not change too much from one training dataset to the next, meaning that the algorithm is good at picking out the hidden underlying mapping between the inputs and the output variables. Examples of low-variance machine learning algorithms include: Linear Regression, Linear Discriminant Analysis and Logistic Regression. Examples of high-variance machine learning algorithms include: Decision Trees, k-Nearest Neighbors and Support Vector Machines. 2.2.2 Bias Error Bias are the simplifying assumptions made by a model to make the target function easier to learn. Generally, linear algorithms have a high bias making them fast to learn and easier to understand but generally less flexible. In turn, they have lower predictive performance on complex problems that fail to meet the simplifying assumptions of the algorithms bias. Low Bias Suggests less assumptions about the form of the target function. High-Bias Suggests more assumptions about the form of the target function. Examples of low-bias machine learning algorithms include: Decision Trees, k-Nearest Neighbors and Support Vector Machines. Examples of high-bias machine learning algorithms include: Linear Regression, Linear Discriminant Analysis and Logistic Regression. 2.2.3 Bias-Variance Trafe-Off The goal of any supervised machine learning algorithm is to achieve low bias and low variance. In turn the algorithm should achieve good prediction performance. Linear machine learning algorithms often have a high bias but a low variance. Nonlinear machine learning algorithms often have a low bias but a high variance. The parameterization of machine learning algorithms is often a battle to balance out bias and variance. 2.3 The Classification Setting The most common approach for quantifying the accuracy of our estimate \\(\\hat{f}\\) is the training error rate, the proportion of mistakes that are made if we apply our estimate \\(\\hat{f}\\) to the training observations: \\[\\begin{equation} \\frac{1}{n}\\sum_{i = 1}^{n} I (y_{i} \\neq \\hat{y_{i}}) \\tag{2.2} \\end{equation}\\] Whereas: \\(\\hat{y_{i}}\\): the predicted class label for the \\(i\\)th observation using \\(\\hat{f}\\) \\(I (y_{i} \\neq \\hat{y_{i}})\\): an indicator variable that equal 1 if \\(y_{i} \\neq \\hat{y_{i}}\\) and 0 if \\(y_{i} = \\hat{y_{i}}\\). If: \\(I (y_{i} \\neq \\hat{y_{i}}) = 0\\): correct classification \\(I (y_{i} \\neq \\hat{y_{i}}) \\neq 0\\): incorrect classification (misclassified) A good classifier is one for which the test error is smallest where the test error rate associated with a set of test observations of the from \\((x_{0}, y_{0})\\). 2.3.1 The Bayes Classifier This algorithm is called Naïve because it works on the naïve assumption that the features are independent. Naïve Bayes Classifier works with principle of Bayes Theorem. Conditional probability of an event \\(A\\) given \\(B\\), \\(P(A|B)\\) is the probability of \\(A\\) given that \\(B\\) has already occurred. It is often defined as the ratio of joint probability of \\(A\\) and \\(B\\) (probability of \\(A\\) and \\(B\\) occurring together) to the marginal probability of \\(A\\) (probability of event \\(A\\)) Pros Easy to implement Performs reasonably well with noisy data Cons Poor performance with continuous features Assumption that features are independent is risky 2.3.2 K-Nearest Neighbors (KNN) K-Nearest neighbors (KNN) algorithm can be used to solve both classification and regression problems. When algorithms such as Naïve Bayes Classifier uses probabilities from training samples for predictions, KNN is Lazy learner that does not create any model in advance. The just find the closest based on feature similarity. Pros Easy to implement No assumptions involved Cons Optimal K is always a challenge Lazy learner- computationally expensive "],["LINEAR-REGRESSION.html", "Section 3 Simple Linear Regression 3.1 Estimating the Coefficients 3.2 Assessing the Accuracy of the Coefficient Estimates 3.3 Assessing the Accuracy of the Model 3.4 Consideration", " Section 3 Simple Linear Regression Simple linear regression is a very straightforward approach for predicting a qualitative response \\(Y\\) on the basis of a single predictor \\(X\\). Mathematically, we can write this linear relationship as equation (3.1): \\[\\begin{equation} \\Large Y \\approx \\beta_0 + \\beta_1 X \\tag{3.1} \\end{equation}\\] Whereas: \\(\\beta_0\\): intercept \\(\\beta_1\\): slope 3.1 Estimating the Coefficients In practice, \\(\\beta_0\\) and \\(\\beta_1\\) are unknown. So before make predictions, we must use data to estimate the coefficients. Let \\((x_1, Y_1), (x_2, Y_2),...,(x_n, Y_n)\\) represent \\(n\\) observation pairs, each of which consists of a measurement of \\(X\\) and a measurement of \\(Y\\). Our goal is to obtain coefficient estimates \\(\\beta_0\\) and \\(\\beta_1\\) such that the linear model fits the available data well. In other words, we want to find an intercept \\(\\beta_0\\) and a slope \\(\\beta_1\\) such that the resulting line is as close as possible to the \\(n\\) data points. Figure 3.1 illustrated the oveview of linear regression elements. Figure 3.1: Regression Line There are numbers of ways of measuring closeness. However, by far the most common approach involves minimizing the least squares criterion, and we take that approach in this chapter. Let \\(Y_i = \\beta_0 + \\beta_1 X_i\\) be the prediction for \\(Y\\) based on the \\(i\\)th value of \\(X\\). Then \\(e_i = y_i -\\hat{y}+i\\) represents the \\(i\\)th residual - this is the difference between the \\(i\\)th value and the \\(i\\)th reponse value that is predicted by our linear model. We define the residual sum of square (RSS) as (3.2) equation: \\[\\begin{equation} \\Large RSS = e_1^2 + e_2^2 + ... + e_n^2 \\tag{3.2} \\end{equation}\\] or equivalent as (3.3): \\[\\begin{equation} RSS = (y_1 - \\hat{\\beta_0} - \\hat{\\beta_1}x_2)^2 + (y_2 - \\hat{\\beta_0} - \\hat{\\beta_1}x_2)^2 + ... + (y_n - \\hat{\\beta_0} - \\hat{\\beta_1}x_n)^2 \\tag{3.3} \\end{equation}\\] The least square approach chooses \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\) to minimize the RSS. 3.2 Assessing the Accuracy of the Coefficient Estimates Population Regression Line \\[\\begin{equation} \\Large Y=\\beta_0 + \\beta_1X + \\epsilon \\tag{3.4} \\end{equation}\\] where \\(\\epsilon\\) mean zero random error term. Least Squares Line \\[\\begin{equation} \\Large \\hat{y}=\\hat{\\beta_0} + \\hat{\\beta_1}X \\tag{3.5} \\end{equation}\\] true relationship between \\(X\\) and \\(Y\\) takes the form \\(Y=f(X)+\\epsilon\\). Fundamentally, the concept of these two lines (Population Regression Line vs. Least Squares Line) is a natural extension of the standard statistical approach of using information from a sample to estimate characteristics of a large population. For example, suppose that we are interested in knowing the population mean \\(\\mu\\) of some random variable \\(Y\\). Unfortunately, \\(\\mu\\) is unknown, but we do have access to \\(n\\) observations from \\(Y\\), which we can write as \\(y_1\\), \\(y_2\\), \\(\\cdots\\), \\(y_n\\), and which we can use to estimate \\(\\mu\\). A reasonable estimate is \\(\\hat{\\mu}\\) = \\(\\hat{y}\\), where \\(\\hat{y} = \\dfrac {1}{n}\\sum^n_{i=1}{y_i}\\) is the sample mean. The sample mean and the population mean are different, but in general the sample mean will provide a good estimate of the population mean. In the same way, the unknown coefficients \\(\\beta_0\\) and \\(\\beta_1\\) in linear regression define the population regression line. We seek to estimate these unknown coefficients using and given in (3.5). These coefficient estimates define the least squares line. The analogy between linear regression and estimation of the mean of a random variable is an apt one based on the concept of bias. If we use the sample mean \\(\\hat{\\mu}\\) to estimate \\(\\mu\\), this estimate is unbiased, in the sense that on average, we expect \\(\\hat{\\mu}\\) to equal \\(\\mu\\). In other words, if we estimate \\(\\beta_0\\) and \\(\\beta_1\\) on the basis of a particular data set, then our estimates wont be exactly equal to \\(\\beta_0\\) and \\(\\beta_1\\). But if we could average the estimates obtained over a huge number of data sets, then the average of these estimates would be spot on! In fact, we can see from the right-hand panel of Figure 3.2 that the average of many least squares lines, each estimated from a separate data set, is pretty close to the true population regression line. Figure 3.2: A simulated data set As isllutrated in Figure 3.2: Left: The red line represents the true relationship, \\(f(X) = 2+3X\\), which is known as the population regression line. The blue line is the least squares line; it is the least squares estimate for \\(f(X)\\) based on the observed data, shown in black. Right: The population regression line is again shown in red, and the least squares line in dark blue. In light blue, ten least squares lines are shown, each computed on the basis of a separate random set of observations. Each least squares line is different, but on average, the least squares lines are quite close to the population regression line. How Accurate Is The Sample Mean \\(\\hat{\\mu}\\) As An Estimate Of Population Mean \\(\\mu\\) We answer this question by computing the standard error of \\(\\mu\\), written as SE(\\(\\hat{\\mu}\\)). Standard errors can also be used to perform hypothesis tests on the coefficients. The most common hypothesis test involves testing the null hypothesis of \\(H_0\\): There is no relationship between \\(X\\) and \\(Y\\) \\(H_a\\): There is some relationship between \\(X\\) and \\(Y\\) Mathematically, this corresponds to testing \\[\\begin{equation} H_0: \\beta_1 = 0 \\end{equation}\\] versus \\[\\begin{equation} H_1: \\beta_1 \\neq 0 \\end{equation}\\] If \\(\\beta_1 = 0\\) then the model reduces to \\(Y=\\beta_0+\\epsilon\\), and \\(X\\) is not associated with \\(Y\\). To test the null hypothesis, we need to determine whether \\(\\hat{\\beta_1}\\), our estimate for \\(\\beta_1\\), is sufficiently far from zero that we can be confident that \\(\\beta_1\\) is non-zero. How Far Is Far Enough? This is course depends on the accuracy of \\(\\hat{\\beta_1}\\) -that is, it depends on \\(SE(\\hat{\\beta_1})\\). If SE(\\(\\hat{\\beta_1}\\)) is small, then even relatively small values of \\(\\hat{\\beta_1}\\) may provide strong evidence that \\(\\beta_1 \\neq 0\\), and hence that there is a relationship between \\(X\\) and \\(Y\\). In contrast, if SE(\\(\\hat{\\beta_1}\\)) is large, then \\(\\hat{\\beta_1}\\) must be large in absolute value in order for us to reject the null hypothesis. T-distribution The T-distribution describes the standardized distances of sample means to the population mean when the population standard deviation is not known, and the observations come from a normally distributed population. In practice, we compute a t-statistic, given by \\[\\begin{equation} \\Large t=\\dfrac{\\hat{\\beta_1}-0}{SE(\\hat{\\beta_1})} \\end{equation}\\] For example, we have a t-distribution with \\(n-2\\) degrees of freedom as in Figure 3.3. Figure 3.3: Sampling Distribution of Test Statistic The t-distribution has a bell shape and for values of n greater than approximately 30 it is quite similar to the normal distribution. Consequently, it is a simple matter to compute probability of observing any value equal to \\(|t|\\) or larger, assuming \\(\\beta_1 =0\\). We call this probability the p-value. \\[\\begin{equation} \\Large P(|T| &gt; t) = p \\end{equation}\\] Roughly speaking, we interpret the p-value as follows: If we see a small p-value, then we can infer that there is an association between the predictor and the response. We reject the null hypothesis-that is, we declare a relationship to exist between X and Y- if the p-value is small enough. Typical p-value cutoffs for rejecting the null hypothesis are 5 or 1%. When \\(n\\) = 30, these corresponds to t-statistics of around 2 and 2.75, respectively. 3.3 Assessing the Accuracy of the Model After we have rejected the null hypothesis, t is natural to want to quantify the extend of the model fits the data. The quality of a linear regression fit is typically assessed using 2 related quantities: the residual standard error and the \\(R^2\\) statistic. 3.3.1 Residual Standard Error The Residual Standard Error (RSE) is an estimation of the standard deviation of \\(\\epsilon\\). Roughly speaking, it is the average amount that the response will deviate from the true regression line. It is computed using (3.6): \\[\\begin{equation} RSE=\\sqrt{\\frac{1}{n-2}} RSS=\\sqrt{\\frac{1}{n-2}{\\sum_{i=1}^n (y_{i} - \\hat{y_1})^{2}}} \\tag{3.6} \\end{equation}\\] The RSE is considered a measure of the lack of fit of the model to the data: If RSE is small, we can conclude that the model fits the data very well. In contrast, if RSE is large, it indicates that the model doesnt fit the data well. One consideration of RSE is it is the measure in the units of \\(Y\\) and thus, the method is not always clear what constitute a good RSE. 3.3.2 \\(R^2\\) Standard Error The \\(R^2\\) statistic takes the form of a proportion - the proportion of variance explained-and so it always takes on a value between 0 and 1, and is independent of the scale of \\(Y\\). To calculate \\(R^2\\), we use the formula (3.7): \\[\\begin{equation} \\Large R^2 = \\frac{TSS}{TSSRSS}=1\\frac{TSS}{RSS} \\tag{3.7} \\end{equation}\\] Whereas: TSS: total sum of square. RSS: measures the amount of variability that is left unexplained after performing the regression. In terms of evaluating the \\(R^2\\) statistic result: \\(R^2\\) statistic close to 1 - the model fits well. \\(R^2\\) statistic close to 0 - the model does not fit well. 3.4 Consideration Simple linear regression is a useful approach for predicting a response on the basis of a single predictor variable. However, in practice we often have more than one predictor. The solution for this is to use another linear regression named as Multiple Linear Regression, which will be discussed in the next section. "],["multiple-linear-regression.html", "Section 4 Multiple Linear Regression 4.1 Response and Predictors Relationships 4.2 Dealing With Large Number Of Variables 4.3 Model Fit", " Section 4 Multiple Linear Regression In general, the Multiple Linear Regression model takes the form of (4.1): \\[\\begin{equation} \\Large Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_nX_n + \\epsilon \\tag{4.1} \\end{equation}\\] When we perform multiple linear regression, we usually are interested in answering a few important questions. Is at least one of the predictor \\(X_1, X_2,...,X_p\\) useful in predicting the response? Do all the predictors help to explain \\(Y\\), or is only a subset of the predictors useful? How well does th model fit the data? 4.1 Response and Predictors Relationships In the multiple regression setting with \\(p\\) predictors, we can use a hypothesis test to discover the relationship between predictors and response: \\[ H_0 = \\beta_1 = \\beta_2 = ... = \\beta_p = 0 \\] versus the alternative: \\[ H_a = \\beta_j \\neq 0 \\] This hypothesis test is performed by computing the F-statistic as (4.2): \\[\\begin{equation} \\Large F = \\frac{(TSS-RSS)/p}{RSS/(n-p-1)} \\tag{4.2} \\end{equation}\\] If F-statistic: Near 1 - no relationship between response and predictors. Greater than 1 - there is some relationship between response and predictors. F-statistic only apply when the number of \\(p\\) predictors is small. When p is large, some of the approaches discussed in the next section, such as forward selection, can be used. 4.2 Dealing With Large Number Of Variables The task of determining which predictors are associated with the response is referred to as variable selection. There are 3 classical approaches: Forward selection: starts with no selected variables. During subsequent steps, it evaluates if each candidate variable improves some chosen statistical criterion given previously selected variables, and adds the variable that improves the criterion most. It repeats these steps until none of the remaining variables improves the criterion. Backward selection:starts with the full model, that is, all the variables. At each step, it removes a variable that is least important and does not meet the criterion. Mixed (Stepwise) selection: is the combination of forward selection and backward elimination. Start with no variables in the model, then add variables that is the best fit. However, if at any point the p-value for one of the variables in the model series rises above a certain threshold, then we remove that variable out of the model. 4.3 Model Fit Using the same methods: RSE and \\(R^2\\) statistic as simple linear regression. "],["other-considerations-in-the-regression-model.html", "Section 5 Other Considerations in the Regression Model 5.1 Non-linearity of the Data 5.2 Correlation of Error Terms 5.3 Non-constant Variance of Error Terms (Heteroscedasticity) 5.4 Outlier 5.5 High Leverage Points 5.6 Collinearity", " Section 5 Other Considerations in the Regression Model When we fit a linear regression model, many problems can occur, to name a few: Non-linearity of the response-predictor relationships. Correlation of error terms. Non-constant variance of error terms. Outliner. High-leverage points. Collinearity. 5.1 Non-linearity of the Data The first assumption of Linear Regression is that relations between the independent and dependent variables must be linear. Although this assumption is not always cited in the literature, it is logical and important to check for it. After all, if your relationships are not linear, you should not use a linear model, but rather a non-linear model of which plenty exist. We can check for linear relationships easily by making a scatter plot for each independent variable with the dependent variable as in Figure 5.1. Figure 5.1: Plots of residuals versus predicted (or fitted) values for the Auto data set In each plot, the red line is a smooth fit to the residuals, intended to make it easier to identify a trend. Left: A linear regression of mpg on horsepower. A strong pattern in the residuals indicates non-linearity in the data. Right: A linear regression of mpg on horsepower and horsepower2. There is little pattern in the residuals. 5.2 Correlation of Error Terms If there is correlation among the error terms \\(\\epsilon_1, \\epsilon_2,...,\\epsilon_n\\), then the estimated standard errors (SE) will tend to underestimate the true SE. As the result, p-value associated with the model will be lower than they should be, which could cause us to erroneously conclude that a parameter is statistically significant. Such correlations frequently occur in the context of time series data, which consist of observations for which measurements are obtained at adjacent time points will have positively correlated errors. In order to determine if this is the case for a given data set, we can plot the residuals from our model as a function of time. Figure 5.2 provides an illustration. In the top panel, we see the residuals from a linear regression fit to data generated with uncorrelated errors. There is no evidence of a time-related trend in the residuals. In contrast, the residuals in the bottom panel are from a data set in which adjacent errors had a correlation of 0.9. Now there is a clear pattern in the residualsadjacent residuals tend to take on similar values. Finally, the center panel illustrates a more moderate case in which the residuals had a correlation of 0.5. There is still evidence of tracking, but the pattern is less clear. Figure 5.2: Plots of residuals from simulated time series data sets generated with differing levels of correlation p between error terms for adjacent time 5.3 Non-constant Variance of Error Terms (Heteroscedasticity) Heteroscedasticity in a model means that the error is constant along the values of the dependent variable. One can identify non-constant variances in the errors, or heteroscedasticity, from the presence of a funnel shape in heteroscedathe residual plot. An example is shown in the left-hand panel of Figure 5.3, heteroscedasticity in which the magnitude of the residuals tends to increase with the fitted values. In each plot, the red line is a smooth fit to the residuals, intended to make it easier to identify a trend. The blue lines track the outer quantiles of the residuals, and emphasize patterns: Left: The funnel shape indicates heteroscedasticity. Right: The response has been log transformed, and there is now no evidence of heteroscedasticity. Figure 5.3: Residual plots Some of the suggested solutions are: Do some work on your input data like having some variables to add or remove. Do transformations, like applying concave function such as logistics (\\(logY\\)) or square root \\(\\sqrt{Y}\\). If this doesnt change anything, you can also switch to the weighted least squares model. Weighted least squares is a model that can deal with unconstant variances and heteroscedasticity is therefore not a problem. 5.4 Outlier An outlier is a point for which \\(y_i\\) is far from the value predicted by model. Outliers can arise for a variety of reasons, such as incorrect recording of an observation during data collection. As illustrated in Figure 5.4: Left: The least squares regression line is shown in red, and the regression line after removing the outlier is shown in blue. Center: The residual plot clearly identifies the outlier. Right: The outlier has a studentized residual of 6; typically we expect values between 3 and 3. Figure 5.4: Outlier plots If we believe that an outlier has occurred due to an error in data collection or recording, then one solution is to simply remove the observation. 5.5 High Leverage Points In contrast to outlier with unusual for response value \\(y\\), observations with high leverage have an unusual value for \\(x_i\\). As illustrated in Figure 5.5: Figure 5.5: Leveraging Observations Plots Left: Observation 41 is a high leverage point, while 20 is not. The red line is the fit to all the data, and the blue line is the fit with observation 41 removed. Center: The red observation is not unusual in terms of its X1 value or its X2 value, but still falls outside the bulk of the data, and hence has high leverage. Right: Observation 41 has a high leverage and a high residual. In order to quantify an observations leverage, we compute the leverage statistic as in (5.1): \\[\\begin{equation} \\Large h_i = \\frac{1}{n} + \\frac{(x_i - \\overline{x})^2}{\\sum_{i&#39;=1}^{n}(x_i&#39; - \\overline x)^2} \\tag{5.1} \\end{equation}\\] A large value of this statistic indicates an observation with high leverage. 5.6 Collinearity Collinearity refers to the situation in which two or more predictor variables are closely related to one another. In order to check for collinearity, we can either use Correlation Matrix or Variance Inflation Factor (VIF). 5.6.1 Correlation Matrix A simple way to detect collinearity is to look at the correlation matrix of the predictors. An element of this matrix that is large in absolute value indicates a pair of highly correlated variables, and therefore a collinearity problem in the data. Figure 5.6: Sample Correlation Matrix using R Unfortunately, not all collinearity problems can be detected by inspection of the correlation matrix: it is possible for collinearity to exist between three or more variables even if no pair of variables has a particularly high correlation. We call this situation multicollinearity. Multicollinearity causes problems in using regression models to draw conclusions about the relationships between predictors and outcome. An individual predictors p-value may test non-significant even though it is important. Confidence intervals for regression coefficients in a multicollinear model may be so high that tiny changes in individual observations have a large effect on the coefficients, sometimes reversing their signs. 5.6.2 Variance Inflation Factor (VIF) Instead of inspecting the correlation matrix, a better way to assess collinearity is to compute the variance inflation factor (VIF). This can easily be calculated in R using software packages. When faced with the problem of collinearity, there are two simple solutions: The first is to drop one of the problematic variables from the regression. The second solution is to combine the collinear variables together into a single predictor. "],["case-study---marketing-plan.html", "Section 6 Case Study - Marketing Plan 6.1 Data Overview 6.2 Important Questions", " Section 6 Case Study - Marketing Plan 6.1 Data Overview The Advertising data set consists of the sales of that product in 200 different markets, along with advertising budgets for the product in each of those markets for three different media: TV, radio, and newspaper. The data are displayed in Figure 6.1. The plot displays sales, in thousands of units, as a function of TV, radio, and newspaper budgets, in thousands of dollars, for 200 different markets. In each plot we show the simple least squares fit of sales to that variable, as described in previous section. In other words, each blue line represents a simple model that can be used to predict sales using TV, radio, and newspaper, respectively. Figure 6.1: the Advertising data set 6.2 Important Questions Suppose that in our role as statistical consultants we are asked to suggest, on the basis of this data, a marketing plan for next year that will result in high product sales. Here are a few important questions that we might seek to address: Is there a relationship between advertising sales and budget? This question can be answered by fitting a multiple regression model (6.1) of sales onto TV, radio, and newspaper, as: \\[\\begin{equation} \\Large sales = \\beta_0 + \\beta_1 × TV + \\beta_2 × radio + \\beta_3 × newspaper + \\epsilon. \\tag{6.1} \\end{equation}\\] and testing the hypothesis \\[ H_0: \\beta_{TV} = \\beta_{radio} = \\beta_{newspaper} = 0 \\] F-statistic can be used to determine whether or not we should reject this null hypothesis. In this case, the p-value corresponding to the F-statistic in Table 6.1 is very low, indicating clear evidence of a relationship between advertising and sales. Table 6.1: More information about the least squares model for the regression of number of units sold on TV, newspaper, and radio advertising budgets in the Advertising data Quantity Value Residual standard error 1.69 \\(R^2\\) 0.897 F-statistic 570 2.How strong is the relationship? First, the RSE estimates the standard deviation of the response from the population regression line. For the Advertising data, the RSE is 1,681 units while the mean value for the response is 14,022, indicating a percentage error of roughly 12%. Second, the \\(R^2\\) statistic records the percentage of variability in the response that is explained by the predictors. The predictors explain almost 90% of the variance in sales. The RSE and \\(R^2\\) statistics are displayed in Figure 6.1. Which media contribute to sales? To answer this question, we can examine the p-values associated with each predictors t-statistic. In the multiple linear regression displayed in Figure 6.3, the p-values for TV and radio are low, but the p-value for newspaper is not. This suggests that only TV and radio are related to sales. Table 6.2. More information about the least squares model for the regression of number of units sold on TV, newspaper, and radio advertising budgets in the Advertising data Coefficient Std. error t-statistic p-value Intercept 2.939 0.3119 9.42 &lt; 0.0001 TV 0.046 0.0014 32.81 &lt; 0.0001 radio 0.189 0.0086 21.89 &lt; 0.0001 newspaper -0.001 0.0059 -0.18 0.8599 How large is the effect of each medium on sales? The standard error of \\(\\hat{\\beta_j}\\) can be used to construct confidence intervals for \\(\\beta_j\\). For the Advertising data, the 95% confidence intervals are as follows: (0.043, 0.049) for TV, (0.172, 0.206) for radio, and (0.013, 0.011) for newspaper. The confidence intervals for TV and radio are narrow and far from zero, providing evidence that these media are related to sales. But the interval for newspaper includes zero, indicating that the variable is not statistically significant given the values of TV and radio. We saw in previous section that collinearity can result in very wide standard errors. Could collinearity be the reason that the confidence interval associated with newspaper is so wide? The VIF scores are 1.005, 1.145, and 1.145 for TV,radio, and newspaper, suggesting no evidence of collinearity. In order to assess the association of each medium individually on sales, we can perform three separate simple linear regressions. Results are shown in Table 6.3, 6.4 and 6.5. Table 6.3. Simple regression of sales on TV Coefficient Std. error t-statistic p-value Intercept 7.0325 0.4578 15.36 &lt;0.0001 TV 0.0475 0.0027 17.67 &lt;0.0001 Coefficients of the least squares model for the regression of number of units sold on TV advertising budget. An increase of $1,000 in the TV advertising budget is associated with an increase in sales by around 50 units. Table 6.4. Simple regression of sales on radio Coefficient Std. error t-statistic p-value Intercept 9.312 0.563 16.54 &lt;0.0001 radio 0.203 0.02 9.92 &lt;0.0001 Table 6.5. Simple regression of sales on newspaper Coefficient Std. error t-statistic p-value Intercept 12.351 0.621 19.88 &lt;0.0001 newspaper 0.055 0.017 3.30 0.00115 Coefficients of the simple linear regression model for number of units sold on: Top: radio advertising budget. Bottom: newspaper advertising budget. A $1,000 increase in spending on radio advertising is associated with an average increase in sales by around 203 units, while the same increase in spending on newspaper advertising is associated with an average increase in sales by around 55 units. There is evidence of an extremely strong association between TV and sales and between radio and sales. There is evidence of a mild association between newspaper and sales, when the values of TV and radio are ignored. How accurately can we predict future sales? The response can be predicted using (6.2): \\[\\begin{equation} \\Large y = \\hat{\\beta_0} + \\hat{\\beta_1}x_1 + \\hat{\\beta_2}x_2 + · · · + \\hat{\\beta_0}x_p. \\tag{6.2} \\end{equation}\\] The accuracy associated with this estimate depends on whether we wish to predict an individual response, \\(Y = f(X) + \\epsilon\\), or the average response, \\(f(X)\\). If the former, we use a prediction interval, and if the latter, we use a confidence interval. Prediction intervals will always be wider than confidence intervals because they account for the uncertainty associated with \\(\\epsilon\\), the irreducible error. Is the relationship linear? In Considerations section, we saw that residual plots can be used in order to identify non-linearity. If the relationships are linear, then the residual plots should display no pattern. In the case of the Advertising data, we observe a non-linear effect in Figure 6.2, though this effect could also be observed in a residual plot. We also discussed the inclusion of transformations of the predictors in the linear regression model in order to accommodate non-linear relationships. Figure 6.2: A linear regression fit to sales using TV and radio as predictors From the pattern of the residuals, we can see that there is a pronounced non-linear relationship in the data. The positive residuals (those visible above the surface), tend to lie along the 45-degree line, where TV and radio budgets are split evenly. The negative residuals (most not visible), tend to lie away from this line, where budgets are more lopsided. Is there synergy among the advertising media? The standard linear regression model assumes an additive relationship between the predictors and the response. An additive model is easy to interpret because the effect of each predictor on the response is unrelated to the values of the other predictors. However, the additive assumption may be unrealistic for certain data sets. A small p-value associated with the interaction term indicates the presence of such relationships. Figure 6.2 suggested that the Advertising data may not be additive. Including an interaction term in the model results in a substantial increase in \\(R^2\\), from around 90% to almost 97%. "],["an-overview-of-classification.html", "Section 7 An Overview of Classification", " Section 7 An Overview of Classification In this chapter, we study approaches for predicting qualitative responses, a process that is known as Classification. Predicting a qualitative response for an observation can be referred to as classifying that observation, since it involves assigning the observation to a category, or class. On the other hand, often the methods used for classification first predict the probability that the observation belongs to each of the categories of a qualitative variable, as the basis for making the classification. Generally, we discuss some widely-used classifiers: Logistic Regression Linear Discriminant Analysis Quadratic Dislogistic Regression Naive Bayes K-nearest Neighbors The discussion of logistic regression is used as a jumping-off point for a discussion of Generalized Linear Models, and in particular, Poisson Regression. "],["logistic-regression.html", "Section 8 Logistic Regression 8.1 Logistic Model 8.2 Estimating the Regression Coefficients 8.3 Multiple Logistic Regression 8.4 Multinomial Logistic Regression", " Section 8 Logistic Regression 8.1 Logistic Model In logistic regression, we use the logistic function, which is defined as: \\[ \\Large p(X) = \\frac{e^{\\beta_0 + \\beta_1X}}{1 + e^{\\beta_0 + \\beta_1X}} \\] After a bit of manipulation of the above equation, we find that \\[ \\Large \\frac{p(X)}{1 - p(X)} = e^{\\beta_0 + \\beta_1X} \\] Whereas: \\(p(X)/[1p(X)]\\): the odds, and can take on any value between 0 and . By taking the logarithm of both sides of (4.3), we arrive at \\[ \\Large log(\\frac{p(X)}{1 - p(X)}) = \\beta_0 + \\beta_1X \\] Whereas: \\(log(\\frac{p(X)}{1 - p(X})\\): the _log odds or logit. In a logistic regression model, increasing \\(X\\) by one unit changes the log odds by \\(\\beta_1\\). 8.2 Estimating the Regression Coefficients Although we could use (non-linear) least squares to fit the model, the more general method of maximum likelihood is preferred, since it has better statistical properties. In general, logistic regression and other models can be easily fit using statistical software such as R, and so we do not need to concern ourselves with the details of the maximum likelihood fitting procedure. An explanation of Maximum Likelihood for Machine Learning can be found here: This is a coefficient table Coefficient Std. error z-statistic p-value Intercept 10.6513 0.3612 29.5 &lt;0.0001 balance 0.0055 0.0002 24.9 &lt;0.0001 Table above shows the coefficient estimates and related information that result from fitting a logistic regression model on the Default data in order to predict the probability of default=Yes using balance. We see that \\(\\hat\\beta_1\\) = 0.0055; this indicates that an increase in balance is associated with an increase in the probability of default. To be precise, a one-unit increase in balance is associated with an increase in the log odds of default by 0.0055 units. 8.3 Multiple Logistic Regression We now consider the problem of predicting a binary response using multiple predictors. By analogy with the extension from simple to multiple linear regression in Chapter 3, we can generalize the logistics regression formula as follows: \\[ \\Large p(X) = \\frac{e^{\\beta_0+\\beta_1X_1+...+\\beta_pX_p}}{1+e^{\\beta_0+\\beta_1X_1+...+\\beta_pX_p}} \\] 8.4 Multinomial Logistic Regression We sometimes wish to classify a response variable that has more than two classes. However, the logistic regression approach that we have seen in this section only allows for K = 2 classes for the response variable. It turns out that it is possible to extend the two-class logistic regression approach to the setting of \\(K &gt; 2\\) classes. This extension is sometimes known as multinomial logistic regression. "],["generative-models-for-classification.html", "Section 9 Generative Models for Classification 9.1 Linear Discriminant Analysis 9.2 Quadratic Discriminant Analysis", " Section 9 Generative Models for Classification Why do we need another method, when we have logistic regression? There are several reasons: When there is substantial separation between the two classes, the parameter estimates for the logistic regression model are surprisingly unstable. The methods that we consider in this section do not suffer from this problem. If the distribution of the predictors X is approximately normal in each of the classes and the sample size is small, then the approaches in this section may be more accurate than logistic regression. The methods in this section can be naturally extended to the case of more than two response classes. (In the case of more than two response classes, we can also use multinomial logistic regression.) Suppose that we wish to classify an observation into one of \\(K\\) classes, where \\(K  2\\). Then Bayes theorem states that: \\[ \\Large \\Pr(A|B)=\\frac{\\Pr(B|A)\\Pr(A)}{\\Pr(B|A)\\Pr(A)+\\Pr(B|\\neg A)\\Pr(\\neg A)} \\] In the following sections, we discuss three classifiers that use different estimates of \\(f_k(x)\\) to approximate the Bayes classifier: 1. linear discriminant analysis 2. quadratic discriminant analysis 3. naive Bayes 9.1 Linear Discriminant Analysis a. When \\(p\\) = 1 For now, assume that \\(p\\) = 1that is, we have only one predictor. We will then classify an observation to the class for which pk(x) is greatest. To estimate \\(f_k(x)\\), we will first make some assumptions about its form: In particular, we assume that \\(f_k(x)\\) is normal or Gaussian. In practice, even if we are quite certain of our assumption that \\(X\\) is drawn from a Gaussian distribution within each class, to apply the Bayes classifier we still have to estimate the parameters \\(\\mu_1, . . . ,\\mu_K\\), \\(\\pi_1, . . . ,\\pi_K\\), and \\(\\sigma^2\\). The linear discriminant analysis (LDA) method approximates the Bayes classifier by plugging estimates for \\(\\pi_k, \\mu_k\\), and \\(\\sigma^2\\). To reiterate, the LDA classifier results from assuming that the observations within each class come from a normal distribution with a class-specific mean and a common variance \\(\\sigma^2\\), and plugging estimates for these parameters into the Bayes classifier. b. When \\(p\\) &gt; 1 We now extend the LDA classifier to the case of multiple predictors. To do this, we will assume that \\(X = (X_1,X_2, . . . ,X_p)\\) is drawn from a multivariate Gaussian (or multivariate normal) distribution, with a class-specific mean vector and a common covariance matrix. The multivariate Gaussian distribution assumes that each individual predictor follows a one-dimensional normal distribution. c. Problems with LDA d. Confusion Matrix A confusion matrix is a summarized table of the number of correct and incorrect predictions (or actual and predicted values) yielded by a classifier (or classification model) for binary classification tasks. In simple words,  A confusion matrix is a performance measurement for machine learning algorithm. By visualizing the confusion matrix, an individual could determine the accuracy of the model by observing the diagonal values for measuring the number of accurate classification. A breif explanation of the confusion matrix is illustrated in Figure 9.1. Figure 9.1: A sample 2x2 confusion matrix i. True Positive, True Negative, False Positive and False Negative True Positive is an outcome where the model correctly predicts the positive class, True Negative is an outcome where the model correctly predicts the negative class. False Positive (Type I Error) is an outcome where the model incorrectly predicts the positive class when the actual class is negative. False Negative (Type II Error) is an outcome where the model incorrectly predicts the negative class when the actual class is positive. ii. Precision Precision explains how many correctly predicted values came out to be positive actually. Or simply it gives the number of correct outputs given by the model out of all the correctly predicted positive values by the model. It determines whether a model is reliable or not. It is useful for the conditions where false positive is a higher concern as compared to a false negative. For calculating the precision, the formula is: \\[ \\Large \\Large Precision = \\frac{TP}{TP+FP} \\] iii. Recall Recall describes how many of the actual positive values to be predicted correctly out of the model. It is useful when false-negative dominates false positives. The formula for calculating the recall is \\[ \\Large Recall = \\frac{TP}{TP+FN} \\] Increasing precision decreases recall and vice versa, this is known as the precision/recall tradeoff. iv. Accuracy One of the significant parameters in determining the accuracy of the classification problems, it explains how regularly the model predicts the correct outputs and can be measured as the ratio of the number of correct predictions made by the classifier over the total number of predictions made by the classifiers. The formula is \\[ \\Large Accuracy = \\frac{TP+TN}{TP+TN+FP+FN} \\] v. F-Measure For the condition when two models have low precision and high recall or vice versa, it becomes hard to compare those models, therefore to solve this issue we can deploy F-score. ::tip F-score is a harmonic mean of Precision and Recall. ::: By calculating F-score, we can evaluate the recall and precision at the same time. Also, if the recall is equal to precision, The F-score is maximum and can be calculated using the below formula \\[ \\Large F-measure = \\frac{2 \\times Recall \\times precision}{Recall + Precision} \\] vi. ROC curve The ROC curve is a popular graphic for simultaneously displaying the two types of errors for all possible thresholds. Figure 9.2: A ROC curve for the LDA classifier on the Default data Figure above displays the ROC curve for the LDA classifier on the training data. The overall performance of a classifier, summarized over all possible thresholds, is given by the area under the (ROC) curve (AUC). An ideal ROC curve will hug the top left corner, so the large area under the (ROC) curve the AUC the better the classifier. For this data the AUC is 0.95, which is close to the maximum of one so would be considered very good. 9.2 Quadratic Discriminant Analysis Quadratic discriminant analysis (QDA) provides an alternative approach. Like LDA, the QDA classifier assumes that the observations from each class of Y are drawn from a Gaussian distribution. However, unlike LDA, QDA assumes that each class has its own covariance matrix. In other words, the predictor variables are not assumed to have common variance across each of the \\(k\\) levels in \\(Y\\). Mathematically, it assumes that an observation from the \\(k\\)th class is of the form, where is a covariance matrix for the \\(k\\)th class. Under this assumption, the classifier assigns an observation to the class for which is largest. Why is this important? Consider the image below. In trying to classify the observations into the three (color-coded) classes, LDA (left plot) provides linear decision boundaries that are based on the assumption that the observations vary consistently across all classes. However, when looking at the data it becomes apparent that the variability of the observations within each class differ. Consequently, QDA (right plot) is able to capture the differing covariances and provide more accurate non-linear classification decision boundaries. "],["a-comparison-of-classification-methods.html", "Section 10 A Comparison of Classification Methods Figure for Scenarios 1, 2, 3 10.1 Scenario 1 10.2 Scenario 2 10.3 Scenario 3 Figure for Scenarios 4, 5, 6 10.4 Scenario 4 10.5 Scenario 5 10.6 Scenario 6 Conclusion", " Section 10 A Comparison of Classification Methods We now compare the empirical (practical) performance of logistic regression, LDA, QDA, naive Bayes, and KNN. We generated data from six different scenarios, each of which involves a binary (two-class) classification problem. In each of the six scenarios, there were p = 2 quantitative predictors. The scenarios were as follows: Figure for Scenarios 1, 2, 3 Figure 10.1: Boxplots of the test error rates for each of the linear scenarios described in the main text 10.1 Scenario 1 There were 20 training observations in each of two classes. The observations within each class were uncorrelated random normal variables with a different mean in each class. The left-hand panel of Figure 12.1 shows that: LDA performed well in this setting, as one would expect since this is the model assumed by LDA. Logistic regression also performed quite well, since it assumes a linear decision boundary. KNN performed poorly because it paid a price in terms of variance that was not offset by a reduction in bias. QDA also performed worse than LDA, since it fit a more flexible classifier than necessary. 10.2 Scenario 2 Details are as in Scenario 1, except that within each class, the two predictors had a correlation of 0.5. The center panel of Figure 12.1 indicates that the performance of most methods is similar to the previous scenario. 10.3 Scenario 3 As in the previous scenario, there is substantial negative correlation between the predictors within each class. However, this time we generated \\(X_1\\) and \\(X_2\\) from the t-distribution, with 50 observations per class. The t-distribution has a similar shape to the normal distribution, but it has a tendency to yield more extreme pointsthat is, more points that are far from the mean. In this setting, the decision boundary was still linear, and so fit into the logistic regression framework. The set-up violated the assumptions of LDA, since the observations were not drawn from a normal distribution. The right-hand panel of Figure 12.1 shows that: * Logistic regression outperformed LDA, though both methods were superior to the other approaches. * In particular, the QDA results deteriorated considerably as a consequence of non-normality. Figure for Scenarios 4, 5, 6 Figure 10.2: Boxplots of the test error rates for each of the linear scenarios described in the main text 10.4 Scenario 4 The data were generated from a normal distribution, with a correlation of 0.5 between the predictors in the first class, and correlation of 0.5 between the predictors in the second class. This setup corresponded to the QDA assumption, and resulted in quadratic decision boundaries. The left-hand panel of Figure 12.2 shows that QDA outperformed all of the other approaches. 10.5 Scenario 5 The data were generated from a normal distribution with uncorrelated predictors. Then the responses were sampled from the logistic function applied to a complicated non-linear function of the predictors. Figure 10.3: Boxplots of the test error rates for each of the linear scenarios described in the main text Consequently, there is a quadratic decision boundary. The center panel or Figure 12.2 shows that the QDA once again outperformed best, followed closely by KNN-CV. The linear method had poor performance. 10.6 Scenario 6 The observations were generated from a normal distribution with a different diagonal covariance matrix for each class. However, the sample size was very small: just \\(n = 6\\) in each class. Figure 10.4: Boxplots of the test error rates for each of the linear scenarios described in the main text As the result, even the quaratic decision boundary was not a good fit. The right hand panel of Figure 12.2 shows that: QDA gave slightly better results than the linear methods. Much more flexible KNN-CV method gave the best performance. However, \\(K\\) = 1 gave the worst performance out of all methods. This highlight the fact that even when the data exhibit a complex non-linear relationship, a non-parametric method such as KNN can still give poor results if the level of smoothness is not chosen correctly. Conclusion These six examples illustrate that no one method will dominate the others in every situation: When the true decision boundaries are linear, then the LDA and logistic regression approaches will tend to perform well. When the boundaries are moderately non-linear, QDA may give better results. Finally, for much more complicated decision boundaries, a non-parametric approach such as KNN can be superior. But the level of smoothness for a non-parametric approach must be chosen carefully. "],["generalized-linear-model.html", "Section 11 Generalized Linear Model 11.1 Overview 11.2 Generalized Linear Models in Greater Generality", " Section 11 Generalized Linear Model 11.1 Overview In reality, we may sometimes be faced with situations in which \\(Y\\) is neither qualitative nor quantitative, and so neither linear regression nor the classification approaches covered in this chapter is applicable. To overcome the inadequacies of linear regression for analyzing the Bikeshare data set, we will make use of an alternative approach, called Poisson regression. The Poisson distribution: is typically used to model counts; this is a natural choice for a number of reasons, including the fact that counts, like the Poisson distribution, take on nonnegative integer values. Some important distinctions between the Poisson regression model and the linear regression model are as follows: Interpretation: To interpret the coefficients in the Poisson regression model, we must pay close attention to (4.37), which states that an increase in \\(X_j\\) by one unit is associated with a change in \\(E(Y)\\) = \\(\\lambda\\) by a factor of exp(\\(\\beta_j\\)). For example, a change in weather from clear to cloudy skies is associated with a change in mean bike usage by a factor of exp(0.08) = 0.923, i.e. on average, only 92.3% as many people will use bikes when it is cloudy relative to when it is clear. If the weather worsens further and it begins to rain, then the mean bike usage will further change by a factor of exp(0.5) = 0.607, i.e. on average only 60.7% as many people will use bikes when it is rainy relative to when it is cloudy. Mean-variance relationship: by modeling the data set with a Poisson regression, we implicitly assume that mean output in a given factor equals the variance of output to that variable. By contrast, under a linear regression model, the variance of the output always takes on a constant value. Thus, the Poisson regression model is able to handle the mean-variance relationship seen in the data set in a way that the linear regression model is not nonnegative fitted values: There are no negative predictions using the Poisson regression model. This is because the Poisson model itself only allows for nonnegative values. 11.2 Generalized Linear Models in Greater Generality We have now discussed three types of regression models: linear, logistic and Poisson. These approaches share some common characteristics: Each approach uses predictors \\(X_1,...,X_p\\) to predict a response \\(Y\\). We assume that, conditional on \\(X_1,...,X_p\\), \\(Y\\) belongs to a certain family of distributions: For linear regression, we typically assume that \\(Y\\) follows a Gaussian or normal distribution. For logistic regression, we assume that \\(Y\\) follows a Bernoulli distribution. For Poisson regression, we assume that \\(Y\\) follows a Poisson distribution. Each approach models the mean of \\(Y\\) as a function of the predictors: In linear regression, the mean of \\(Y\\) takes the form: \\[ E(Y|X_1, . . . ,X_p) = \\beta_0 + \\beta_1X_1 + · · · + \\beta_pX_p \\] For logistic regression, the mean instead takes the form: \\[ E(Y|X_1, . . . ,X_p) = Pr(Y=1|X_1, . . . ,X_p) \\] For Poisson regression, the mean takes the form: \\[ E(Y|X_1, . . . ,X_p) = \\lambda(X_1, . . . ,X_p) \\] The Gaussian, Bernoulli and Poisson distributions are all members of a wider class of distributions, known as the exponential family. Other wellexponential known members of this family are the exponential distribution, the Gamma distribution, and the negative binomial distribution (which is not cover in this secion). In general, we can perform a regression by modeling the response \\(Y\\) as coming from a particular member of the exponential family, and then transforming the mean of the response so that the transformed mean is a linear function of the predictors Poisson. Any regression approach that follows this very general recipe is known as a generalized linear model (GLM). Thus, linear regression, logistic generalized linear model regression, and Poisson regression are three examples of GLMs. "],["an-overview-of-resampling-methods.html", "Section 12 An Overview of Resampling Methods", " Section 12 An Overview of Resampling Methods Resampling methods are an indispensable tool in modern statistics. They involve repeatedly drawing samples from a training set and refitting a model of interest on each sample in order to obtain additional information about the fitted model. For example, in order to estimate the variability of a linear regression fit, we can repeatedly draw different samples from the training data, fit a linear regression to each new sample, and then examine the extent to which the resulting fits differ. Such an approach may allow us to obtain information that would not be available from fitting the model only once using the original training sample. In this chapter, we discuss two of the most commonly used resampling methods: cross-validation and the bootstrap, which are important tools in the practical application of many statistical learning procedures. Cross-validation can be used to estimate the test error associated with a given statistical learning method in order to evaluate its performance, or to select the appropriate level of flexibility. The bootstrap is used in several contexts, most commonly model to provide a measure of accuracy of a parameter estimate or of a given selection statistical learning method. Model assessment The process of evaluating a models performance. Assessment Model selection The process of selecting the proper level of flexibility for a model. "],["cross-validation.html", "Section 13 Cross-Validation 13.1 Test and Training Error Rate 13.2 The Validation Set Approach 13.3 Leave-One-Out Cross-Validation 13.4 \\(k\\)-Fold Cross-Validation 13.5 Bias-Variance Trade-Off for k-Fold Cross-Validation", " Section 13 Cross-Validation 13.1 Test and Training Error Rate In previous section, we discuss the distinction between the test error rate and the training error rate as follows: The test error is the average error that results from using a statistical learning method to predict the response on a new observation that is, a measurement that was not used in training the method. The test error can be easily calculated if a designated test set is available. Unfortunately, this is usually not the case. The training error can be easily calculated by applying the statistical learning method to the observations used in its training. However, the training error rate often is quite different from the test error rate, and in particular the former can dramatically underestimate the latter. In this section, we instead consider a class of methods that estimate the test error rate by holding out a subset of the training observations from the fitting process, and then applying the statistical learning method to those held out observations. 13.2 The Validation Set Approach The validation set approach is a cross-validation technique in Machine learning. In the Validation Set approach, the dataset which will be used to build the model is divided randomly into 2 parts namely training set and validation set(or testing set). Steps Involved in the Validation Set Approach: A random splitting of the dataset into a certain ratio(generally 70-30 or 80-20 ratio is preferred) Training of the model on the training data set The resultant model is applied to the validation set Models accuracy is calculated through prediction error by using model performance metrics The validation set approach is illustrated in Figure 13.1. A set of \\(n\\) observations are randomly split into a training set (shown in blue, containing observations 7, 22, and 13, among others) and a validation set (shown in beige, and containing observation 91, among others). The statistical learning method is fit on the training set, and its performance is evaluated on the validation set. Figure 13.1: A schematic display of CV The validation set approach is conceptually simple and is easy to implement. But it has two potential drawbacks: the validation estimate of the test error rate can be highly variable, depending on precisely which observations are included in the training set and which observations are included in the validation set. In the validation approach, only a subset of the observationsthose that are included in the training set rather than in the validation setare used to fit the model. Since statistical methods tend to perform worse when trained on fewer observations, this suggests that the validation set error rate may tend to overestimate the test error rate for the model fit on the entire data set. 13.3 Leave-One-Out Cross-Validation Leave-one-out cross-validation (LOOCV) is closely related to the validation leave-oneout crossvalidation set approach of Section Validation Set Approach, but it attempts to address that methods drawbacks. Figure 13.2: A schematic display of LOOCV A display of LOOCV is illustrated in Figure 13.2. A set of \\(n\\) data points is repeatedly split into a training set (shown in blue) containing all but one observation (\\(n-1\\)), and a validation set that contains only that observation (shown in beige). The test error is then estimated by averaging the \\(n\\) resulting \\(MSE\\)s. The first training set contains all but observation 1, the second training set contains all but observation 2, and so forth. The benefit of so many fit and evaluated models is a more robust estimate of model performance as each row of data is given an opportunity to represent the entirety of the test dataset. Specifically, LOOCV has a couple of major advantages over the validation set approach: First, it has far less bias. In LOOCV, we repeatedly fit the statistical learning method using training sets that contain \\(n  1\\) observations, almost as many as are in the entire data set. This is in contrast to the validation set approach, in which the training set is typically around half the size of the original data set. Consequently, the LOOCV approach tends not to overestimate the test error rate as much as the validation set approach does. Second, there is no randomness in the training/validation set splits. In contrast to the validation approach which will yield different results when applied repeatedly due to randomness in the training/validation set splits, performing LOOCV multiple times will always yield the same results. However, this method has the maximum computational cost. It requires one model to be created and evaluated for each example in the training dataset and thus, is not suggested to apply when there is very lage number of \\(n\\). Dont Use LOOCV: Large datasets or costly models to fit. Use LOOCV: Small datasets or when estimated model performance is critical. 13.4 \\(k\\)-Fold Cross-Validation An alternative to LOOCV is k-fold CV. This approach involves randomly k-fold CV dividing the set of observations into \\(k\\) groups, or folds, of approximately equal size. The first fold is treated as a validation set, and the method is fit on the remaining \\(k  1\\) folds. It is not hard to see that LOOCV is a special case of k-fold CV in which \\(k\\) is set to equal \\(n\\). In practice, one typically performs k-fold CV using \\(k = 5\\) or \\(k = 10\\). The most obvious advantage is computational. LOOCV requires fitting the statistical learning method \\(n\\) times. This has the potential to be computationally expensive. Some statistical learning methods have computationally intensive fitting procedures, and so performing LOOCV may pose computational problems. In contrast, performing 10-fold CV requires fitting the learning procedure only ten times, which may be much more feasible. But putting computational issues aside, a less obvious but potentially more important advantage of k-fold CV is that it often gives more accurate estimates of the test error rate than does LOOCV. This has to do with a bias-variance trade-off. 13.5 Bias-Variance Trade-Off for k-Fold Cross-Validation It was mentioned in Section Validation Set Approach that the validation set approach can lead to overestimates of the test error rate, since in this approach the training set used to fit the statistical learning method contains only half the observations of the entire data set. Using this logic, it is not hard to see that LOOCV will give approximately unbiased estimates of the test error, since each training set contains \\(n1\\) observations, which is almost as many as the number of observations in the full data set. And performing k-fold CV for, say, \\(k = 5\\) or \\(k = 10\\) will lead to an intermediate level of bias - fewer than in the LOOCV approach, but substantially more than in the validation set approach. Therefore, from the perspective of bias reduction, it is clear that LOOCV is to be preferred to k-fold CV. To summarize, there is a bias-variance trade-off associated with the choice of k in k-fold cross-validation. Typically, given these considerations, one performs k-fold cross-validation using \\(k = 5\\) or \\(k = 10\\), as these values have been shown empirically to yield test error rate estimates that suffer neither from excessively high bias nor from very high variance. "],["the-bootstrap.html", "Section 14 The Bootstrap", " Section 14 The Bootstrap The bootstrap is a widely applicable and extremely powerful statistical tool that can be used to quantify the uncertainty associated with a given estimator or statistical learning method. The bootstrap method is a statistical technique for estimating quantities about a population by averaging estimates from multiple small data samples. Importantly, samples are constructed by drawing observations from a large data sample one at a time and returning them to the data sample after they have been chosen. This allows a given observation to be included in a given small sample more than once. This approach to sampling is called sampling with replacement. The process for building one sample can be summarized as follows: Choose the size of the sample. While the size of the sample is less than the chosen size: Randomly select an observation from the dataset Add it to the sample Figure 14.1: A graphical illustration of the bootstrap approach on a small sample containing n=3 observation A graphical illustration of the bootstrap approach is illustrated in Figure 14.1. Each bootstrap data set contains \\(n\\) observations, sampled with replacement from the original data set. Each bootstrap data set is used to obtain an estimate of \\(\\alpha\\). "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
