<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Assessing Model Accuracy | A Minimal Book Example</title>
  <meta name="description" content="<p>This is a minimal example of using the bookdown package to write a book.
The HTML output format for this example is bookdown::gitbook,
set in the _output.yml file.</p>" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Assessing Model Accuracy | A Minimal Book Example" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="<p>This is a minimal example of using the bookdown package to write a book.
The HTML output format for this example is bookdown::gitbook,
set in the _output.yml file.</p>" />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Assessing Model Accuracy | A Minimal Book Example" />
  
  <meta name="twitter:description" content="<p>This is a minimal example of using the bookdown package to write a book.
The HTML output format for this example is bookdown::gitbook,
set in the _output.yml file.</p>" />
  

<meta name="author" content="John Doe" />


<meta name="date" content="2021-11-22" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="cross.html"/>
<script src="libs/header-attrs-2.9/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>



<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal Book Example</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> url: your book url like <span>https://bookdown.org/yihui/bookdown</span></a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#usage"><i class="fa fa-check"></i><b>1.1</b> Usage</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#render-book"><i class="fa fa-check"></i><b>1.2</b> Render book</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#preview-book"><i class="fa fa-check"></i><b>1.3</b> Preview book</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#what-is-statistical-learning"><i class="fa fa-check"></i><b>1.4</b> What is Statistical Learning?</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="index.html"><a href="index.html#why-estimate-f"><i class="fa fa-check"></i><b>1.4.1</b> Why Estimate f?</a></li>
<li class="chapter" data-level="1.4.2" data-path="index.html"><a href="index.html#inference"><i class="fa fa-check"></i><b>1.4.2</b> Inference</a></li>
<li class="chapter" data-level="1.4.3" data-path="index.html"><a href="index.html#how-do-we-estimate-f"><i class="fa fa-check"></i><b>1.4.3</b> How Do We Estimate f?</a></li>
<li class="chapter" data-level="1.4.4" data-path="index.html"><a href="index.html#the-tradeoff-between-prediction-accuracy-and-model-interpretability"><i class="fa fa-check"></i><b>1.4.4</b> The Tradeoff Between Prediction Accuracy and Model Interpretability</a></li>
<li class="chapter" data-level="1.4.5" data-path="index.html"><a href="index.html#supervised-vs.-unsupervised-learning"><i class="fa fa-check"></i><b>1.4.5</b> Supervised Vs. Unsupervised Learning</a></li>
<li class="chapter" data-level="1.4.6" data-path="index.html"><a href="index.html#regression-vs.-classification"><i class="fa fa-check"></i><b>1.4.6</b> Regression Vs. Classification</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="assessing-model-accuracy.html"><a href="assessing-model-accuracy.html"><i class="fa fa-check"></i><b>2</b> Assessing Model Accuracy</a>
<ul>
<li class="chapter" data-level="2.0.1" data-path="assessing-model-accuracy.html"><a href="assessing-model-accuracy.html#the-regression-setting"><i class="fa fa-check"></i><b>2.0.1</b> The Regression Setting</a></li>
<li class="chapter" data-level="2.0.2" data-path="assessing-model-accuracy.html"><a href="assessing-model-accuracy.html#the-bias-variance-trade-off"><i class="fa fa-check"></i><b>2.0.2</b> The Bias-Variance Trade-Off</a></li>
<li class="chapter" data-level="2.0.3" data-path="assessing-model-accuracy.html"><a href="assessing-model-accuracy.html#the-classification-setting"><i class="fa fa-check"></i><b>2.0.3</b> The Classification Setting</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="cross.html"><a href="cross.html"><i class="fa fa-check"></i><b>3</b> Cross-references</a>
<ul>
<li class="chapter" data-level="3.1" data-path="cross.html"><a href="cross.html#chapters-and-sub-chapters"><i class="fa fa-check"></i><b>3.1</b> Chapters and sub-chapters</a></li>
<li class="chapter" data-level="3.2" data-path="cross.html"><a href="cross.html#captioned-figures-and-tables"><i class="fa fa-check"></i><b>3.2</b> Captioned figures and tables</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="parts.html"><a href="parts.html"><i class="fa fa-check"></i><b>4</b> Parts</a></li>
<li class="chapter" data-level="5" data-path="footnotes-and-citations.html"><a href="footnotes-and-citations.html"><i class="fa fa-check"></i><b>5</b> Footnotes and citations</a>
<ul>
<li class="chapter" data-level="5.1" data-path="footnotes-and-citations.html"><a href="footnotes-and-citations.html#footnotes"><i class="fa fa-check"></i><b>5.1</b> Footnotes</a></li>
<li class="chapter" data-level="5.2" data-path="footnotes-and-citations.html"><a href="footnotes-and-citations.html#citations"><i class="fa fa-check"></i><b>5.2</b> Citations</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="blocks.html"><a href="blocks.html"><i class="fa fa-check"></i><b>6</b> Blocks</a>
<ul>
<li class="chapter" data-level="6.1" data-path="blocks.html"><a href="blocks.html#equations"><i class="fa fa-check"></i><b>6.1</b> Equations</a></li>
<li class="chapter" data-level="6.2" data-path="blocks.html"><a href="blocks.html#theorems-and-proofs"><i class="fa fa-check"></i><b>6.2</b> Theorems and proofs</a></li>
<li class="chapter" data-level="6.3" data-path="blocks.html"><a href="blocks.html#callout-blocks"><i class="fa fa-check"></i><b>6.3</b> Callout blocks</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="sharing-your-book.html"><a href="sharing-your-book.html"><i class="fa fa-check"></i><b>7</b> Sharing your book</a>
<ul>
<li class="chapter" data-level="7.1" data-path="sharing-your-book.html"><a href="sharing-your-book.html#publishing"><i class="fa fa-check"></i><b>7.1</b> Publishing</a></li>
<li class="chapter" data-level="7.2" data-path="sharing-your-book.html"><a href="sharing-your-book.html#pages"><i class="fa fa-check"></i><b>7.2</b> 404 pages</a></li>
<li class="chapter" data-level="7.3" data-path="sharing-your-book.html"><a href="sharing-your-book.html#metadata-for-sharing"><i class="fa fa-check"></i><b>7.3</b> Metadata for sharing</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Minimal Book Example</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="assessing-model-accuracy" class="section level1" number="2">
<h1><span class="header-section-number">Chapter 2</span> Assessing Model Accuracy</h1>
<blockquote>
<p><em>"<strong>There is no free lunch in statistics"</strong></em></p>
</blockquote>
<p>No one method dominates all others over all possible data sets. This section introduce some common ways to assess the accuracy of a model to select a statistical learning procedure for a specific data set.</p>
<div id="the-regression-setting" class="section level3" number="2.0.1">
<h3><span class="header-section-number">2.0.1</span> The Regression Setting</h3>
<div id="measuring-the-quality-of-fit" class="section level4" number="2.0.1.1">
<h4><span class="header-section-number">2.0.1.1</span> Measuring the Quality of fit</h4>
<p>In order to evaluate the performance of a model, we need to measure how well its predictions actually match the observed data. In the regression setting, the most commonly used measure is the <strong>mean squared error (MSE)</strong>:</p>
<p><span class="math display">\[
MSE = \frac{1}{n}\sum_{i = 1}^{n} (y_{n} - \hat{f}(x_{i})^2
\]</span></p>
<p>given by where <span class="math display">\[\hat{f}(x_{i}\]</span> is the prediction that <span class="math display">\[\hat{f}\]</span> gives for the ith observations.</p>
<p>The MSE will be small if the predicted responses are very close to the true response, and will be large if for some observations, the predicted and true responses differ substantially.</p>
<ul>
<li><em><strong>training MSE</strong></em>: The MSE is computed using the training data that was used to fit the model.</li>
<li><em><strong>test MSE</strong></em>: The MSE is computed using the previously unseen test observation not used to train the statistical learning method.</li>
</ul>
<p>When a given method yields a small training MSE but a large test MSE, we are said to be <em>overfitting</em> the data. When we overfit the training data, the test MSE will be very large because the supposed patterns that the method found in the training data simply don’t exist in the test data.</p>
</div>
</div>
<div id="the-bias-variance-trade-off" class="section level3" number="2.0.2">
<h3><span class="header-section-number">2.0.2</span> The Bias-Variance Trade-Off</h3>
<div id="variance-error" class="section level4" number="2.0.2.1">
<h4><span class="header-section-number">2.0.2.1</span> <strong>Variance Error</strong></h4>
<blockquote>
<p><strong>Variance</strong> is the amount that the estimate of <span class="math display">\[\hat{f}\]</span> will change if different training data was used.</p>
</blockquote>
<p>Ideally, it should not change too much from one training dataset to the next, meaning that the algorithm is good at picking out the hidden underlying mapping between the inputs and the output variables.</p>
<p>{% hint style=“info” %}
Examples of low-variance machine learning algorithms include: <strong>Linear Regression, Linear Discriminant Analysis and Logistic Regression.</strong>
{% endhint %}</p>
<p>{% hint style=“info” %}
Examples of high-variance machine learning algorithms include: <strong>Decision Trees, k-Nearest Neighbors</strong> and <strong>Support Vector Machines.</strong>
{% endhint %}</p>
</div>
<div id="bias-error" class="section level4" number="2.0.2.2">
<h4><span class="header-section-number">2.0.2.2</span> <strong>Bias Error</strong></h4>
<p><strong>Bias</strong> are the simplifying assumptions made by a model to make the target function easier to learn.</p>
<p>Generally, linear algorithms have a high bias making them fast to learn and easier to understand but generally less flexible. In turn, they have lower predictive performance on complex problems that fail to meet the simplifying assumptions of the algorithms bias.</p>
<ul>
<li><strong>Low Bias:</strong> Suggests less assumptions about the form of the target function.</li>
<li><strong>High-Bias:</strong> Suggests more assumptions about the form of the target function.</li>
</ul>
<p>{% hint style=“info” %}
Examples of low-bias machine learning algorithms include: <strong>Decision Trees, k-Nearest Neighbors </strong>and <strong>Support Vector Machines.</strong>
{% endhint %}</p>
<p>{% hint style=“info” %}
Examples of high-bias machine learning algorithms include: <strong>Linear Regression, Linear Discriminant Analysis</strong> and <strong>Logistic Regression.</strong>
{% endhint %}</p>
</div>
<div id="bias-variance-trafe-off" class="section level4" number="2.0.2.3">
<h4><span class="header-section-number">2.0.2.3</span> <strong>Bias-Variance Trafe-Off</strong></h4>
<p>The goal of any supervised machine learning algorithm is to achieve <em>low bias</em> and <em>low variance</em>. In turn the algorithm should achieve good prediction performance.</p>
<p>You can see a general trend in the examples above:</p>
<ul>
<li><strong>Linear machine learning algorithms</strong> often have a high bias but a low variance.</li>
<li><strong>Nonlinear machine learning algorithms</strong> often have a low bias but a high variance.</li>
<li>The parameterization of machine learning algorithms is often a battle to balance out bias and variance.</li>
</ul>
<hr />
</div>
</div>
<div id="the-classification-setting" class="section level3" number="2.0.3">
<h3><span class="header-section-number">2.0.3</span> The Classification Setting</h3>
<p>The most common approach for quantifying the accuracy of our estimate <span class="math display">\[\hat{f}\]</span> is the <em>training error</em>_ rate, the proportion of mistakes that are made if we apply our estimate <span class="math display">\[\hat{f}\]</span> to the training observations:</p>
<p><span class="math display">\[
\frac{1}{n}\sum_{i = 1}^{n} I (y_{i} \neq \hat{y_{i}})
\]</span></p>
<p>Whereas:</p>
<ul>
<li><span class="math display">\[\hat{y_{i}}\]</span>: the predicted class label for the ith observation using <span class="math display">\[\hat{f}\]</span></li>
<li><span class="math display">\[I (y_{i} \neq \hat{y_{i}})\]</span>: an <em>indicator variable</em> that equal <strong>1</strong> if <span class="math display">\[y_{i} \neq \hat{y_{i}}\]</span> and <strong>0</strong> if <span class="math display">\[y_{i} = \hat{y_{i}}\]</span>. If:
<ul>
<li><span class="math display">\[I (y_{i} \neq \hat{y_{i}}) = 0\]</span>: correct classification</li>
<li><span class="math display">\[I (y_{i} \neq \hat{y_{i}}) \neq 0\]</span>: incorrect classification (misclassified)</li>
</ul></li>
</ul>
<p>A good classifier is one for which the <em>test error</em> is smallest where the <em>test error</em> rate associated with a set of test observations of the from <span class="math display">\[(x_{0}, y_{0})\]</span>.</p>
<div id="the-bayes-classifier" class="section level4" number="2.0.3.1">
<h4><span class="header-section-number">2.0.3.1</span> The Bayes Classifier</h4>
<p>This algorithm is called Naïve because it works on the naïve assumption that the features are independent. Naïve Bayes Classifier works with principle of Bayes Theorem.</p>
<blockquote>
<p>Conditional probability of an event <span class="math display">\[A\]</span> given <span class="math display">\[B\]</span>, <span class="math display">\[P(A|B)\]</span> is the probability of <span class="math display">\[A\]</span> given that <span class="math display">\[B\]</span> has already occurred. It is often defined as the ratio of joint probability of <span class="math display">\[A\]</span> and <span class="math display">\[B\]</span> (probability of <span class="math display">\[A\]</span> and <span class="math display">\[B\]</span> occurring together) to the marginal probability of <span class="math display">\[A\]</span> (probability of event <span class="math display">\[A\]</span>)</p>
</blockquote>
<p><strong>Pros</strong></p>
<ul>
<li>Easy to implement</li>
<li>Performs reasonably well with noisy data</li>
</ul>
<p><strong>Cons</strong></p>
<ul>
<li>Poor performance with continuous features</li>
<li>Assumption that features are independent is risky</li>
</ul>
</div>
<div id="k-nearest-neighbors-knn" class="section level4" number="2.0.3.2">
<h4><span class="header-section-number">2.0.3.2</span> K-Nearest Neighbors (KNN)</h4>
<p>K-Nearest neighbors algorithm can be used to solve both classification and regression problems. When algorithms such as Naïve Bayes Classifier uses probabilities from training samples for predictions, KNN is Lazy learner that does not create any model in advance. The just find the closest based on feature similarity.</p>
<p><strong>Pros</strong></p>
<ul>
<li>Easy to implement</li>
<li>No assumptions involved</li>
</ul>
<p><strong>Cons</strong></p>
<ul>
<li>Optimal K is always a challenge</li>
<li>Lazy learner- computationally expensive</li>
</ul>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="cross.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/ndleah/stat-learning/edit/main/01-intro.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
