<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 What is Statistical Learning? | A Minimal Book Example</title>
  <meta name="description" content="<p>This is a minimal example of using the bookdown package to write a book.
The HTML output format for this example is bookdown::gitbook,
set in the _output.yml file.</p>" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 What is Statistical Learning? | A Minimal Book Example" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="<p>This is a minimal example of using the bookdown package to write a book.
The HTML output format for this example is bookdown::gitbook,
set in the _output.yml file.</p>" />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 What is Statistical Learning? | A Minimal Book Example" />
  
  <meta name="twitter:description" content="<p>This is a minimal example of using the bookdown package to write a book.
The HTML output format for this example is bookdown::gitbook,
set in the _output.yml file.</p>" />
  

<meta name="author" content="John Doe" />


<meta name="date" content="2021-11-22" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="assessing-model-accuracy.html"/>
<script src="libs/header-attrs-2.9/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>



<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal Book Example</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> url: your book url like <span>https://bookdown.org/yihui/bookdown</span></a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#usage"><i class="fa fa-check"></i><b>1.1</b> Usage</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#render-book"><i class="fa fa-check"></i><b>1.2</b> Render book</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#preview-book"><i class="fa fa-check"></i><b>1.3</b> Preview book</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="what-is-statistical-learning.html"><a href="what-is-statistical-learning.html"><i class="fa fa-check"></i><b>2</b> What is Statistical Learning?</a>
<ul>
<li class="chapter" data-level="2.1" data-path="what-is-statistical-learning.html"><a href="what-is-statistical-learning.html#why-estimate-f"><i class="fa fa-check"></i><b>2.1</b> Why Estimate f?</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="what-is-statistical-learning.html"><a href="what-is-statistical-learning.html#prediction"><i class="fa fa-check"></i><b>2.1.1</b> Prediction</a></li>
<li class="chapter" data-level="2.1.2" data-path="what-is-statistical-learning.html"><a href="what-is-statistical-learning.html#inference"><i class="fa fa-check"></i><b>2.1.2</b> Inference</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="what-is-statistical-learning.html"><a href="what-is-statistical-learning.html#part-how-do-we-estimate-f"><i class="fa fa-check"></i><b>2.2</b> (PART) How Do We Estimate f?</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="what-is-statistical-learning.html"><a href="what-is-statistical-learning.html#parametric-methods"><i class="fa fa-check"></i><b>2.2.1</b> Parametric Methods</a></li>
<li class="chapter" data-level="2.2.2" data-path="what-is-statistical-learning.html"><a href="what-is-statistical-learning.html#non-parametric-methods"><i class="fa fa-check"></i><b>2.2.2</b> Non-Parametric Methods</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="what-is-statistical-learning.html"><a href="what-is-statistical-learning.html#the-tradeoff-between-prediction-accuracy-and-model-interpretability"><i class="fa fa-check"></i><b>2.3</b> The Tradeoff Between Prediction Accuracy and Model Interpretability</a></li>
<li class="chapter" data-level="2.4" data-path="what-is-statistical-learning.html"><a href="what-is-statistical-learning.html#supervised-vs.-unsupervised-learning"><i class="fa fa-check"></i><b>2.4</b> Supervised Vs. Unsupervised Learning</a></li>
<li class="chapter" data-level="2.5" data-path="what-is-statistical-learning.html"><a href="what-is-statistical-learning.html#regression-vs.-classification"><i class="fa fa-check"></i><b>2.5</b> Regression Vs. Classification</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="assessing-model-accuracy.html"><a href="assessing-model-accuracy.html"><i class="fa fa-check"></i><b>3</b> Assessing Model Accuracy</a>
<ul>
<li class="chapter" data-level="3.1" data-path="assessing-model-accuracy.html"><a href="assessing-model-accuracy.html#the-regression-setting"><i class="fa fa-check"></i><b>3.1</b> The Regression Setting</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="assessing-model-accuracy.html"><a href="assessing-model-accuracy.html#measuring-the-quality-of-fit"><i class="fa fa-check"></i><b>3.1.1</b> Measuring the Quality of fit</a></li>
<li class="chapter" data-level="3.1.2" data-path="assessing-model-accuracy.html"><a href="assessing-model-accuracy.html#the-bias-variance-trade-off"><i class="fa fa-check"></i><b>3.1.2</b> The Bias-Variance Trade-Off</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="assessing-model-accuracy.html"><a href="assessing-model-accuracy.html#the-classification-setting"><i class="fa fa-check"></i><b>3.2</b> The Classification Setting</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="assessing-model-accuracy.html"><a href="assessing-model-accuracy.html#the-bayes-classifier"><i class="fa fa-check"></i><b>3.2.1</b> The Bayes Classifier</a></li>
<li class="chapter" data-level="3.2.2" data-path="assessing-model-accuracy.html"><a href="assessing-model-accuracy.html#k-nearest-neighbors-knn"><i class="fa fa-check"></i><b>3.2.2</b> K-Nearest Neighbors (KNN)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="cross.html"><a href="cross.html"><i class="fa fa-check"></i><b>4</b> Cross-references</a>
<ul>
<li class="chapter" data-level="4.1" data-path="cross.html"><a href="cross.html#chapters-and-sub-chapters"><i class="fa fa-check"></i><b>4.1</b> Chapters and sub-chapters</a></li>
<li class="chapter" data-level="4.2" data-path="cross.html"><a href="cross.html#captioned-figures-and-tables"><i class="fa fa-check"></i><b>4.2</b> Captioned figures and tables</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="parts.html"><a href="parts.html"><i class="fa fa-check"></i><b>5</b> Parts</a></li>
<li class="chapter" data-level="6" data-path="footnotes-and-citations.html"><a href="footnotes-and-citations.html"><i class="fa fa-check"></i><b>6</b> Footnotes and citations</a>
<ul>
<li class="chapter" data-level="6.1" data-path="footnotes-and-citations.html"><a href="footnotes-and-citations.html#footnotes"><i class="fa fa-check"></i><b>6.1</b> Footnotes</a></li>
<li class="chapter" data-level="6.2" data-path="footnotes-and-citations.html"><a href="footnotes-and-citations.html#citations"><i class="fa fa-check"></i><b>6.2</b> Citations</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="blocks.html"><a href="blocks.html"><i class="fa fa-check"></i><b>7</b> Blocks</a>
<ul>
<li class="chapter" data-level="7.1" data-path="blocks.html"><a href="blocks.html#equations"><i class="fa fa-check"></i><b>7.1</b> Equations</a></li>
<li class="chapter" data-level="7.2" data-path="blocks.html"><a href="blocks.html#theorems-and-proofs"><i class="fa fa-check"></i><b>7.2</b> Theorems and proofs</a></li>
<li class="chapter" data-level="7.3" data-path="blocks.html"><a href="blocks.html#callout-blocks"><i class="fa fa-check"></i><b>7.3</b> Callout blocks</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="sharing-your-book.html"><a href="sharing-your-book.html"><i class="fa fa-check"></i><b>8</b> Sharing your book</a>
<ul>
<li class="chapter" data-level="8.1" data-path="sharing-your-book.html"><a href="sharing-your-book.html#publishing"><i class="fa fa-check"></i><b>8.1</b> Publishing</a></li>
<li class="chapter" data-level="8.2" data-path="sharing-your-book.html"><a href="sharing-your-book.html#pages"><i class="fa fa-check"></i><b>8.2</b> 404 pages</a></li>
<li class="chapter" data-level="8.3" data-path="sharing-your-book.html"><a href="sharing-your-book.html#metadata-for-sharing"><i class="fa fa-check"></i><b>8.3</b> Metadata for sharing</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Minimal Book Example</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="what-is-statistical-learning" class="section level1" number="2">
<h1><span class="header-section-number">Chapter 2</span> What is Statistical Learning?</h1>
<p>There are 2 types of statistical learning: <em>supervised</em> and <em>unsupervised</em>.</p>
<blockquote>
<ul>
<li><strong>Supervised learning</strong> is when you have a label for each data point, which mean it involves building a model that can predict an <em>output</em> based on one or more <em>inputs</em>.</li>
<li><strong>Unsupervised learning</strong> is when you don’t have a label for each data point, where there are <em>inputs</em> but no supervising <em>output</em>.</li>
</ul>
</blockquote>
<p>In statistical learning, <em><strong>input variables</strong></em> <span class="math display">\[(X_{n})\]</span> are typically denoted by <em>features</em>, <em>predictors</em>, <em>indepedent variables</em> or <em>variables</em> while <em><strong>output variable</strong></em> <span class="math display">\[(Y)\]</span> often called <em>dependent variable</em> or <em>response</em>.</p>
<p>To assess the relationship between predictors <span class="math display">\[X_{1}, X_{2}, ...,X_{p}\]</span>, we have the equation as following:</p>
<p><span class="math display">\[
Y=f(X) + \epsilon
\]</span></p>
<p>Whereas:</p>
<ul>
<li><span class="math display">\[f\]</span> is fixed but unknown function of <span class="math display">\[X_{1},...,X_{p}\]</span> and <span class="math display">\[\epsilon\]</span> is a random <em>error term</em>, which is independent of <span class="math display">\[X\]</span> and has mean zero.</li>
</ul>
<p>In essence, statistical learning refers to a set of approaches for estimating <span class="math display">\[f\]</span>.</p>
<div id="why-estimate-f" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> Why Estimate f?</h2>
<p>There are 2 main reasons: <em><strong>prediction</strong></em> and <em><strong>inference</strong></em>.</p>
<div id="prediction" class="section level3" number="2.1.1">
<h3><span class="header-section-number">2.1.1</span> Prediction</h3>
<p>Hypothetically, let’s say we have the <em><strong>error term</strong></em> averages to 0, predicting <span class="math display">\[Y\]</span> can be assessed using this equation:</p>
<p><span class="math display">\[
\hat{Y} = \hat{f}(X)
\]</span></p>
<p>Whereas:</p>
<ul>
<li><span class="math display">\[\hat{f}\]</span> represents the estimate for <span class="math display">\[f\]</span></li>
<li><span class="math display">\[\hat{Y}\]</span> represents the resulting predcition for <span class="math display">\[Y\]</span></li>
</ul>
<p>The accuracy of <span class="math display">\[\hat{Y}\]</span> as a prediction for <span class="math display">\[Y\]</span> depends on 2 quantities: <em><strong>reducible error</strong></em> and <em><strong>irriducible error</strong></em>.</p>
<blockquote>
<ul>
<li><em><strong>reducible error</strong></em>: Reducible error is the error arising from the mismatch between <span class="math display">\[\hat{f}\]</span> and <span class="math display">\[f\]</span>. Can be improved by choosing a better model. Usually caused by <a href="overview.md#variance-error">Variance Error</a>/<a href="overview.md#bias-error">Bias Error</a>.</li>
<li><em><strong>irriducible error</strong></em>: Errors which can’t be removed no matter what algorithm you apply. These errors are caused by unknown variables that are affecting the independent/output variable but are not one of the dependent/input variable while designing the model.</li>
</ul>
</blockquote>
</div>
<div id="inference" class="section level3" number="2.1.2">
<h3><span class="header-section-number">2.1.2</span> Inference</h3>
<p>Neccessary questions need to be asked in order to further understand the relationship between predictors <span class="math display">\[(X_{n})\]</span> and outcome <span class="math display">\[(Y)\]</span>:</p>
<ul>
<li><em><strong>Which predictors are associated with response?</strong></em> Only a small fraction of the available predictors are associated with the response.</li>
<li><em><strong>What is the relationship between predictors and response?</strong></em> The relationship between predictors and response is not always linear.</li>
<li><em><strong>Can the relationship between predictors and response be explained by the linear model or it is more complicated?</strong></em> The model can explain the relationship between predictors and response if the model is able to predict the response based on the predictors.</li>
</ul>
<hr />
</div>
</div>
<div id="part-how-do-we-estimate-f" class="section level2" number="2.2">
<h2><span class="header-section-number">2.2</span> (PART) How Do We Estimate f?</h2>
<p>In order to estimate <span class="math display">\[f\]</span>, our goal is to apply a statistical learning method to the training data. Broadly speaking, most statistical learning methods for this task can be characterized as either <em><strong>parametric</strong></em> or <em><strong>non-parametric</strong></em>.</p>
<div id="parametric-methods" class="section level3" number="2.2.1">
<h3><span class="header-section-number">2.2.1</span> Parametric Methods</h3>
<p>Parametric methods (model-based approach) are those that are able to estimate the parameters of the model based on the training data. It involves a <strong>two-step model-based approach</strong>:</p>
<ol style="list-style-type: decimal">
<li>First, we make an assumption about the functional form, or shape, of <span class="math display">\[f\]</span>. In other words, we need to choose the model that best fits the data.</li>
<li>After the model has been selected, we need a procedure that uses the training data to <em>fit</em> or <em>train</em> the model.</li>
</ol>
<p>Potential disadvantages of parametric methods is that the model will not usually match the true <span class="math display">\[f\]</span>. This can be avoid by choosing a more <em>flexible</em> models that can fit many possible functions for <span class="math display">\[f\]</span> forms and usually require greater number of parameters.</p>
<p>{% hint style=“info” %}
Example fitting models for parametric methods (linear): <strong>Odirnary Least Squares (OLS), Lasso.</strong>
{% endhint %}</p>
</div>
<div id="non-parametric-methods" class="section level3" number="2.2.2">
<h3><span class="header-section-number">2.2.2</span> Non-Parametric Methods</h3>
<p>Non-parametric methods (model-free approach) seek an estimate of <span class="math display">\[f\]</span> without make explicit assumptions about the functional form of of <span class="math display">\[f\]</span>. Major disadvantages of this approach is that a very large number of observations is required in order to obtain an accurate estimate for <span class="math display">\[f\]</span>.</p>
<p>{% hint style=“info” %}
Example fitting models for non-parametric methods: <em><strong>smooth thin-plate spline fit</strong></em> and <em><strong>rough thin-plate spline fit</strong></em><strong>.</strong>
{% endhint %}</p>
<hr />
</div>
</div>
<div id="the-tradeoff-between-prediction-accuracy-and-model-interpretability" class="section level2" number="2.3">
<h2><span class="header-section-number">2.3</span> The Tradeoff Between Prediction Accuracy and Model Interpretability</h2>
<div class="figure">
<img src="img/xkcd-interpretability-vs-flexibility.png" alt="" />
<p class="caption">Figure 1. Tradeoff Between Prediction Accuracy and Model Interpretability</p>
</div>
<p>Figure 1 illustrates the tradeoff between flexibility and interpretability of using different statistical learning methods. In general, as the flexibility increase, the interpretability decreases.</p>
<p><strong>Why would we ever choose to use a more restrictive method instead of a very flexible approach?</strong> </p>
<p>If we are mainly interested in the interpretability of the model, we would rather use a more flexible model. This is because the flexibility of the model is usually better than the interpretability of the model. </p>
<p>In contrast, if we are interested in the prediction accuracy of the model, we would rather use a more restrictive model. This is because the flexibility of the model is usually better than the prediction accuracy of the model.</p>
<hr />
</div>
<div id="supervised-vs.-unsupervised-learning" class="section level2" number="2.4">
<h2><span class="header-section-number">2.4</span> Supervised Vs. Unsupervised Learning</h2>
<p>Most statistical learning problems involve both <strong>supervised</strong> and <strong>unsupervised learning</strong>.</p>
<p><strong>In supervised learning</strong>, we wish to fit a model to the training data and predict the response variable based on the predictors, with the aim of accurately predicting the response variable or better understanding the relationship between predictors and response variable.</p>
<p>Some of the statistical approaches that apply the supervised learning method are:</p>
<ul>
<li><strong>Linear Regression</strong></li>
<li><strong>Logistic Regression</strong></li>
<li><strong>Boosting &amp; Support Vector Machine</strong></li>
<li><strong>Generalized Additive Models (GAMs)</strong></li>
</ul>
<p>In contrast, <strong>unsupervised learning methods </strong>are those that do not require any training data. One statistical learning tool that we may use in this setting is <em>cluster analysis</em> or clustering. The goal of this method is to ascertain, whether observations fall into distinct groups.</p>
<hr />
</div>
<div id="regression-vs.-classification" class="section level2" number="2.5">
<h2><span class="header-section-number">2.5</span> Regression Vs. Classification</h2>
<p>Variables can be characterized as either <em><strong>quantitative</strong></em> or <em><strong>qualitative</strong></em>. </p>
<ul>
<li>Quantitative variables are those that can be measured in terms of a number. </li>
<li>Qualitative variables are those that can be measured in terms of a set of categories.</li>
</ul>
<p>We tend to refer to problems with a quantitative response variable as <strong>regression</strong> problems and problems with a qualitative response variable as <strong>classification</strong> problems. However, an important note is that it does not matter much whether the predictors/variables are quantitative or qualitative.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="assessing-model-accuracy.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/ndleah/stat-learning/edit/main/01-intro.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
